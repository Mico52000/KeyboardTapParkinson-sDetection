{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705d3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nqDataLoader as nq #data loading library\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7affcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>gt</th>\n",
       "      <th>updrs108</th>\n",
       "      <th>afTap</th>\n",
       "      <th>sTap</th>\n",
       "      <th>nqScore</th>\n",
       "      <th>typingSpeed</th>\n",
       "      <th>file_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>79.0</td>\n",
       "      <td>184.5</td>\n",
       "      <td>0.107179</td>\n",
       "      <td>56.866667</td>\n",
       "      <td>1424946827.1000_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>96.5</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.056286</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1427279751.1001_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>140.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>119.037037</td>\n",
       "      <td>1426676689.1002_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>83.5</td>\n",
       "      <td>191.5</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>74.266667</td>\n",
       "      <td>1429866367.1004_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>68.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>74.969697</td>\n",
       "      <td>1430134526.1005_001_014.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pID  gt  updrs108  afTap   sTap   nqScore  typingSpeed  \\\n",
       "0  1000   1        27   79.0  184.5  0.107179    56.866667   \n",
       "1  1001   1        16   96.5  189.0  0.056286   118.000000   \n",
       "2  1002   0         5  140.0  158.0  0.039519   119.037037   \n",
       "3  1004   1        22   83.5  191.5  0.034853    74.266667   \n",
       "4  1005   1        17   68.0  150.0  0.048307    74.969697   \n",
       "\n",
       "                        file_1  \n",
       "0  1424946827.1000_001_014.csv  \n",
       "1  1427279751.1001_001_014.csv  \n",
       "2  1426676689.1002_001_014.csv  \n",
       "3  1429866367.1004_001_014.csv  \n",
       "4  1430134526.1005_001_014.csv  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the early stage dataset \n",
    "early_stage = pd.read_csv('GT_DataPD_MIT-CS2PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "early_stage[\"gt\"] = early_stage[\"gt\"].astype(int)\n",
    "early_stage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea5e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_time_series = []\n",
    "for index, row in early_stage.iterrows():\n",
    "    fileloc = row.file_1\n",
    "    keyPressed, htArr, pressArr, releaseArr =  nq.getDataFiltHelper( \"data_MIT-CS2PD/\" + early_stage.loc[index]['file_1'])\n",
    "    htArr =np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "\n",
    "X1 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f32abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>gt</th>\n",
       "      <th>updrs108</th>\n",
       "      <th>afTap</th>\n",
       "      <th>sTap</th>\n",
       "      <th>nqScore</th>\n",
       "      <th>typingSpeed</th>\n",
       "      <th>file_1</th>\n",
       "      <th>file_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>14.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.25</td>\n",
       "      <td>0.117543</td>\n",
       "      <td>189.372549</td>\n",
       "      <td>1402930351.011_001_014.csv</td>\n",
       "      <td>1403706430.011_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.25</td>\n",
       "      <td>0.070350</td>\n",
       "      <td>60.533333</td>\n",
       "      <td>1402932300.060_001_014.csv</td>\n",
       "      <td>1403708258.060_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>25.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.75</td>\n",
       "      <td>0.223411</td>\n",
       "      <td>54.333333</td>\n",
       "      <td>1401117235.067_001_014.csv</td>\n",
       "      <td>1401978395.067_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159.00</td>\n",
       "      <td>0.074973</td>\n",
       "      <td>71.800000</td>\n",
       "      <td>1401114972.068_001_014.csv</td>\n",
       "      <td>1401980765.068_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>26.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.50</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>39.614035</td>\n",
       "      <td>1404311419.070_001_014.csv</td>\n",
       "      <td>1404743687.070_003_014.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pID  gt  updrs108  afTap    sTap   nqScore  typingSpeed  \\\n",
       "0   11   1     14.25    NaN  162.25  0.117543   189.372549   \n",
       "1   60   0      2.00    NaN  162.25  0.070350    60.533333   \n",
       "2   67   1     25.25    NaN  133.75  0.223411    54.333333   \n",
       "3   68   0      6.00    NaN  159.00  0.074973    71.800000   \n",
       "4   70   1     26.25    NaN  113.50  0.175751    39.614035   \n",
       "\n",
       "                       file_1                      file_2  \n",
       "0  1402930351.011_001_014.csv  1403706430.011_003_014.csv  \n",
       "1  1402932300.060_001_014.csv  1403708258.060_003_014.csv  \n",
       "2  1401117235.067_001_014.csv  1401978395.067_003_014.csv  \n",
       "3  1401114972.068_001_014.csv  1401980765.068_003_014.csv  \n",
       "4  1404311419.070_001_014.csv  1404743687.070_003_014.csv  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the de-novo dataset \n",
    "de_novo = pd.read_csv('GT_DataPD_MIT-CS1PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "print(len(de_novo))\n",
    "de_novo[\"gt\"] = de_novo[\"gt\"].astype(int)\n",
    "de_novo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5259add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##using both files \n",
    "hit_time_series = []\n",
    "for index, row in de_novo.iterrows():\n",
    "    fileloc1 = row.file_1\n",
    "    keyPressed, htArr1, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_1'])\n",
    "    htArr1 = np.array(htArr1)\n",
    "    keyPressed, htArr2, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_2'])\n",
    "    htArr2 = np.array(htArr2)\n",
    "    htArr =np.concatenate((htArr1,htArr2),axis =0)\n",
    "    htArr=np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "X2 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282ec3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "40\n",
      "43\n",
      "1975.5882352941176\n",
      "299\n",
      "85 85\n",
      "[array([0.3179, 0.1892, 0.1641, 0.2597, 0.1994, 0.1016, 0.122 , 0.139 ,\n",
      "        0.147 , 0.2011, 0.0758, 0.1902, 0.1341, 0.1766, 0.0912, 0.2218,\n",
      "        0.1295, 0.1171, 0.1057, 0.077 , 0.0969, 0.2141, 0.1101, 0.1355,\n",
      "        0.1697, 0.1038, 0.1234, 0.1405, 0.1393, 0.1209, 0.1359, 0.1342,\n",
      "        0.1935, 0.0775, 0.1834, 0.178 , 0.2392, 0.1381, 0.1584, 0.1741,\n",
      "        0.1349, 0.2007, 0.1402, 0.1964, 0.2102, 0.1575, 0.1527, 0.0831,\n",
      "        0.1709, 0.1588, 0.1162, 0.1702, 0.1259, 0.1004, 0.1751, 0.1479,\n",
      "        0.1835, 0.1342, 0.1687, 0.1576, 0.1129, 0.2006, 0.1176, 0.1315,\n",
      "        0.2539, 0.1244, 0.1316, 0.1911, 0.1904, 0.1438, 0.1757, 0.1734,\n",
      "        0.1517, 0.18  , 0.1208, 0.1003, 0.1092, 0.1556, 0.1204, 0.2012,\n",
      "        0.1609, 0.214 , 0.1929, 0.3871, 0.1575, 0.118 , 0.1306, 0.2313,\n",
      "        0.1446, 0.1225, 0.1309, 0.1261, 0.1912, 0.1307, 0.1356, 0.0955,\n",
      "        0.1739, 0.1349, 0.1399, 0.1971, 0.1133, 0.1099, 0.1257, 0.1404,\n",
      "        0.153 , 0.1179, 0.1884, 0.1261, 0.1448, 0.1837, 0.148 , 0.2308,\n",
      "        0.1405, 0.1566, 0.1071, 0.1095, 0.1273, 0.1338, 0.1787, 0.151 ,\n",
      "        0.2489, 0.1791, 0.1969, 0.1134, 0.1006, 0.125 , 0.1261, 0.1352,\n",
      "        0.1489, 0.1178, 0.1349, 0.1314, 0.1438, 0.1481, 0.1841, 0.1176,\n",
      "        0.1885, 0.1657, 0.2059, 0.1557, 0.201 , 0.188 , 0.1076, 0.1782,\n",
      "        0.1177, 0.1532, 0.1354, 0.179 , 0.1178, 0.1275, 0.1098, 0.1273,\n",
      "        0.1378, 0.1512, 0.1176, 0.12  , 0.149 , 0.1001, 0.1131, 0.1209,\n",
      "        0.1003, 0.1403, 0.131 , 0.1359, 0.2185, 0.1105, 0.146 , 0.1692,\n",
      "        0.1282, 0.1394, 0.1206, 0.1631, 0.1745, 0.1068, 0.1622, 0.0923,\n",
      "        0.1843, 0.1177, 0.1832, 0.1105, 0.1582, 0.1654, 0.1301, 0.1889,\n",
      "        0.148 , 0.1483, 0.1206, 0.179 , 0.1306, 0.1172, 0.1685, 0.1449,\n",
      "        0.1535, 0.1378, 0.1785, 0.1533, 0.1274, 0.1077, 0.1412, 0.1873,\n",
      "        0.1494, 0.1836, 0.1278, 0.1275, 0.1277, 0.1566, 0.1764, 0.1759,\n",
      "        0.1517, 0.1176, 0.0998, 0.1558, 0.134 , 0.1357, 0.1258, 0.2407,\n",
      "        0.1884, 0.161 , 0.1087, 0.2094, 0.1277, 0.0958, 0.0964, 0.1264,\n",
      "        0.1359, 0.1587, 0.1488, 0.1537, 0.2109, 0.1911, 0.1611, 0.1986,\n",
      "        0.1484, 0.1137, 0.167 , 0.1758, 0.179 , 0.1352, 0.1843, 0.1609,\n",
      "        0.1046, 0.1669, 0.1306, 0.0871, 0.1247, 1.2744, 0.463 , 0.1119,\n",
      "        0.0279, 0.0804, 0.0539, 0.0737, 0.1188, 0.1014, 0.0696, 0.0699,\n",
      "        0.041 , 0.0307, 0.1175, 0.0493, 0.0583, 0.0737, 0.0597, 0.0447,\n",
      "        0.0577, 0.036 , 0.0701, 0.0815, 0.0311, 0.0831, 0.0778, 0.1539,\n",
      "        0.1758, 0.1035, 0.1485, 0.0782, 0.1163, 0.1276, 0.1783, 0.1081,\n",
      "        0.1175, 0.1784, 0.1129, 0.1249, 0.2037, 0.1015, 0.1763, 0.2361,\n",
      "        0.1156, 0.1158, 0.1076, 0.1471, 0.1471, 0.1637, 0.1497, 0.151 ,\n",
      "        0.1465, 0.15  , 0.1036, 0.1507, 0.1681, 0.1511, 0.1703, 0.1686,\n",
      "        0.124 , 0.1916, 0.2113, 0.3489, 0.0986, 0.101 , 0.1766, 0.1469,\n",
      "        0.1591, 0.1464, 0.1773, 0.1672, 0.1485, 0.1278, 0.161 , 0.1175,\n",
      "        0.1658, 0.1714, 0.201 , 0.1509, 0.1582, 0.1137, 0.1177, 0.161 ,\n",
      "        0.1308, 0.1734, 0.1721, 0.1493, 0.1547, 0.1157, 0.1729, 0.1595,\n",
      "        0.361 , 0.1259, 0.1228, 0.1014, 0.1128, 0.136 , 0.2081, 0.1004,\n",
      "        0.1135, 0.1405, 0.1335, 0.1453, 0.149 , 0.1351, 0.0814, 0.1309,\n",
      "        0.1788, 0.184 , 0.1081, 0.1386, 0.1485, 0.1963, 0.1307, 0.1232,\n",
      "        0.1842, 0.1009, 0.1845, 0.1354, 0.1767, 0.1571, 0.1136, 0.0958,\n",
      "        0.0505, 0.1715, 0.1396, 0.2031, 0.0951, 0.1231, 0.1179, 0.1185,\n",
      "        0.1303, 0.1967, 0.1991, 0.3322, 0.123 , 0.1203, 0.1004, 0.1034,\n",
      "        0.2039, 0.1407, 0.2046, 0.2132, 0.1585, 0.1055, 0.1878, 0.1526,\n",
      "        0.1969, 0.1262, 0.2316, 0.1706, 0.1318, 0.1491, 0.1611, 0.1307,\n",
      "        0.1401, 0.1179, 0.0829, 0.1755, 0.0997, 0.1281, 0.1584, 0.1429,\n",
      "        0.1175, 0.1042, 0.0938, 0.1337, 0.1707, 0.1005, 0.1256, 0.2259,\n",
      "        0.1307, 0.1183, 0.0828, 0.1691, 0.2216, 0.1509, 0.1442, 0.1356,\n",
      "        0.1977, 0.113 , 0.1481, 0.1346, 0.147 , 0.0753, 0.0986, 0.1007,\n",
      "        0.2092, 0.1308, 0.1001, 0.1968, 0.14  , 0.1384, 0.1059, 0.1583,\n",
      "        0.1307, 0.1383, 0.1333, 0.1002, 0.2183, 0.1532, 0.1957, 0.1948,\n",
      "        0.1558, 0.179 , 0.1439, 0.1658, 0.1564, 0.1077, 0.109 , 0.1006,\n",
      "        0.1332, 0.1279, 0.1296, 0.099 , 0.1362, 0.1176, 0.1145, 0.158 ,\n",
      "        0.0952, 0.1332, 0.131 , 0.0967, 0.2088, 0.1053, 0.1003, 0.1869,\n",
      "        0.1232, 0.1391, 0.1793, 0.1923, 0.1489, 0.1027, 0.1088, 0.1793,\n",
      "        0.1361, 0.1527, 0.1081, 0.118 , 0.1163, 0.1308, 0.1211, 0.0962,\n",
      "        0.1823, 0.147 , 0.1004, 0.0836, 0.1264, 0.1481, 0.11  , 0.1537,\n",
      "        0.1607, 0.2207, 0.1248, 0.3244, 0.1194, 0.1354, 0.1572, 0.1208,\n",
      "        0.129 , 0.1335, 0.1336, 0.1208, 0.1293, 0.094 , 0.155 , 0.1392,\n",
      "        0.1161, 0.1119, 0.1244, 0.1596, 0.0984, 0.157 , 0.1015, 0.1388,\n",
      "        0.1421, 0.1295, 0.1761, 0.148 , 0.3358, 0.5507, 0.3517, 0.1509,\n",
      "        0.1784, 0.1553, 0.1291, 0.1477, 0.2233, 0.1454, 0.197 , 0.1448,\n",
      "        0.1033, 0.1324, 0.1996, 0.1387, 0.139 , 0.1528, 0.1134, 0.1979,\n",
      "        0.1881, 0.1752, 0.1758, 0.127 , 0.1733, 0.1681, 0.1305, 0.1711,\n",
      "        0.1175, 0.1309, 0.2714, 0.1639, 0.0936, 0.1761, 0.1464, 0.1646,\n",
      "        0.1115, 0.1198, 0.1633, 0.1431, 0.1196, 0.1292, 0.1244, 0.2436,\n",
      "        0.1514, 0.177 , 0.1082, 0.1088, 0.1523, 0.1294, 0.1677, 0.2547,\n",
      "        0.1947, 0.1634, 0.2234, 0.1056, 0.1359, 0.1784, 0.1435, 0.1308,\n",
      "        0.1312, 0.1266, 0.1314, 0.1135, 0.1171, 0.1758, 0.1136, 0.0885,\n",
      "        0.0826, 0.1453, 0.2107, 0.1567, 0.1347, 0.0926, 0.14  , 0.1097,\n",
      "        0.1175, 0.1825, 0.1255, 0.1688, 0.1306, 0.1006, 0.1552, 0.1705,\n",
      "        0.1349, 0.1003, 0.1172, 0.1755, 0.1987, 0.1683, 0.0912, 0.1007,\n",
      "        0.1003, 0.1254, 0.0821, 0.1399, 0.0999, 0.1183, 0.1127, 0.1179,\n",
      "        0.0977, 0.1203, 0.1611, 0.1678, 0.0359, 0.1361, 0.0833, 0.171 ,\n",
      "        0.1131, 0.136 , 0.1541, 0.1278, 0.1487, 0.1744, 0.2218, 0.1357,\n",
      "        0.1128, 0.1558, 0.1439, 0.0738, 0.1967, 0.1441, 0.1309, 0.1838,\n",
      "        0.1307, 0.1842, 0.2155, 0.1938, 0.1136, 0.1614, 0.1262, 0.1004,\n",
      "        0.1272, 0.1089, 0.1246, 0.1599, 0.1458, 0.1661, 0.1314, 0.1763,\n",
      "        0.0885, 0.0845, 0.1967, 0.1408, 0.1637, 0.1055, 0.1534, 0.1832,\n",
      "        0.1356, 0.1484, 0.1308, 0.1518, 0.0961, 0.1788, 0.1542, 0.1364,\n",
      "        0.1612, 0.131 , 0.1913, 0.1656, 0.4023, 0.1368, 0.1172, 0.1097,\n",
      "        0.148 , 0.1615, 0.1486, 0.1312, 0.1059, 0.1668, 0.1745, 0.1628,\n",
      "        0.1262, 0.1212, 0.1153, 0.1561, 0.1263, 0.1982, 0.11  , 0.1555,\n",
      "        0.1044, 0.2086, 0.1339, 0.2739, 0.1463, 0.2717, 0.2868, 0.2113,\n",
      "        0.241 , 0.1829, 0.1179, 0.2128, 0.2046, 0.0864, 0.0768, 0.158 ,\n",
      "        0.1335, 0.1644, 0.1172, 0.112 , 0.1818, 0.1256, 0.0892, 0.0937,\n",
      "        0.1087, 0.1653, 0.1424, 0.1216, 0.1243, 0.1662, 0.1863, 0.2188,\n",
      "        0.1616, 0.1381, 0.1277, 0.1606, 0.1358, 0.1171, 0.2128, 0.18  ,\n",
      "        0.1273, 0.2261, 0.1263, 0.1308, 0.1229, 0.1533, 0.1977, 0.1038,\n",
      "        0.1003, 0.1207, 0.1159, 0.1081, 0.1462, 0.1133, 0.1352, 0.1321,\n",
      "        0.1054, 0.1669, 0.1604, 0.1581, 0.189 , 0.1008, 0.0806, 0.1362,\n",
      "        0.1691, 0.1244, 0.1246, 0.1412, 0.1789, 0.1131, 0.1522, 0.135 ,\n",
      "        0.151 , 0.1954, 0.1319, 0.2262, 0.1452, 0.1274, 0.1132, 0.1302,\n",
      "        0.1306, 0.1487, 0.1561, 0.1446, 0.1525, 0.1524, 0.1059, 0.1315,\n",
      "        0.1487, 0.1544, 0.2104, 0.1238, 0.119 , 0.1127, 0.1113, 0.115 ,\n",
      "        0.1768, 0.1748, 0.1367, 0.0956, 0.1307, 0.1326, 0.152 , 0.183 ,\n",
      "        0.1478, 0.1764, 0.115 , 0.1194, 0.1466, 0.169 , 0.1678, 0.1363,\n",
      "        0.2435, 0.1777, 0.1472, 0.1788, 0.1338, 0.1244, 0.1525, 0.1418,\n",
      "        0.1454, 0.1429, 0.1508, 0.2744, 0.1443, 0.1762, 0.1525, 0.1384,\n",
      "        0.1411, 0.2527, 0.1418, 0.1948, 0.14  , 0.123 , 0.1866, 0.1245,\n",
      "        0.1644, 0.1089, 0.1652, 0.1166, 0.1261, 0.1332, 0.1131, 0.1397,\n",
      "        0.1818, 0.12  , 0.1246, 0.2272, 0.1015, 0.0946, 0.0514])\n",
      " array([0.1135, 0.1269, 0.1317, ..., 0.0978, 0.0979, 0.1241])\n",
      " array([0.0619, 0.0598, 0.1309, ..., 0.0707, 0.0747, 0.0869])\n",
      " array([0.1093, 0.0939, 0.1115, ..., 0.1407, 0.036 , 0.1439])\n",
      " array([0.1002, 0.1057, 0.0784, ..., 0.085 , 0.1167, 0.0967])\n",
      " array([0.166 , 0.1032, 0.0502, ..., 0.07  , 0.0731, 0.0927])\n",
      " array([0.1542, 0.1139, 0.1873, ..., 0.1514, 0.0842, 0.1007])\n",
      " array([0.1245, 0.1519, 0.1872, ..., 0.0959, 0.1429, 0.1171])\n",
      " array([0.0913, 0.0784, 0.0875, ..., 0.0857, 0.0693, 0.0776])\n",
      " array([0.0639, 0.0698, 0.1483, ..., 0.0779, 0.091 , 0.0828])\n",
      " array([0.0855, 0.0753, 0.0773, ..., 0.0608, 0.0774, 0.0793])\n",
      " array([0.2108, 0.1698, 0.1414, ..., 0.1527, 0.1333, 0.1432])\n",
      " array([0.1515, 0.1756, 0.0994, ..., 0.1742, 0.2873, 0.178 ])\n",
      " array([0.1128, 0.1038, 0.1295, ..., 0.0874, 0.0707, 0.0748])\n",
      " array([0.1139, 0.0822, 0.0607, ..., 0.0532, 0.0481, 0.0411])\n",
      " array([0.2359, 0.1929, 0.4013, 0.2103, 0.183 , 0.2391, 0.2081, 0.2162,\n",
      "        0.2703, 0.1732, 0.2609, 0.3407, 0.2966, 0.1762, 0.2005, 0.2436,\n",
      "        0.2081, 0.1903, 0.2833, 0.2001, 0.2138, 0.2162, 0.2382, 0.2482,\n",
      "        0.2057, 0.2628, 0.2129, 0.2304, 0.1378, 0.1786, 0.2556, 0.1477,\n",
      "        0.0985, 0.2182, 0.1814, 0.2715, 0.1093, 0.1527, 0.2069, 0.1834,\n",
      "        0.2147, 0.1692, 0.2492, 0.1342, 0.2394, 0.2313, 0.2384, 0.2411,\n",
      "        0.2268, 0.2304, 0.1734, 0.2518, 0.1542, 0.1869, 0.3955, 0.2704,\n",
      "        0.2382, 0.1746, 0.1801, 0.213 , 0.2134, 0.2177, 0.216 , 0.1926,\n",
      "        0.2223, 0.1585, 0.2706, 0.175 , 0.2659, 0.1601, 0.2489, 0.219 ,\n",
      "        0.2151, 0.2343, 0.2433, 0.2617, 0.1273, 0.1617, 0.1306, 0.1739,\n",
      "        0.204 , 0.2305, 0.1905, 0.2156, 0.1942, 0.2273, 0.2089, 0.1779,\n",
      "        0.2301, 0.2177, 0.0385, 0.2525, 0.1735, 0.1958, 0.2828, 0.2266,\n",
      "        0.1955, 0.2742, 0.2226, 0.1928, 0.2259, 0.1821, 0.1958, 0.2748,\n",
      "        0.1924, 0.1707, 0.1876, 0.2012, 0.2015, 0.2362, 0.1898, 0.1773,\n",
      "        0.1342, 0.1499, 0.2176, 0.1894, 0.1955, 0.2127, 0.3181, 0.1697,\n",
      "        0.2587, 0.1493, 0.1203, 0.1473, 0.1632, 0.1786, 0.1632, 0.2233,\n",
      "        0.2478, 0.2056, 0.2049, 0.1957, 0.21  , 0.1735, 0.2561, 0.2654,\n",
      "        0.2534, 0.2224, 0.1346, 0.0951, 0.1735, 0.2005, 0.183 , 0.181 ,\n",
      "        0.1826, 0.2176, 0.1933, 0.2279, 0.2511, 0.1999, 0.306 , 0.246 ,\n",
      "        0.2362, 0.2396, 0.2049, 0.2306, 0.1735, 0.1449, 0.2264, 0.2004,\n",
      "        0.2173, 0.3058, 0.1875, 0.1473, 0.134 , 0.17  , 0.1876, 0.1872,\n",
      "        0.2486, 0.1252, 0.2228, 0.2514, 0.1914, 0.3389, 0.2385, 0.2119,\n",
      "        0.1996, 0.3345, 0.1554, 0.3493, 0.2211, 0.2233, 0.2337, 0.2003,\n",
      "        0.1954, 0.2082, 0.2828, 0.2183, 0.175 , 0.1484, 0.2006, 0.2254,\n",
      "        0.1854, 0.2348, 0.2005, 0.165 , 0.1675, 0.1812, 0.3107, 0.23  ,\n",
      "        0.2381, 0.2403, 0.2807, 0.1532, 0.2785, 0.3321, 0.1677, 0.2117,\n",
      "        0.2034, 0.2361, 0.1729, 0.1401, 0.1558, 0.2078, 0.3339, 0.176 ,\n",
      "        0.2416, 0.2169, 0.3699, 0.2088, 0.2082, 0.1395, 0.1917, 0.206 ,\n",
      "        0.1356, 0.1198, 0.2475, 0.2294, 0.2101, 0.2554, 0.1945, 0.1969,\n",
      "        0.1305, 0.1705, 0.1337, 0.1883, 0.2968, 0.2609, 0.3009, 0.2029,\n",
      "        0.2007, 0.2363, 0.2188, 0.179 , 0.1856, 0.2522, 0.1614, 0.1708,\n",
      "        0.1625, 0.2183, 0.2082, 0.2813, 0.223 , 0.262 , 0.2308, 0.2354,\n",
      "        0.2274, 0.2525, 0.1862, 0.2328, 0.1898, 0.1985, 0.1991, 0.2594,\n",
      "        0.2069, 0.1468, 0.3296, 0.1731, 0.2728, 0.1348, 0.1906, 0.2082,\n",
      "        0.1969, 0.2088, 0.2349, 0.2158, 0.1826, 0.2403, 0.1911, 0.2133,\n",
      "        0.2135, 0.2323, 0.2959, 0.2739, 0.3264, 0.2919, 0.1909, 0.2383,\n",
      "        0.2136, 0.3004, 0.1643, 0.2213, 0.2502, 0.1701, 0.1692, 0.2134,\n",
      "        0.138 , 0.195 , 0.188 , 0.2881, 0.2703, 0.1704, 0.1888, 0.2406,\n",
      "        0.2403, 0.2909, 0.2851, 0.183 , 0.1827, 0.3518, 0.2787, 0.2667,\n",
      "        0.248 , 0.2931, 0.2437, 0.1601, 0.1354, 0.1694, 0.1703, 0.2137,\n",
      "        0.2033, 0.2796, 0.2488, 0.1828, 0.2964, 0.2048, 0.3041, 0.2395,\n",
      "        0.1697, 0.266 , 0.2202, 0.2269, 0.2179, 0.2325, 0.2557, 0.4865,\n",
      "        0.2814, 0.2649, 0.3514, 0.2727, 0.2617, 0.3348, 0.3313, 0.2165,\n",
      "        0.199 , 0.2561, 0.3268, 0.1545, 0.2482, 0.2004, 0.2235, 0.1946,\n",
      "        0.0518, 0.2402, 0.224 , 0.2627, 0.2404, 0.1552, 0.16  , 0.1964,\n",
      "        0.2015, 0.3004, 0.1947, 0.3526, 0.2821, 0.1919, 0.1302, 0.2188,\n",
      "        0.1479, 0.1758, 0.1965, 0.2359, 0.218 , 0.3693, 0.1477, 0.2671,\n",
      "        0.3106, 0.1354, 0.1757, 0.2193, 0.1913, 0.2678, 0.1653, 0.2193,\n",
      "        0.2592, 0.216 , 0.1457, 0.2371, 0.2068, 0.2419, 0.1793, 0.3083,\n",
      "        0.2099, 0.1656, 0.2436, 0.1735, 0.2182, 0.2314, 0.2698, 0.1836,\n",
      "        0.1527, 0.2619, 0.1707, 0.2062, 0.254 , 0.2593, 0.1364, 0.1968,\n",
      "        0.3446, 0.2441, 0.2394, 0.2315, 0.1966, 0.2443, 0.2489, 0.231 ,\n",
      "        0.2167, 0.2116, 0.2144, 0.1967, 0.2368, 0.2111, 0.1568, 0.2231,\n",
      "        0.2443, 0.1803, 0.2696, 0.1841, 0.1364, 0.297 , 0.267 , 0.2538,\n",
      "        0.2446, 0.1536, 0.1385, 0.2274, 0.176 , 0.1795, 0.274 , 0.1628,\n",
      "        0.3095, 0.3448, 0.2539, 0.2573, 0.1396, 0.2048, 0.1991, 0.1808,\n",
      "        0.2318, 0.262 , 0.2515, 0.2482, 0.2762, 0.1809, 0.2272, 0.1395,\n",
      "        0.1276, 0.1653, 0.1424, 0.1809, 0.1747, 0.1999, 0.3057, 0.2749,\n",
      "        0.1876, 0.2207, 0.3183, 0.2697, 0.1985, 0.1832, 0.2526, 0.2307,\n",
      "        0.2189, 0.2308, 0.1646, 0.2704, 0.1703, 0.2138, 0.2304, 0.2853,\n",
      "        0.1639, 0.2606, 0.1335, 0.1918, 0.256 ])\n",
      " array([0.1787, 0.0959, 0.1241, ..., 0.1585, 0.0577, 0.1055])\n",
      " array([0.1465, 0.152 , 0.1184, ..., 0.2147, 0.1237, 0.1188])\n",
      " array([0.0701, 0.1527, 0.064 , ..., 0.0735, 0.0649, 0.1129])\n",
      " array([0.1205, 0.1329, 0.1888, ..., 0.1829, 0.1506, 0.1803])\n",
      " array([0.083 , 0.2014, 0.0783, ..., 0.1386, 0.0902, 0.174 ])\n",
      " array([0.1487, 0.114 , 0.1281, ..., 0.1239, 0.1216, 0.1662])\n",
      " array([0.2446, 0.3646, 0.2989, 0.0448, 0.1181, 0.0699, 0.2322, 0.0785,\n",
      "        0.1177, 0.2227, 0.071 , 0.0878, 0.2577, 0.2931, 0.0737, 0.2152,\n",
      "        0.1054, 0.1311, 0.1878, 0.1659, 0.1236, 0.2321, 0.0702, 0.0779,\n",
      "        0.0751, 0.2122, 0.0683, 0.2373, 0.3101, 0.2743, 0.0616, 0.2926,\n",
      "        0.1739, 0.1045, 0.0781, 0.0886, 0.1229, 0.2376, 0.0719, 0.1302,\n",
      "        0.2731, 0.1356, 0.2009, 0.2488, 0.1352, 0.1371, 0.2896, 0.1028,\n",
      "        0.2723, 0.0906, 0.208 , 0.1151, 0.0767, 0.0835, 0.2196, 0.1357,\n",
      "        0.2642, 0.1076, 0.0835, 0.0797, 0.1449, 0.3018, 0.1666, 0.0603,\n",
      "        0.0693, 0.143 , 0.1342, 0.074 , 0.2619, 0.1117, 0.0822, 0.0696,\n",
      "        0.3192, 0.2091, 0.262 , 0.0957, 0.1704, 0.2624, 0.2237, 0.1269,\n",
      "        0.1522, 0.1577, 0.0898, 0.0866, 0.2313, 0.0904, 0.231 , 0.0604,\n",
      "        0.1048, 0.2085, 0.0605, 0.1139, 0.3075, 0.0783, 0.1031, 0.2419,\n",
      "        0.0904, 0.0627, 0.149 , 0.0873, 0.1574, 0.2411, 0.1458, 0.2666,\n",
      "        0.1171, 0.117 , 0.1025, 0.2626, 0.2278, 0.1285, 0.3127, 0.3333,\n",
      "        0.2852, 0.1178, 0.0825, 0.0774, 0.0587, 0.154 , 0.2296, 0.071 ,\n",
      "        0.1265, 0.0875, 0.0878, 0.0914, 0.0832, 0.0529, 0.1526, 0.0622,\n",
      "        0.1234, 0.2415, 0.0672, 0.0895, 0.0785, 0.0628, 0.2151, 0.0966,\n",
      "        0.0709, 0.1234, 0.0877, 0.087 , 0.2204, 0.0976, 0.1836, 0.2235,\n",
      "        0.108 , 0.065 , 0.0782, 0.2368, 0.0683, 0.2196, 0.0657, 0.2151,\n",
      "        0.1966, 0.2625, 0.071 , 0.0609, 0.0792, 0.2204, 0.0995, 0.0648,\n",
      "        0.2183, 0.2285, 0.3223, 0.0859, 0.1422, 0.1457, 0.2964, 0.2005,\n",
      "        0.3453, 0.2324, 0.0866, 0.1534, 0.0947, 0.0946, 0.1778, 0.1568,\n",
      "        0.0849, 0.1933, 0.2393, 0.1605, 0.2754, 0.2185, 0.0785, 0.0822,\n",
      "        0.2132, 0.1543, 0.1339, 0.0951, 0.0608, 0.0744, 0.2424, 0.0664,\n",
      "        0.0999, 0.1481, 0.2473, 0.1133, 0.2055, 0.2686, 0.2467, 0.0759,\n",
      "        0.0849, 0.0886, 0.0649, 0.2476, 0.0839, 0.0704, 0.0706, 0.1699,\n",
      "        0.2417, 0.0817, 0.0755, 0.0831, 0.0883, 0.1721, 0.1546, 0.1506,\n",
      "        0.1002, 0.0687, 0.0822, 0.1712, 0.1031, 0.0846, 0.1526, 0.1492,\n",
      "        0.1559, 0.2566, 0.1169, 0.3073, 0.2139, 0.1141, 0.2437, 0.2438,\n",
      "        0.1212, 0.2564, 0.087 , 0.1149, 0.0952, 0.2009, 0.2966, 0.3315,\n",
      "        0.0813, 0.2533, 0.0848, 0.2534, 0.1065, 0.1267, 0.2536, 0.0716,\n",
      "        0.2221, 0.1247, 0.0759, 0.051 , 0.1342, 0.2447, 0.0866, 0.1586,\n",
      "        0.2879, 0.07  , 0.0733, 0.0927, 0.0626, 0.1452, 0.3101, 0.0884,\n",
      "        0.0963, 0.0708, 0.0689, 0.2279, 0.1046, 0.1089, 0.0793, 0.1637,\n",
      "        0.0981, 0.0885, 0.075 , 0.1076, 0.1984, 0.1355, 0.1804, 0.3105,\n",
      "        0.0685, 0.1484, 0.1367, 0.0835, 0.0863, 0.0726, 0.1096, 0.2722,\n",
      "        0.2623, 0.1786, 0.0831, 0.1083, 0.2398, 0.1007, 0.2016, 0.0922,\n",
      "        0.2274, 0.2596, 0.1111, 0.096 , 0.0717, 0.2325, 0.0888, 0.2371,\n",
      "        0.1561, 0.2833, 0.0972, 0.0706, 0.0728, 0.1999, 0.2448, 0.0835,\n",
      "        0.228 , 0.0563, 0.0652, 0.0778, 0.1844, 0.2195, 0.1632, 0.2574,\n",
      "        0.2889, 0.0939, 0.0733, 0.1659, 0.1104, 0.0935, 0.2682, 0.0831,\n",
      "        0.0521, 0.1483, 0.0733, 0.2681, 0.0819, 0.1707, 0.2542, 0.091 ,\n",
      "        0.0559, 0.0548, 0.2362, 0.1135, 0.2611, 0.4017, 0.3568, 0.2314,\n",
      "        0.2465, 0.1216, 0.0834, 0.107 , 0.0826, 0.2622, 0.101 , 0.0653,\n",
      "        0.1222, 0.2199, 0.0705, 0.1138, 0.229 , 0.0631, 0.092 , 0.1566,\n",
      "        0.0946, 0.0945, 0.0875, 0.0786, 0.1573, 0.1121, 0.1471, 0.2425,\n",
      "        0.0783, 0.1105, 0.1121, 0.0607, 0.1135, 0.1103, 0.1612, 0.2611,\n",
      "        0.1193, 0.0775, 0.2517, 0.0536, 0.296 , 0.2965, 0.1049, 0.1429,\n",
      "        0.288 , 0.2407, 0.1332, 0.1746, 0.2274, 0.2441, 0.1174, 0.0778,\n",
      "        0.0744, 0.1399, 0.236 , 0.1092, 0.0831, 0.139 , 0.1345, 0.1394,\n",
      "        0.2486, 0.0869, 0.0689, 0.0568, 0.1742, 0.2091, 0.2853, 0.1315,\n",
      "        0.1791, 0.1109, 0.2784, 0.0707, 0.0525, 0.0976, 0.2573, 0.101 ,\n",
      "        0.078 , 0.1347, 0.2421, 0.1537, 0.0758, 0.0706, 0.0692, 0.245 ,\n",
      "        0.245 , 0.072 , 0.0834, 0.2975, 0.119 , 0.3032, 0.1137, 0.0861,\n",
      "        0.0729, 0.1572, 0.2592, 0.0825, 0.126 , 0.0658, 0.061 , 0.0557,\n",
      "        0.1284, 0.0925, 0.0683, 0.2672, 0.113 , 0.0862, 0.0626, 0.2071,\n",
      "        0.1931, 0.2673, 0.0753, 0.0932, 0.134 , 0.1846, 0.0877, 0.0811,\n",
      "        0.2548, 0.0758, 0.1281, 0.2935, 0.2351, 0.1236, 0.0755, 0.0735,\n",
      "        0.2368, 0.0705, 0.2319, 0.0527, 0.0927, 0.0736, 0.0673, 0.224 ,\n",
      "        0.0827, 0.2452, 0.0607, 0.2089, 0.1715, 0.0832, 0.1056, 0.0827,\n",
      "        0.0831, 0.0789, 0.2209, 0.1933, 0.2371, 0.3201, 0.0876, 0.1246,\n",
      "        0.078 , 0.0725, 0.0692, 0.0574, 0.4951, 0.2549, 0.1343, 0.0834,\n",
      "        0.2171, 0.2625, 0.1484, 0.0599, 0.0752, 0.0653, 0.149 , 0.2496,\n",
      "        0.0757, 0.1609, 0.1777, 0.1175, 0.2463, 0.1347, 0.2362, 0.0942,\n",
      "        0.0952, 0.0689, 0.2227, 0.1341, 0.2514, 0.0642, 0.1509, 0.2287,\n",
      "        0.0838, 0.0497, 0.0865, 0.2274, 0.0795, 0.1647, 0.266 , 0.0833,\n",
      "        0.0725, 0.1176, 0.2661, 0.2726, 0.3139, 0.124 , 0.236 , 0.0963,\n",
      "        0.2103, 0.0872, 0.2328, 0.1197, 0.2199, 0.2314, 0.2899, 0.0871,\n",
      "        0.2099, 0.1395, 0.0964, 0.2768, 0.097 , 0.1843, 0.2444, 0.0831,\n",
      "        0.1936, 0.1187, 0.0888, 0.0791, 0.118 , 0.0949, 0.2224, 0.5013,\n",
      "        0.248 , 0.0848, 0.0869, 0.2737, 0.1172, 0.2072, 0.0885, 0.1703,\n",
      "        0.0791, 0.0577, 0.1494, 0.1075, 0.0801, 0.0795, 0.0702, 0.0526,\n",
      "        0.1804, 0.2602, 0.2954, 0.1157, 0.0912, 0.1067, 0.2212, 0.2584,\n",
      "        0.1422, 0.1631, 0.2114, 0.2284, 0.1639, 0.1172, 0.0747, 0.2638,\n",
      "        0.1333, 0.2549, 0.1015, 0.2161, 0.0961, 0.1629, 0.2459, 0.2603,\n",
      "        0.0956, 0.1677, 0.2808, 0.092 , 0.2327, 0.2234, 0.086 , 0.2431,\n",
      "        0.088 , 0.1818, 0.2986, 0.2077, 0.3035, 0.0963, 0.1224, 0.2678,\n",
      "        0.2146, 0.0839, 0.0703, 0.1501, 0.2506, 0.1148, 0.1057, 0.1018,\n",
      "        0.1411, 0.0786, 0.2245, 0.1498, 0.0964, 0.0783, 0.2372, 0.2936,\n",
      "        0.115 , 0.1739, 0.2754, 0.1037, 0.0781, 0.2902, 0.1154, 0.0785,\n",
      "        0.1464, 0.2845, 0.1004, 0.1237, 0.0658, 0.053 , 0.1039, 0.2672,\n",
      "        0.0913, 0.1055, 0.1092, 0.0741, 0.0756, 0.0595, 0.0688, 0.2534,\n",
      "        0.061 , 0.1233, 0.1008, 0.0788, 0.0685, 0.0586, 0.0654, 0.1106,\n",
      "        0.2051, 0.1446, 0.2717, 0.1586, 0.0861, 0.0686, 0.2246, 0.1213,\n",
      "        0.0924, 0.061 , 0.1034, 0.258 , 0.1088, 0.229 , 0.2195, 0.146 ,\n",
      "        0.2446, 0.1312, 0.2066, 0.2445, 0.2624, 0.1665, 0.1311, 0.207 ,\n",
      "        0.1135, 0.105 , 0.0834, 0.1522, 0.2579, 0.1439, 0.0897, 0.1496,\n",
      "        0.1102, 0.1212, 0.2503, 0.2592, 0.0908, 0.1256, 0.2577, 0.3104,\n",
      "        0.1322, 0.3204, 0.1258, 0.0789, 0.0798, 0.0831, 0.1361, 0.2723,\n",
      "        0.0659, 0.0825, 0.1108, 0.0658, 0.0606, 0.1695, 0.0831, 0.0706,\n",
      "        0.1229, 0.3329, 0.0777, 0.0886, 0.2582, 0.1247, 0.078 , 0.0794,\n",
      "        0.0578, 0.1369, 0.0658, 0.1545, 0.276 , 0.0796, 0.2284, 0.0654,\n",
      "        0.0628, 0.0762, 0.0758, 0.0582, 0.0942, 0.2771, 0.098 , 0.2076,\n",
      "        0.3086, 0.0786, 0.0831, 0.0681, 0.0833, 0.0658, 0.1148, 0.1137,\n",
      "        0.2855, 0.1209, 0.0773, 0.0759, 0.1495, 0.2858, 0.1088, 0.1072,\n",
      "        0.2888, 0.0877, 0.2781, 0.0924, 0.1437, 0.2894, 0.3193, 0.1781,\n",
      "        0.0882, 0.0833, 0.2912, 0.1499, 0.2338, 0.1003, 0.176 , 0.2246,\n",
      "        0.1492, 0.1188, 0.0753, 0.0787, 0.2706, 0.1493, 0.2228, 0.071 ,\n",
      "        0.2467, 0.1635, 0.2583, 0.2473, 0.0943, 0.273 , 0.1533, 0.2653,\n",
      "        0.1197, 0.0945, 0.0683, 0.061 , 0.1598, 0.2888, 0.1717, 0.2736,\n",
      "        0.1333, 0.0784, 0.0935, 0.1149, 0.2085, 0.2643, 0.2559, 0.2465,\n",
      "        0.2579, 0.1117, 0.247 , 0.2588, 0.0909, 0.0682, 0.2246, 0.2359,\n",
      "        0.1894, 0.2599, 0.2575, 0.3412, 0.1614, 0.0801, 0.1666, 0.0804,\n",
      "        0.0609, 0.232 , 0.1715, 0.2024, 0.2146, 0.2771, 0.1879, 0.1344,\n",
      "        0.1778, 0.1455, 0.1009, 0.0881, 0.0701, 0.0912, 0.1706, 0.2116,\n",
      "        0.1485, 0.0921, 0.1477, 0.2258, 0.0644, 0.241 , 0.2109, 0.0735,\n",
      "        0.2363, 0.2605, 0.1234, 0.1272, 0.1947, 0.2898, 0.1264, 0.0959,\n",
      "        0.1919, 0.1458, 0.1137, 0.2566, 0.0576, 0.1006, 0.1857, 0.2066,\n",
      "        0.267 , 0.1493, 0.1005, 0.0687, 0.1002, 0.1962, 0.2974, 0.2797,\n",
      "        0.1446, 0.122 , 0.0807, 0.078 , 0.0825, 0.2543, 0.218 , 0.0607,\n",
      "        0.0756, 0.2271, 0.065 , 0.0902, 0.2316, 0.0928, 0.0832, 0.1387,\n",
      "        0.1006, 0.0782, 0.0704, 0.0578, 0.106 ])\n",
      " array([0.1469, 0.1643, 0.1058, ..., 0.1189, 0.075 , 0.0564])\n",
      " array([0.073 , 0.0734, 0.0584, ..., 0.1482, 0.1081, 0.1395])\n",
      " array([0.0933, 0.1265, 0.1098, ..., 0.1246, 0.1262, 0.1456])\n",
      " array([0.1353, 0.099 , 0.1179, ..., 0.0994, 0.1045, 0.1399])\n",
      " array([0.0638, 0.11  , 0.1146, ..., 0.1033, 0.0755, 0.0914])\n",
      " array([0.237 , 0.0607, 0.1674, ..., 0.0655, 0.0956, 0.0672])\n",
      " array([0.073 , 0.0828, 0.1006, ..., 0.0834, 0.0649, 0.0964])\n",
      " array([0.0964, 0.0963, 0.0879, ..., 0.0745, 0.1052, 0.1146])\n",
      " array([0.0596, 0.0924, 0.171 , ..., 0.0831, 0.1013, 0.1184])\n",
      " array([0.0892, 0.0591, 0.0665, ..., 0.0837, 0.0665, 0.0815])\n",
      " array([0.2195, 0.2326, 0.1748, 0.1544, 0.2424, 0.22  , 0.1974, 0.1966,\n",
      "        0.2506, 0.1039, 0.1655, 0.176 , 0.1933, 0.2068, 0.2019, 0.1686,\n",
      "        0.2104, 0.1811, 0.2136, 0.231 , 0.2717, 0.1969, 0.2092, 0.1668,\n",
      "        0.2768, 0.1815, 0.254 , 0.1573, 0.1792, 0.2098, 0.407 , 0.2114,\n",
      "        0.2807, 0.219 , 0.2279, 0.1944, 0.2247, 0.2203, 0.2327, 0.2725,\n",
      "        0.2099, 0.2846, 0.2148, 0.1795, 0.2238, 0.2038, 0.1892, 0.2329,\n",
      "        0.1855, 0.2165, 0.238 , 0.186 , 0.2199, 0.2026, 0.2212, 0.2549,\n",
      "        0.295 , 0.2624, 0.2067, 0.3626, 0.2506, 0.2667, 0.1912, 0.233 ,\n",
      "        0.1719, 0.189 , 0.1794, 0.3466, 0.334 , 0.2632, 0.228 , 0.2502,\n",
      "        0.1486, 0.2849, 0.1832, 0.2497, 0.2102, 0.3814, 0.485 , 0.245 ,\n",
      "        0.2416, 0.2286, 0.1883, 0.1416, 0.181 , 0.2031, 0.262 , 0.1738,\n",
      "        0.2271, 0.3765, 0.2012, 0.2011, 0.2567, 0.1759, 0.2859, 0.2002,\n",
      "        0.1929, 0.2253, 0.1912, 0.3199, 0.2698, 0.1751, 0.2182, 0.1523,\n",
      "        0.14  , 0.1219, 0.222 , 0.1684, 0.2045, 0.2007, 0.2389, 0.2055,\n",
      "        0.1727, 0.2633, 0.2624, 0.3774, 0.1475, 0.2442, 0.2132, 0.3621,\n",
      "        0.2088, 0.2309, 0.2338, 0.2357, 0.2649, 0.2315, 0.1478, 0.2668,\n",
      "        0.2443, 0.3321, 0.1699, 0.2313, 0.1914, 0.1596, 0.1902, 0.1923,\n",
      "        0.1681, 0.2255, 0.191 , 0.1816, 0.2094, 0.2056, 0.2079, 0.3399,\n",
      "        0.1687, 0.2354, 0.1908, 0.1879, 0.2441, 0.1923, 0.2165, 0.1692,\n",
      "        0.1873, 0.1701, 0.1417, 0.2012, 0.1999, 0.1695, 0.3924, 0.1609,\n",
      "        0.196 , 0.3221, 0.2191, 0.1308, 0.207 , 0.2605, 0.2055, 0.2094,\n",
      "        0.1388, 0.2578, 0.3707, 0.126 , 0.1921, 0.2258, 0.1311, 0.2046,\n",
      "        0.2091, 0.1666, 0.2492, 0.1607, 0.1721, 0.1963, 0.21  , 0.2205,\n",
      "        0.196 , 0.2138, 0.1597, 0.3269, 0.2936, 0.204 , 0.2045, 0.1926,\n",
      "        0.2337, 0.2426, 0.1714, 0.2074, 0.1966, 0.2544, 0.2179, 0.2125,\n",
      "        0.2459, 0.4673, 0.1559, 0.2188, 0.2   , 0.191 , 0.1888, 0.3152,\n",
      "        0.2635, 0.2253, 0.2824, 0.1482, 0.2047, 0.1445, 0.1741, 0.2781,\n",
      "        0.2969, 0.2655, 0.2081, 0.2508, 0.2108, 0.2255, 0.3322, 0.1296,\n",
      "        0.227 , 0.1449, 0.1967, 0.1937, 0.1396, 0.1823, 0.2155, 0.1967,\n",
      "        0.196 , 0.2106, 0.2075, 0.1693, 0.1877, 0.1905, 0.1574, 0.2087,\n",
      "        0.2553, 0.205 , 0.2616, 0.3616, 0.1699, 0.1774, 0.2307, 0.1731,\n",
      "        0.2636, 0.1363, 0.1293, 0.2171, 0.223 , 0.1258, 0.1753, 0.2141,\n",
      "        0.2022, 0.2646, 0.179 , 0.2489, 0.1122, 0.2281, 0.1917, 0.1794,\n",
      "        0.196 , 0.1759, 0.1644, 0.1255, 0.2031, 0.1448, 0.2025, 0.2117,\n",
      "        0.2116, 0.1971, 0.1482, 0.1636, 0.2223, 0.2706, 0.2613, 0.1672,\n",
      "        0.1644, 0.192 , 0.1791, 0.2115, 0.1999, 0.2275, 0.2064, 0.1366,\n",
      "        0.2144, 0.1834, 0.1407, 0.2119, 0.2221, 0.2192, 0.1786, 0.1158,\n",
      "        0.1822, 0.2476, 0.1249, 0.1919, 0.2001, 0.1479, 0.2041, 0.1904,\n",
      "        0.187 , 0.2051, 0.213 , 0.2527, 0.1635, 0.1617, 0.2253, 0.1993,\n",
      "        0.1784, 0.1829, 0.1755, 0.2125, 0.1817, 0.4613, 0.1954, 0.1232,\n",
      "        0.2545, 0.2064, 0.1836, 0.188 , 0.2179, 0.1561, 0.1921, 0.3003,\n",
      "        0.2394, 0.1489, 0.1916, 0.1487, 0.1808, 0.1884, 0.1965, 0.2045,\n",
      "        0.2926, 0.0308, 0.1233, 0.1802, 0.1652, 0.1644, 0.189 , 0.1536,\n",
      "        0.1458, 0.1749, 0.2324, 0.2127, 0.1457, 0.1542, 0.2143, 0.1608,\n",
      "        0.2267, 0.2571, 0.1621, 0.1838, 0.2103, 0.2312, 0.2743, 0.1661,\n",
      "        0.1431, 0.1881, 0.2396, 0.136 , 0.1758, 0.1615, 0.148 , 0.1762,\n",
      "        0.1891, 0.1483, 0.1658, 0.1915, 0.179 , 0.224 , 0.1613, 0.1427,\n",
      "        0.3901, 0.1997, 0.2491, 0.1712, 0.2097, 0.2194, 0.1874, 0.2009,\n",
      "        0.1665, 0.1889, 0.2276, 0.2051, 0.2672, 0.2363, 0.1284, 0.2145,\n",
      "        0.191 , 0.2147, 0.1815, 0.1453, 0.1284, 0.1534, 0.1913, 0.1654,\n",
      "        0.1869, 0.1922, 0.2191, 0.2271, 0.215 , 0.1708, 0.1909, 0.1834,\n",
      "        0.1988, 0.1385, 0.2012, 0.2135, 0.1696, 0.2046, 0.2434, 0.2049,\n",
      "        0.1997, 0.1561, 0.378 , 0.1547, 0.1948, 0.1511, 0.1647, 0.1597,\n",
      "        0.2128, 0.223 , 0.2893, 0.1693, 0.1297, 0.1606, 0.1555, 0.1953,\n",
      "        0.1827, 0.1996, 0.2225, 0.2002, 0.215 , 0.1999, 0.1779, 0.2127,\n",
      "        0.1926, 0.1826, 0.1564, 0.1479, 0.1296, 0.2184, 0.2002, 0.2175,\n",
      "        0.2262, 0.2358, 0.192 , 0.1588, 0.2081, 0.1658, 0.2115, 0.1719,\n",
      "        0.1716, 0.2022, 0.2228, 0.1794, 0.2194, 0.113 , 0.1846, 0.1532,\n",
      "        0.1901, 0.1623, 0.1921, 0.162 , 0.2018, 0.1673, 0.1698, 0.1674,\n",
      "        0.2402, 0.1667, 0.1787, 0.2574, 0.1894, 0.1849, 0.1783, 0.1615,\n",
      "        0.1631, 0.1302, 0.174 , 0.1503, 0.2017, 0.1533, 0.201 , 0.1918,\n",
      "        0.1655, 0.1767, 0.2094, 0.1444, 0.166 , 0.2064, 0.1889, 0.1935,\n",
      "        0.1714, 0.1923, 0.1703, 0.1998, 0.1577, 0.2081, 0.1562, 0.1309,\n",
      "        0.1604, 0.1469, 0.2138, 0.1774, 0.163 , 0.148 , 0.1875, 0.2058,\n",
      "        0.1795, 0.1856, 0.1467, 0.2085, 0.3789, 0.1735, 0.2297, 0.1736,\n",
      "        0.1406, 0.2141, 0.1962, 0.1767, 0.2168, 0.2015, 0.2176, 0.154 ,\n",
      "        0.2093, 0.1959, 0.2047, 0.144 , 0.1452, 0.1913, 0.2198, 0.1657,\n",
      "        0.1305, 0.1309, 0.2571, 0.1959, 0.1835, 0.342 , 0.1653, 0.1435,\n",
      "        0.1827, 0.1656, 0.1613, 0.1531, 0.2106, 0.1836, 0.1433, 0.202 ,\n",
      "        0.1793, 0.1692, 0.219 , 0.1836, 0.1928, 0.1892, 0.2097, 0.1613,\n",
      "        0.184 , 0.1719, 0.2565, 0.1988, 0.292 , 0.2177, 0.1394, 0.148 ,\n",
      "        0.1088, 0.1556, 0.1481, 0.1567, 0.2256, 0.1081, 0.1741, 0.2089,\n",
      "        0.1976, 0.2127, 0.1827, 0.1915, 0.1959, 0.1915, 0.1307, 0.2007,\n",
      "        0.1967, 0.2034, 0.1791, 0.2085, 0.2485, 0.154 , 0.1186, 0.1658,\n",
      "        0.1176, 0.1283, 0.1256, 0.1005, 0.1664, 0.1705, 0.1515, 0.1398,\n",
      "        0.2068, 0.1721, 0.1667, 0.2238, 0.1971, 0.1502, 0.1319, 0.2327,\n",
      "        0.1615, 0.1267, 0.1208, 0.1438, 0.1664, 0.1148, 0.2143, 0.1786,\n",
      "        0.2361, 0.222 , 0.1328, 0.1661, 0.1612, 0.2674, 0.1579, 0.2239,\n",
      "        0.1863, 0.2622, 0.2289, 0.1535, 0.1278, 0.1352, 0.1793, 0.1937,\n",
      "        0.2153, 0.1541, 0.1838, 0.1958, 0.1864, 0.1972, 0.1836, 0.1425,\n",
      "        0.1913, 0.1844, 0.1614, 0.2114, 0.1433, 0.1309, 0.1835, 0.1711,\n",
      "        0.1909, 0.1421, 0.1745, 0.192 , 0.1968, 0.1786, 0.1836, 0.1997,\n",
      "        0.1576, 0.2573, 0.2466, 0.2129, 0.1615, 0.1545, 0.2047, 0.1622,\n",
      "        0.1744, 0.1301])\n",
      " array([0.1494, 0.1281, 0.1263, ..., 0.0883, 0.1066, 0.055 ])\n",
      " array([0.0828, 0.0656, 0.0657, ..., 0.0747, 0.0878, 0.0705])\n",
      " array([0.1311, 0.1301, 0.0829, ..., 0.1498, 0.1014, 0.1101])\n",
      " array([0.2252, 0.1104, 0.1003, 0.1011, 0.1715, 0.101 , 0.1038, 0.1853,\n",
      "        0.2375, 0.1215, 0.2156, 0.122 , 0.2886, 0.0818, 0.1429, 0.1914,\n",
      "        0.1101, 0.2318, 0.1103, 0.2415, 0.1006, 0.2281, 0.098 , 0.1008,\n",
      "        0.0883, 0.1749, 0.1812, 0.2567, 0.274 , 0.1169, 0.1068, 0.1083,\n",
      "        0.2606, 0.0919, 0.113 , 0.0946, 0.0959, 0.0734, 0.1412, 0.1692,\n",
      "        0.1664, 0.1936, 0.1735, 0.1323, 0.1359, 0.2153, 0.1482, 0.2142,\n",
      "        0.1105, 0.1131, 0.2041, 0.2142, 0.1254, 0.1381, 0.1307, 0.1709,\n",
      "        0.3099, 0.1575, 0.127 , 0.2312, 0.1956, 0.1098, 0.1408, 0.1929,\n",
      "        0.1154, 0.1976, 0.1166, 0.2008, 0.2709, 0.1772, 0.3454, 0.2186,\n",
      "        0.1388, 0.171 , 0.1313, 0.205 , 0.152 , 0.2844, 0.1274, 0.1877,\n",
      "        0.1435, 0.126 , 0.2172, 0.166 , 0.1617, 0.1712, 0.1355, 0.1391,\n",
      "        0.1176, 0.2569, 0.167 , 0.1491, 0.2314, 0.1757, 0.2146, 0.1357,\n",
      "        0.1592, 0.1255, 0.3089, 0.1477, 0.1551, 0.1655, 0.1957, 0.1229,\n",
      "        0.2086, 0.1659, 0.1403, 0.1386, 0.139 , 0.1398, 0.1602, 0.1183,\n",
      "        0.1343, 0.1296, 0.1336, 0.2784, 0.1715, 0.1264, 0.1241, 0.1169,\n",
      "        0.1789, 0.1353, 0.1305, 0.1616, 0.1957, 0.2509, 0.0995, 0.1379,\n",
      "        0.1252, 0.2326, 0.1222, 0.212 , 0.1336, 0.1217, 0.1516, 0.2013,\n",
      "        0.2649, 0.131 , 0.2793, 0.1788, 0.1567, 0.1584, 0.2963, 0.1908,\n",
      "        0.2586, 0.2049, 0.2444, 0.1174, 0.1531, 0.2036, 0.1404, 0.1364,\n",
      "        0.1399, 0.1425, 0.3009, 0.1298, 0.2419, 0.1209, 0.2039, 0.1306,\n",
      "        0.123 , 0.1358, 0.1837, 0.1379, 0.1305, 0.1333, 0.1179, 0.2501,\n",
      "        0.1356, 0.1171, 0.2575, 0.1355, 0.1093, 0.1048, 0.217 , 0.1487,\n",
      "        0.4899, 0.1254, 0.0683, 0.2011, 0.1071, 0.1517, 0.125 , 0.1233,\n",
      "        0.2058, 0.2961, 0.1913, 0.1736, 0.1072, 0.2381, 0.1297, 0.1099,\n",
      "        0.2036, 0.1452, 0.1044, 0.1178, 0.1226, 0.0916, 0.2313, 0.1489,\n",
      "        0.142 , 0.1271, 0.1002, 0.2746, 0.1572, 0.1256, 0.0901, 0.1189,\n",
      "        0.1223, 0.1209, 0.1406, 0.3714, 0.3319, 0.1248, 0.1176, 0.1054,\n",
      "        0.1035, 0.1734, 0.125 , 0.1849, 0.1175, 0.105 , 0.1075, 0.1032,\n",
      "        0.1307, 0.1055, 0.2455, 0.1031, 0.154 , 0.1426, 0.2406, 0.196 ,\n",
      "        0.2973, 0.1709, 0.139 , 0.2269, 0.4134, 0.1572, 0.1132, 0.1417,\n",
      "        0.1474, 0.2357, 0.1783, 0.1957, 0.1311, 0.1304, 0.13  , 0.2136,\n",
      "        0.1257, 0.1179, 0.1746, 0.1536, 0.2718, 0.167 , 0.1279, 0.334 ,\n",
      "        0.1214, 0.2368, 0.1257, 0.1513, 0.2266, 0.1193, 0.1141, 0.1009,\n",
      "        0.1831, 0.2449, 0.0998, 0.118 , 0.3198, 0.1058, 0.1239, 0.1226,\n",
      "        0.1538, 0.161 , 0.1456, 0.1083, 0.1351, 0.1329, 0.1557, 0.1965,\n",
      "        0.1525, 0.1342, 0.1269, 0.1221, 0.2037, 0.1574, 0.2615, 0.1909,\n",
      "        0.1393, 0.1434, 0.1178, 0.3315, 0.1027, 0.1193, 0.1734, 0.2272,\n",
      "        0.1491, 0.1212, 0.1466, 0.1736, 0.2312, 0.1255, 0.1514, 0.1477,\n",
      "        0.1314, 0.3436, 0.1128, 0.0918, 0.1701, 0.1432, 0.1464, 0.1001,\n",
      "        0.0879, 0.1004, 0.1257, 0.1545, 0.1135, 0.2655, 0.0872, 0.1306,\n",
      "        0.32  , 0.1575, 0.1082, 0.2628, 0.1352, 0.1483, 0.2119, 0.1123,\n",
      "        0.1204, 0.1119, 0.145 , 0.1357, 0.2352, 0.1391, 0.1591, 0.2599,\n",
      "        0.1456, 0.1211, 0.3203, 0.1183, 0.0887, 0.2247, 0.1239, 0.1134,\n",
      "        0.1617, 0.2575, 0.1051, 0.2475, 0.128 , 0.1265, 0.1323, 0.1138,\n",
      "        0.1139, 0.18  , 0.1082, 0.0898, 0.0996, 0.0871, 0.1957, 0.1224,\n",
      "        0.0862, 0.0993, 0.1067, 0.2003, 0.1956, 0.1144, 0.1049, 0.1   ,\n",
      "        0.148 , 0.2269, 0.1212, 0.2407, 0.1129, 0.2307, 0.2632, 0.1477,\n",
      "        0.1607, 0.1396, 0.2809, 0.1227, 0.245 , 0.1308, 0.1163, 0.2836,\n",
      "        0.1329, 0.14  , 0.1607, 0.2919, 0.2029, 0.1663, 0.1643, 0.2414,\n",
      "        0.1375, 0.0992, 0.1475, 0.2307, 0.13  , 0.2393, 0.2052, 0.1125,\n",
      "        0.1428, 0.1301, 0.234 , 0.1227, 0.143 , 0.1783, 0.1123, 0.1874,\n",
      "        0.2489, 0.1096, 0.105 , 0.2193, 0.1032, 0.158 , 0.1956, 0.1572,\n",
      "        0.135 , 0.2386, 0.1434, 0.1526, 0.1477, 0.2715, 0.1435, 0.1304,\n",
      "        0.1355, 0.1965, 0.1436, 0.1002, 0.1352, 0.1403, 0.2104, 0.1124,\n",
      "        0.1208, 0.1306, 0.138 , 0.249 , 0.1359, 0.2085, 0.1406, 0.1402,\n",
      "        0.161 , 0.2274, 0.0898, 0.143 , 0.17  , 0.1401, 0.1064, 0.2656,\n",
      "        0.153 , 0.1306, 0.5206, 0.2591, 0.1433, 0.1724, 0.1786, 0.1053,\n",
      "        0.1794, 0.1314, 0.1414, 0.2929, 0.2042, 0.3645, 0.2331, 0.178 ,\n",
      "        0.1194, 0.1234, 0.2011, 0.178 , 0.1878, 0.1622, 0.1559, 0.2175,\n",
      "        0.1223, 0.1208, 0.2063, 0.135 , 0.2792, 0.2109, 0.153 , 0.1472,\n",
      "        0.1303, 0.2662, 0.1301, 0.1093, 0.1949, 0.1434, 0.2454, 0.1174,\n",
      "        0.1315, 0.179 , 0.2798, 0.157 , 0.1482, 0.2449, 0.0925, 0.2023,\n",
      "        0.1315, 0.2244, 0.1709, 0.1361, 0.1184, 0.1659, 0.2226, 0.2108,\n",
      "        0.1172, 0.1362, 0.1256, 0.0941, 0.1089, 0.2927, 0.1365, 0.1188,\n",
      "        0.119 , 0.1413, 0.145 , 0.1414, 0.134 , 0.211 , 0.2353, 0.1311,\n",
      "        0.1714, 0.151 , 0.2523, 0.1481, 0.2131, 0.1481, 0.1558, 0.2635,\n",
      "        0.1493, 0.1222, 0.1086, 0.3101, 0.1891, 0.1246, 0.16  , 0.1888,\n",
      "        0.1215, 0.1926, 0.1407, 0.0964, 0.1974, 0.2477, 0.1402, 0.149 ,\n",
      "        0.2641, 0.1447, 0.1745, 0.211 , 0.128 , 0.1202, 0.1588, 0.1491,\n",
      "        0.2348, 0.1256, 0.1399, 0.0645, 0.1765, 0.2311, 0.132 , 0.118 ,\n",
      "        0.2892, 0.1387, 0.1599, 0.2041, 0.1535, 0.1437, 0.1488, 0.1991,\n",
      "        0.2016, 0.0697, 0.1446, 0.35  , 0.1231, 0.1227, 0.1171, 0.1531,\n",
      "        0.1755, 0.1655, 0.1255, 0.1376, 0.1305, 0.1559, 0.225 , 0.1383,\n",
      "        0.1052, 0.1184, 0.0972, 0.1956, 0.1433, 0.1907, 0.2441, 0.1924,\n",
      "        0.1376, 0.1279, 0.1078, 0.1483, 0.1226, 0.1307, 0.1654, 0.2435,\n",
      "        0.0951, 0.0947, 0.1477, 0.153 , 0.2561, 0.1202, 0.1612, 0.1397,\n",
      "        0.1303, 0.2737, 0.1381, 0.1259, 0.1125, 0.1357, 0.1204, 0.1489,\n",
      "        0.3011, 0.1426, 0.1256, 0.2438, 0.2458, 0.1185, 0.1429, 0.1439,\n",
      "        0.3109, 0.1433, 0.2442, 0.2362, 0.1697, 0.1272, 0.1427, 0.2358,\n",
      "        0.1273, 0.2535, 0.148 , 0.2531, 0.1306, 0.1351, 0.1476, 0.1478,\n",
      "        0.1861, 0.1585, 0.1177, 0.2679, 0.1564, 0.1381, 0.2228, 0.156 ,\n",
      "        0.1747, 0.2357, 0.1562, 0.1399, 0.1461, 0.1273, 0.1172, 0.2217,\n",
      "        0.2289, 0.1358, 0.1225, 0.126 , 0.1229, 0.289 , 0.2088, 0.1123,\n",
      "        0.126 , 0.1384, 0.1258, 0.2312, 0.1406, 0.3443, 0.2482, 0.1307,\n",
      "        0.1521, 0.2441, 0.1476, 0.195 , 0.1804, 0.1051, 0.2571, 0.1132,\n",
      "        0.1151, 0.1185, 0.1652, 0.1955, 0.1187, 0.1818, 0.1704, 0.1203,\n",
      "        0.1427, 0.2475, 0.1424, 0.231 , 0.1609, 0.2661, 0.1158, 0.1694,\n",
      "        0.1355, 0.274 , 0.1584, 0.1624, 0.2468, 0.1448, 0.191 , 0.1174,\n",
      "        0.1527, 0.2789, 0.2126, 0.1037, 0.2307, 0.1832, 0.1139, 0.2129,\n",
      "        0.1405, 0.1403, 0.1963, 0.1403, 0.2994, 0.1708, 0.2437, 0.1481,\n",
      "        0.191 , 0.1696, 0.1399, 0.196 , 0.1661, 0.1076, 0.113 , 0.1258,\n",
      "        0.1185, 0.1659, 0.1307, 0.2735, 0.0361, 0.153 , 0.1002, 0.2156,\n",
      "        0.1389, 0.1708, 0.1609, 0.1122, 0.1388, 0.1677, 0.1239, 0.2143,\n",
      "        0.1986, 0.1432, 0.1612, 0.1967, 0.1305, 0.1177, 0.1207, 0.1176,\n",
      "        0.1221, 0.2514, 0.191 , 0.2455, 0.1839, 0.1177, 0.2042, 0.2567,\n",
      "        0.1448, 0.1134, 0.1631, 0.1787, 0.2181, 0.1306, 0.1393, 0.2083,\n",
      "        0.2888, 0.1439, 0.3565, 0.1402, 0.2536, 0.3018, 0.1491, 0.2184,\n",
      "        0.1443, 0.1119, 0.1833, 0.2792, 0.1383, 0.1403, 0.1626, 0.1804,\n",
      "        0.1351, 0.1307, 0.1275, 0.2039, 0.2313, 0.1398, 0.1314, 0.1349,\n",
      "        0.1429, 0.2336, 0.1361, 0.1177, 0.1662, 0.0967, 0.2393, 0.1448,\n",
      "        0.1654, 0.3241, 0.1144, 0.1363, 0.2219, 0.1372, 0.1344, 0.1398,\n",
      "        0.2654, 0.1403, 0.1142, 0.1277, 0.1092, 0.1253, 0.2967, 0.127 ,\n",
      "        0.1314, 0.0954, 0.1241, 0.135 , 0.1437, 0.1107, 0.0958, 0.2662,\n",
      "        0.126 , 0.1134, 0.1072, 0.1404, 0.1578, 0.1352, 0.1922, 0.1292,\n",
      "        0.4933, 0.0856, 0.0914, 0.2309, 0.1212, 0.1173, 0.2396, 0.1152,\n",
      "        0.0924, 0.1589, 0.2616, 0.0299, 0.2438, 0.1307, 0.1216, 0.151 ,\n",
      "        0.1271, 0.1222, 0.2835, 0.1482, 0.2517, 0.0663, 0.0827, 0.0957,\n",
      "        0.1136, 0.1213, 0.3805, 0.1733, 0.1101, 0.2838, 0.2743, 0.1311,\n",
      "        0.1401, 0.2236, 0.1079, 0.1205, 0.1936, 0.1256, 0.1262, 0.1215,\n",
      "        0.1336, 0.2182, 0.1122, 0.1293, 0.1278, 0.1272, 0.2409, 0.1279,\n",
      "        0.1104, 0.3035, 0.1139, 0.1311, 0.1917, 0.1088, 0.1222, 0.1839,\n",
      "        0.1904, 0.2166, 0.0649, 0.1305, 0.2788, 0.13  , 0.1173, 0.1032,\n",
      "        0.2496, 0.2438, 0.1429, 0.1075, 0.135 , 0.1747, 0.1307, 0.135 ,\n",
      "        0.1231, 0.1421, 0.0995, 0.0599, 0.2091, 0.1968, 0.2138, 0.2139,\n",
      "        0.1507, 0.1247, 0.1129, 0.2765, 0.1049, 0.1583, 0.1653, 0.24  ,\n",
      "        0.1259, 0.1226, 0.165 , 0.1481, 0.2542, 0.2313, 0.1476, 0.1481])\n",
      " array([0.1112, 0.0983, 0.1204, ..., 0.0875, 0.1498, 0.1663])\n",
      " array([0.0949, 0.1079, 0.0912, 0.0775, 0.0814, 0.0762, 0.1082, 0.1111,\n",
      "        0.0861, 0.1266, 0.1089, 0.0936, 0.0952, 0.1266, 0.1041, 0.0986,\n",
      "        0.1337, 0.1002, 0.135 , 0.1009, 0.088 , 0.0927, 0.1056, 0.1051,\n",
      "        0.1351, 0.1009, 0.1013, 0.0876, 0.0955, 0.1541, 0.1145, 0.0856,\n",
      "        0.0829, 0.1034, 0.1137, 0.0907, 0.0876, 0.0755, 0.0867, 0.0936,\n",
      "        0.1113, 0.0468, 0.0772, 0.0858, 0.0973, 0.0959, 0.096 , 0.0824,\n",
      "        0.0986, 0.103 , 0.105 , 0.0878, 0.0826, 0.0699, 0.0857, 0.0745,\n",
      "        0.0927, 0.1083, 0.1233, 0.1001, 0.1146, 0.118 , 0.1078, 0.0956,\n",
      "        0.1185, 0.0905, 0.1054, 0.1003, 0.1005, 0.1   , 0.1283, 0.0857,\n",
      "        0.0909, 0.0972, 0.0907, 0.0826, 0.0924, 0.0871, 0.0782, 0.0952,\n",
      "        0.1179, 0.0951, 0.0926, 0.0798, 0.0903, 0.096 , 0.1212, 0.0952,\n",
      "        0.1052, 0.1004, 0.0876, 0.0856, 0.1004, 0.0832, 0.0926, 0.0955,\n",
      "        0.1038, 0.0906, 0.0876, 0.0952, 0.0856, 0.1103, 0.0842, 0.0863,\n",
      "        0.1065, 0.082 , 0.0684, 0.1131, 0.0975, 0.0943, 0.1121, 0.0867,\n",
      "        0.0993, 0.0945, 0.0911, 0.0946, 0.1205, 0.0823, 0.094 , 0.1117,\n",
      "        0.0897, 0.0947, 0.0817, 0.095 , 0.0926, 0.104 , 0.127 , 0.0994,\n",
      "        0.0737, 0.0893, 0.0885, 0.1019, 0.0719, 0.0786, 0.074 , 0.0818,\n",
      "        0.0859, 0.0988, 0.0832, 0.0939, 0.0918, 0.0756, 0.0867, 0.0808,\n",
      "        0.0994, 0.0814, 0.0777, 0.1033, 0.121 , 0.1004, 0.0953, 0.1092,\n",
      "        0.0958, 0.1071, 0.0823, 0.0989, 0.091 , 0.0987, 0.1136, 0.1014,\n",
      "        0.0858, 0.0808, 0.0764, 0.0862, 0.108 , 0.147 , 0.0945, 0.1035,\n",
      "        0.0934, 0.0993, 0.1024, 0.112 , 0.1212, 0.0847, 0.0809, 0.0987,\n",
      "        0.1047, 0.0942, 0.0765, 0.0991, 0.0715, 0.1064, 0.0941, 0.105 ,\n",
      "        0.0944, 0.0986, 0.1207, 0.081 , 0.1165, 0.0888, 0.0812, 0.0851,\n",
      "        0.1006, 0.1004, 0.1054, 0.0829, 0.1134, 0.0887, 0.0923, 0.1009,\n",
      "        0.1081, 0.1037, 0.0877, 0.0959, 0.0832, 0.0862, 0.0832, 0.0961,\n",
      "        0.1232, 0.0962, 0.117 , 0.1079, 0.0961, 0.1038, 0.0958, 0.092 ,\n",
      "        0.1436, 0.0952, 0.0982, 0.078 , 0.0943, 0.0993, 0.1003, 0.0861,\n",
      "        0.0976, 0.1085, 0.0882, 0.0713, 0.1038, 0.1057, 0.0779, 0.1007,\n",
      "        0.1328, 0.0919, 0.1003, 0.101 , 0.1004, 0.0884, 0.1007, 0.0777,\n",
      "        0.0861, 0.1101, 0.0914, 0.0968, 0.0859, 0.0938, 0.0954, 0.0813,\n",
      "        0.0812, 0.0761, 0.094 , 0.1044, 0.0884, 0.0864, 0.0819, 0.0917,\n",
      "        0.1012, 0.1185, 0.1054, 0.1208, 0.1014, 0.1139, 0.0729, 0.087 ,\n",
      "        0.1153, 0.1037, 0.0879, 0.1316, 0.0878, 0.1003, 0.1056, 0.097 ,\n",
      "        0.1235, 0.0885, 0.0776, 0.0918, 0.1417, 0.088 , 0.048 , 0.0831,\n",
      "        0.0762, 0.0927, 0.0913, 0.101 , 0.1058, 0.0885, 0.1105, 0.1572,\n",
      "        0.0943, 0.097 , 0.1397, 0.0904, 0.0918, 0.0815, 0.0866, 0.1   ,\n",
      "        0.1267, 0.0909, 0.1109, 0.0918, 0.0911, 0.1004, 0.0912, 0.1522,\n",
      "        0.1053, 0.0829, 0.0955, 0.1006, 0.0881, 0.1089, 0.0886, 0.0912,\n",
      "        0.0955, 0.0743, 0.0833, 0.0964, 0.1008, 0.1054, 0.0877, 0.0968,\n",
      "        0.0868, 0.0959, 0.11  , 0.0801, 0.118 , 0.1039, 0.0878, 0.0836,\n",
      "        0.0782, 0.0933, 0.1003, 0.0933, 0.0798, 0.1053, 0.0878, 0.0835,\n",
      "        0.0657, 0.0843, 0.1103, 0.0965, 0.0947, 0.12  , 0.0819, 0.0829,\n",
      "        0.088 , 0.0977, 0.1037, 0.1069, 0.0911, 0.0889, 0.0826, 0.0821,\n",
      "        0.0982, 0.0835, 0.1088, 0.0981, 0.0832, 0.0915, 0.0985, 0.0746,\n",
      "        0.0809, 0.0865, 0.0767, 0.0836, 0.0761, 0.1006, 0.0917, 0.0784,\n",
      "        0.0878, 0.0867, 0.1078, 0.0838, 0.0819, 0.103 , 0.082 , 0.0882,\n",
      "        0.0804, 0.0878, 0.088 , 0.1009, 0.1107, 0.149 , 0.091 , 0.1007,\n",
      "        0.0953, 0.1098, 0.0907, 0.1131, 0.1038, 0.0968, 0.08  , 0.0794,\n",
      "        0.0952, 0.0989, 0.121 , 0.0917, 0.0844, 0.0893, 0.0784, 0.0708,\n",
      "        0.0886, 0.091 , 0.1012, 0.0964, 0.0915, 0.0822, 0.1012, 0.0753,\n",
      "        0.0773, 0.0801, 0.1016, 0.106 , 0.0848, 0.0768, 0.0907, 0.0832,\n",
      "        0.076 , 0.093 , 0.0729, 0.0957, 0.115 , 0.1135, 0.095 , 0.0878,\n",
      "        0.096 , 0.1207, 0.107 , 0.0865, 0.0863, 0.1644, 0.1036, 0.0816,\n",
      "        0.0814, 0.0889, 0.1239, 0.0905, 0.1134, 0.0915, 0.0979, 0.078 ,\n",
      "        0.0879, 0.0894, 0.0796, 0.0783, 0.0981, 0.0807, 0.1103, 0.0784,\n",
      "        0.0975, 0.1007, 0.0783, 0.096 , 0.0882, 0.0788, 0.0779, 0.0882,\n",
      "        0.0864, 0.0814, 0.0765, 0.0869, 0.0756, 0.0705, 0.0834, 0.0964,\n",
      "        0.0885, 0.0833, 0.1181, 0.0991, 0.0808, 0.068 , 0.1207, 0.0814,\n",
      "        0.0734, 0.0854, 0.0937, 0.137 , 0.1202, 0.1193, 0.1021, 0.1138,\n",
      "        0.0937, 0.1142, 0.0817, 0.0988, 0.0814, 0.1026, 0.027 , 0.0885,\n",
      "        0.0932, 0.1023, 0.0911, 0.0529, 0.0963, 0.0665, 0.0817, 0.1067,\n",
      "        0.0813, 0.0903, 0.0781, 0.0951, 0.0886, 0.0642, 0.0788, 0.0935,\n",
      "        0.0934, 0.0845, 0.0956, 0.0836, 0.101 , 0.0749, 0.1065, 0.0608,\n",
      "        0.1108, 0.0799, 0.0751, 0.0804, 0.087 , 0.078 , 0.096 , 0.0835,\n",
      "        0.1316, 0.061 , 0.0977, 0.0879, 0.0868, 0.0735, 0.0936, 0.0962,\n",
      "        0.0831, 0.1041, 0.0834, 0.0759, 0.109 , 0.0782, 0.1084, 0.078 ,\n",
      "        0.0828, 0.0883, 0.1143, 0.1262, 0.0798, 0.1001, 0.1122, 0.1131,\n",
      "        0.0996, 0.0931, 0.0735, 0.0906, 0.0906, 0.1354, 0.1141, 0.0959,\n",
      "        0.0959, 0.0901, 0.1028, 0.1138, 0.0989, 0.094 , 0.0894, 0.0906,\n",
      "        0.0879, 0.0888, 0.114 , 0.0986, 0.0754, 0.0999, 0.0786, 0.0988,\n",
      "        0.1032, 0.1053, 0.0906, 0.114 , 0.0972, 0.0742, 0.0733, 0.1008,\n",
      "        0.1326, 0.1005, 0.1081, 0.0899, 0.0928, 0.097 , 0.1004, 0.1212,\n",
      "        0.1032, 0.1036, 0.1009, 0.0975, 0.0878, 0.0931, 0.1105, 0.1031,\n",
      "        0.1004, 0.1047, 0.0879, 0.0623, 0.0892, 0.0818, 0.0585, 0.0738,\n",
      "        0.0944, 0.1139, 0.0688, 0.0966, 0.0992, 0.0872, 0.0905, 0.0829,\n",
      "        0.1215, 0.0829, 0.106 , 0.0745, 0.086 , 0.0707, 0.0835, 0.1013,\n",
      "        0.117 , 0.0959, 0.0743, 0.1189, 0.1264, 0.0607, 0.0934, 0.0963,\n",
      "        0.1158, 0.1038, 0.0832, 0.1236, 0.0836, 0.1063, 0.106 , 0.1092,\n",
      "        0.1088, 0.0961, 0.0863, 0.1191, 0.0913, 0.0879, 0.1104, 0.0778,\n",
      "        0.093 , 0.0879, 0.071 , 0.0915, 0.0702, 0.0828, 0.0865, 0.0966,\n",
      "        0.0962, 0.075 , 0.1056, 0.1129, 0.1168, 0.0788, 0.1142, 0.1059,\n",
      "        0.0956, 0.0759, 0.082 , 0.0942, 0.0969, 0.0924, 0.1007, 0.08  ,\n",
      "        0.1106, 0.0798, 0.118 , 0.0833, 0.1005, 0.1005, 0.0926, 0.0907,\n",
      "        0.1215, 0.0958, 0.0833, 0.0892, 0.1003, 0.0828, 0.0884, 0.1147,\n",
      "        0.0783, 0.1   , 0.1276, 0.0827, 0.1124, 0.0832, 0.0903, 0.1179,\n",
      "        0.0869, 0.1177, 0.1006, 0.0959, 0.0518, 0.0862, 0.1053, 0.1051,\n",
      "        0.083 , 0.0826, 0.0951, 0.1003, 0.1261, 0.0953, 0.0973, 0.0906,\n",
      "        0.1179, 0.1036, 0.1137, 0.1001, 0.0877, 0.0831, 0.0778, 0.0952,\n",
      "        0.0775, 0.0827, 0.1179, 0.0974, 0.1002, 0.1078, 0.0982, 0.0976,\n",
      "        0.0924, 0.091 , 0.0907, 0.0961, 0.0821, 0.1085, 0.0797, 0.0761,\n",
      "        0.1007, 0.0962, 0.099 , 0.0841, 0.1015, 0.0911, 0.0856, 0.0805,\n",
      "        0.0868, 0.0805, 0.1294, 0.097 , 0.1035, 0.1168, 0.0854, 0.1039,\n",
      "        0.0909, 0.1014, 0.0556, 0.0989, 0.0944, 0.1028, 0.1157, 0.0813,\n",
      "        0.1052, 0.0938, 0.1133, 0.0838, 0.1014, 0.1318, 0.1015, 0.081 ,\n",
      "        0.0941, 0.0731, 0.0635, 0.0812, 0.1021, 0.077 , 0.0885, 0.1019,\n",
      "        0.0915, 0.136 , 0.0919, 0.0953, 0.0771, 0.0758, 0.073 , 0.1057,\n",
      "        0.1097, 0.1186, 0.0781, 0.0911, 0.1193, 0.0982, 0.08  , 0.0889,\n",
      "        0.0732, 0.0913, 0.0887, 0.1064, 0.0985, 0.1106, 0.0984, 0.1081,\n",
      "        0.0978, 0.0735, 0.0878, 0.0756, 0.0855, 0.0723, 0.0758, 0.1136,\n",
      "        0.1278, 0.0975, 0.1028, 0.0827, 0.0756, 0.0654, 0.0872, 0.1002,\n",
      "        0.0978, 0.0775, 0.093 , 0.0701, 0.0831, 0.0921, 0.083 , 0.0864,\n",
      "        0.0874, 0.0814, 0.0987, 0.072 , 0.0803, 0.0664, 0.098 , 0.1014,\n",
      "        0.0766, 0.1086, 0.103 , 0.0878, 0.0957, 0.1008, 0.0961, 0.0513,\n",
      "        0.1232, 0.0827, 0.115 , 0.096 , 0.0788, 0.0781, 0.0933, 0.089 ,\n",
      "        0.0964, 0.1006, 0.0835, 0.0865])\n",
      " array([0.2584, 0.1783, 0.1469, 0.1827, 0.1996, 0.2537, 0.185 , 0.2619,\n",
      "        0.19  , 0.2226, 0.1662, 0.1966, 0.2243, 0.1802, 0.1895, 0.1775,\n",
      "        0.1973, 0.2134, 0.4399, 0.2284, 0.1909, 0.1949, 0.175 , 0.1793,\n",
      "        0.1623, 0.1534, 0.1961, 0.1874, 0.1619, 0.1805, 0.2042, 0.1746,\n",
      "        0.2089, 0.1938, 0.1406, 0.1662, 0.1889, 0.1839, 0.2009, 0.1655,\n",
      "        0.2053, 0.2026, 0.1526, 0.1914, 0.1711, 0.1724, 0.1946, 0.1607,\n",
      "        0.1576, 0.2439, 0.1689, 0.2106, 0.178 , 0.21  , 0.1734, 0.1704,\n",
      "        0.1834, 0.1479, 0.2058, 0.1748, 0.2008, 0.1425, 0.2228, 0.1994,\n",
      "        0.1477, 0.1626, 0.211 , 0.1724, 0.1648, 0.1609, 0.1819, 0.1513,\n",
      "        0.1759, 0.1914, 0.153 , 0.2191, 0.1539, 0.1795, 0.1731, 0.1528,\n",
      "        0.2019, 0.2046, 0.3809, 0.1711, 0.1613, 0.21  , 0.1665, 0.1657,\n",
      "        0.219 , 0.1623, 0.1831, 0.1916, 0.2143, 0.1789, 0.1487, 0.1825,\n",
      "        0.1585, 0.2361, 0.1709, 0.2722, 0.219 , 0.1408, 0.159 , 0.1843,\n",
      "        0.1363, 0.2143, 0.235 , 0.2099, 0.1395, 0.1537, 0.1731, 0.2046,\n",
      "        0.158 , 0.179 , 0.1922, 0.2055, 0.1869, 0.171 , 0.1745, 0.1484,\n",
      "        0.156 , 0.1985, 0.1659, 0.1938, 0.1839, 0.1846, 0.1867, 0.1585,\n",
      "        0.1927, 0.1396, 0.1828, 0.1678, 0.1538, 0.1574, 0.162 , 0.1647,\n",
      "        0.1702, 0.1861, 0.1618, 0.1449, 0.1973, 0.1596, 0.182 , 0.1544,\n",
      "        0.158 , 0.1632, 0.159 , 0.1486, 0.1969, 0.1464, 0.1595, 0.1746,\n",
      "        0.1495, 0.18  , 0.1782, 0.1714, 0.1586, 0.1408, 0.1576, 0.2706,\n",
      "        0.187 , 0.1648, 0.1319, 0.3065, 0.1595, 0.1572, 0.1997, 0.1427,\n",
      "        0.1896, 0.1606, 0.1592, 0.162 , 0.1956, 0.1878, 0.1601, 0.2193,\n",
      "        0.1609, 0.2491, 0.1652, 0.1581, 0.1569, 0.2312, 0.156 , 0.1489,\n",
      "        0.2798, 0.2402, 0.21  , 0.1739, 0.1628, 0.232 , 0.1541, 0.1397,\n",
      "        0.1895, 0.1849, 0.2323, 0.1917, 0.1745, 0.1958, 0.1794, 0.1795,\n",
      "        0.2622, 0.2099, 0.1876, 0.3889, 0.254 , 0.2545, 0.3199, 0.2415,\n",
      "        0.176 , 0.2087, 0.149 , 0.1487, 0.1614, 0.1791, 0.3234, 0.2107,\n",
      "        0.1969, 0.1452, 0.175 , 0.2068, 0.171 , 0.1939, 0.1578, 0.1612,\n",
      "        0.3098, 0.1874, 0.201 , 0.183 , 0.1845, 0.2625, 0.223 , 0.1385,\n",
      "        0.2326, 0.167 , 0.2023, 0.2176, 0.1847, 0.239 , 0.3162, 0.181 ,\n",
      "        0.1522, 0.2058, 0.2112, 0.2191, 0.21  , 0.1626, 0.192 , 0.1792,\n",
      "        0.2356, 0.1969, 0.1589, 0.2268, 0.1484, 0.1662, 0.2055, 0.2067,\n",
      "        0.2149, 0.1919, 0.2722, 0.1575, 0.1888, 0.1854, 0.2109, 0.2285,\n",
      "        0.1623, 0.1528, 0.2201, 0.1628, 0.1566, 0.1901, 0.1735, 0.2121,\n",
      "        0.1762, 0.1671, 0.1716, 0.174 , 0.166 , 0.2208, 0.1816, 0.1683,\n",
      "        0.1808, 0.1972, 0.2039, 0.1567, 0.1793, 0.1979, 0.2134, 0.2389,\n",
      "        0.1609, 0.1825, 0.1309, 0.1948, 0.1964, 0.1597, 0.2393, 0.1697,\n",
      "        0.1879, 0.1877, 0.1908, 0.1965, 0.1488, 0.202 , 0.1975, 0.275 ,\n",
      "        0.1443, 0.2006, 0.1676, 0.1659, 0.1439, 0.2408, 0.1536, 0.1888,\n",
      "        0.2313, 0.1825, 0.2346, 0.1704, 0.1241, 0.1909, 0.2447, 0.2174,\n",
      "        0.148 , 0.2158, 0.2005, 0.142 , 0.1693, 0.2357, 0.178 , 0.2455,\n",
      "        0.1856, 0.142 , 0.2278, 0.227 , 0.3061, 0.1237, 0.2063, 0.1968,\n",
      "        0.1844, 0.2114, 0.1535, 0.149 , 0.1364, 0.194 , 0.1921, 0.1985,\n",
      "        0.2627, 0.1657, 0.2458, 0.1618, 0.201 , 0.1266, 0.1316, 0.1711,\n",
      "        0.3053, 0.2191, 0.2196, 0.2221, 0.1449, 0.1793, 0.2497, 0.1769,\n",
      "        0.2183, 0.1692, 0.2256, 0.2614, 0.2459, 0.2269, 0.333 , 0.2282,\n",
      "        0.1761, 0.2098, 0.1604, 0.2487, 0.2401, 0.0644, 0.1977, 0.1439,\n",
      "        0.2011])\n",
      " array([0.1059, 0.1236, 0.0705, ..., 0.1465, 0.0595, 0.0685])\n",
      " array([0.073 , 0.1149, 0.1556, 0.1568, 0.1185, 0.157 , 0.1801, 0.1377,\n",
      "        0.1181, 0.165 , 0.1661, 0.1848, 0.1439, 0.1484, 0.1623, 0.1652,\n",
      "        0.1283, 0.1942, 0.1339, 0.1722, 0.138 , 0.1648, 0.1394, 0.1506,\n",
      "        0.1978, 0.1857, 0.1599, 0.1502, 0.1687, 0.1325, 0.167 , 0.1036,\n",
      "        0.1294, 0.1844, 0.1113, 0.1934, 0.1835, 0.154 , 0.1117, 0.1448,\n",
      "        0.1912, 0.1487, 0.1553, 0.1318, 0.1725, 0.1576, 0.1502, 0.1118,\n",
      "        0.1402, 0.1365, 0.3319, 0.1578, 0.0996, 0.1368, 0.1126, 0.1082,\n",
      "        0.1096, 0.1379, 0.1262, 0.1214, 0.119 , 0.1591, 0.0704, 0.1365,\n",
      "        0.1357, 0.1236, 0.1219, 0.1108, 0.1084, 0.1183, 0.1145, 0.0912,\n",
      "        0.1562, 0.1322, 0.14  , 0.156 , 0.1258, 0.1089, 0.146 , 0.1669,\n",
      "        0.1803, 0.0982, 0.1242, 0.0969, 0.1371, 0.1102, 0.1671, 0.1666,\n",
      "        0.1542, 0.1613, 0.163 , 0.1549, 0.1401, 0.1111, 0.1542, 0.1185,\n",
      "        0.123 , 0.152 , 0.1312, 0.1164, 0.1353, 0.1369, 0.1504, 0.1515,\n",
      "        0.1181, 0.1069, 0.1536, 0.114 , 0.101 , 0.1061, 0.1418, 0.174 ,\n",
      "        0.1578, 0.1641, 0.1971, 0.1674, 0.1722, 0.1267, 0.1357, 0.1186,\n",
      "        0.1237, 0.0798, 0.1316, 0.1721, 0.1543, 0.1498, 0.1729, 0.1446,\n",
      "        0.1355, 0.1411, 0.125 , 0.1498, 0.1337, 0.119 , 0.162 , 0.1371,\n",
      "        0.1014, 0.1395, 0.1676, 0.1855, 0.1316, 0.1045, 0.1317, 0.1623,\n",
      "        0.1594, 0.0834, 0.1137, 0.1331, 0.2202, 0.1371, 0.1348, 0.1329,\n",
      "        0.1692, 0.1696, 0.1585, 0.161 , 0.1483, 0.1711, 0.2098, 0.1531,\n",
      "        0.1426, 0.1003, 0.1847, 0.1484, 0.1081, 0.162 , 0.1483, 0.1318,\n",
      "        0.166 , 0.1568, 0.1263, 0.1665, 0.1623, 0.1393, 0.1353, 0.1545,\n",
      "        0.1801, 0.1543, 0.14  , 0.1667, 0.1349, 0.1631, 0.1261, 0.1693,\n",
      "        0.1622, 0.1205, 0.162 , 0.1748, 0.1523, 0.1289, 0.1362, 0.1667,\n",
      "        0.1937, 0.1577, 0.1571, 0.1618, 0.1872, 0.1805, 0.1157, 0.1416,\n",
      "        0.1542, 0.2155, 0.1262, 0.1618, 0.1938, 0.1534, 0.1142, 0.1236,\n",
      "        0.1369, 0.1841, 0.162 , 0.1623, 0.172 , 0.1186, 0.1445, 0.1383,\n",
      "        0.1632, 0.1279, 0.1615, 0.126 , 0.1539, 0.122 , 0.1409, 0.1126,\n",
      "        0.1137, 0.1754, 0.1971, 0.1456, 0.1748, 0.1082, 0.1717, 0.16  ,\n",
      "        0.158 , 0.1446, 0.1318, 0.1751, 0.1551, 0.1268, 0.1217, 0.1616,\n",
      "        0.1317, 0.1286, 0.1472, 0.1663, 0.1616, 0.1887, 0.1618, 0.1657,\n",
      "        0.1568, 0.149 , 0.1662, 0.1745, 0.1435, 0.1232, 0.1441, 0.1593,\n",
      "        0.2141, 0.1669, 0.1498, 0.1392, 0.1185, 0.1804, 0.2064, 0.1364,\n",
      "        0.154 , 0.1802, 0.1589, 0.1131, 0.1661, 0.1847, 0.1412, 0.1617,\n",
      "        0.1591, 0.141 , 0.1041, 0.1631, 0.1966, 0.1791, 0.1142, 0.1846,\n",
      "        0.1442, 0.1178, 0.1254, 0.1663, 0.1313, 0.1615, 0.1672, 0.1945,\n",
      "        0.1671, 0.198 , 0.1986, 0.1837, 0.1566, 0.1509, 0.1701, 0.1131,\n",
      "        0.1541, 0.1313, 0.1593, 0.1497, 0.1428, 0.1979, 0.1358, 0.1744,\n",
      "        0.1669, 0.1594, 0.141 , 0.1983, 0.192 , 0.1137, 0.126 , 0.1742,\n",
      "        0.1715, 0.1576, 0.1576, 0.1969, 0.1591, 0.1452, 0.1316, 0.1794,\n",
      "        0.1262, 0.1717, 0.2022, 0.1577, 0.1928, 0.193 , 0.1584, 0.1843,\n",
      "        0.2076, 0.1718, 0.1725, 0.1981, 0.1851, 0.1255, 0.1498, 0.2007,\n",
      "        0.1509, 0.1743, 0.1501, 0.1587, 0.1977, 0.1309, 0.1883, 0.1796,\n",
      "        0.238 , 0.1407, 0.2201, 0.1522, 0.1705, 0.1752, 0.1461, 0.1323,\n",
      "        0.1504, 0.2045, 0.1895, 0.1812, 0.2131, 0.1524, 0.1313, 0.1313,\n",
      "        0.1542, 0.1634, 0.1663, 0.1613, 0.1353, 0.1846, 0.1511, 0.1845,\n",
      "        0.1462, 0.193 , 0.1666, 0.188 , 0.212 , 0.1316, 0.1316, 0.1579,\n",
      "        0.1447, 0.1849, 0.1747, 0.1642, 0.1677, 0.1623, 0.1108, 0.101 ,\n",
      "        0.1923, 0.1589, 0.1405, 0.1416, 0.1537, 0.1882, 0.1543, 0.14  ,\n",
      "        0.1394, 0.1398, 0.136 , 0.1621, 0.1892, 0.1546, 0.1719, 0.1439,\n",
      "        0.1524, 0.149 , 0.118 , 0.1566, 0.1566, 0.1397, 0.1062, 0.1543,\n",
      "        0.1251, 0.1357, 0.1488, 0.1109, 0.184 , 0.19  , 0.1797, 0.1632,\n",
      "        0.1595, 0.156 , 0.1493, 0.1986, 0.1237, 0.1227, 0.2074, 0.1846,\n",
      "        0.1596, 0.1545, 0.1624, 0.1407, 0.184 , 0.1646, 0.1716, 0.155 ,\n",
      "        0.1016, 0.1174, 0.1867, 0.1648, 0.1609, 0.1947, 0.1573, 0.1409,\n",
      "        0.1752, 0.1719, 0.1363, 0.1199, 0.2047, 0.1658, 0.1839, 0.1346,\n",
      "        0.1969, 0.1581, 0.1221, 0.1278, 0.1749, 0.1428, 0.1655, 0.1425,\n",
      "        0.1392, 0.1492, 0.1383, 0.1887, 0.161 , 0.1388, 0.1114, 0.1033,\n",
      "        0.1295, 0.199 , 0.1684, 0.1792, 0.1745, 0.1482, 0.1195, 0.1163,\n",
      "        0.1718, 0.2547, 0.147 , 0.1679, 0.1762, 0.136 , 0.1313, 0.15  ,\n",
      "        0.1281, 0.1499, 0.1366, 0.1766, 0.1721, 0.1435, 0.1049, 0.1746,\n",
      "        0.1428, 0.174 , 0.1541, 0.1973, 0.118 , 0.1193, 0.1414, 0.1448,\n",
      "        0.1406, 0.2071, 0.1539, 0.1898, 0.1462, 0.1359, 0.1439, 0.1408,\n",
      "        0.1732, 0.118 , 0.1262, 0.1729, 0.1404, 0.1719, 0.1546, 0.184 ,\n",
      "        0.1463, 0.1749, 0.1195, 0.1359, 0.1406, 0.1637, 0.1641, 0.1446,\n",
      "        0.1559, 0.2327, 0.1755, 0.1772, 0.1662, 0.1312, 0.1473, 0.1734,\n",
      "        0.112 , 0.1391, 0.1368, 0.1115, 0.1837, 0.1825, 0.1331, 0.1394,\n",
      "        0.1379, 0.1173, 0.1202, 0.1167, 0.1483, 0.1781, 0.1931, 0.1612,\n",
      "        0.1821, 0.1807, 0.1572, 0.1718, 0.1535, 0.1709, 0.1619, 0.1711,\n",
      "        0.1492, 0.1563, 0.1718, 0.2143, 0.1599, 0.1751, 0.1383, 0.1353,\n",
      "        0.1428, 0.0695, 0.1496, 0.2033, 0.1666, 0.1925, 0.1246, 0.1791,\n",
      "        0.1355, 0.1345, 0.126 , 0.1706, 0.1492, 0.149 , 0.1973, 0.1673,\n",
      "        0.1497, 0.1902, 0.1596, 0.1317, 0.1921, 0.2529, 0.1484, 0.1943,\n",
      "        0.1256, 0.1622, 0.1736, 0.1556, 0.1647, 0.173 , 0.1384, 0.1788,\n",
      "        0.2015, 0.1816, 0.215 , 0.131 , 0.1698, 0.1769, 0.15  , 0.1572,\n",
      "        0.184 , 0.1617, 0.1715, 0.1743, 0.1763, 0.2152, 0.1897, 0.1739,\n",
      "        0.2051, 0.1589, 0.1666, 0.1594, 0.1876, 0.1717, 0.1412, 0.1357,\n",
      "        0.1713, 0.1196, 0.1307, 0.1264, 0.1311, 0.1839, 0.1767, 0.1719,\n",
      "        0.1702, 0.1569, 0.1655, 0.1847, 0.1596, 0.1579, 0.1587, 0.1891,\n",
      "        0.1792, 0.1666, 0.1668, 0.1841, 0.1541, 0.1859, 0.149 , 0.1711,\n",
      "        0.1972, 0.1371, 0.1301, 0.1646, 0.1879, 0.1741, 0.1388, 0.1702,\n",
      "        0.1959, 0.1947, 0.1472, 0.1741, 0.1523, 0.1744, 0.1669, 0.2053,\n",
      "        0.1663, 0.1667, 0.2491, 0.1845, 0.1846, 0.1236, 0.1214, 0.1984,\n",
      "        0.1696])\n",
      " array([0.2473, 0.2035, 0.245 , 0.1773, 0.1653, 0.1676, 0.2046, 0.1724,\n",
      "        0.1302, 0.1969, 0.1781, 0.17  , 0.1213, 0.1835, 0.1406, 0.1753,\n",
      "        0.1598, 0.1653, 0.218 , 0.2965, 0.1955, 0.1607, 0.1698, 0.1123,\n",
      "        0.1612, 0.227 , 0.1422, 0.1428, 0.153 , 0.1885, 0.1828, 0.1374,\n",
      "        0.1596, 0.1418, 0.1297, 0.1657, 0.1779, 0.2359, 0.156 , 0.1374,\n",
      "        0.1207, 0.1247, 0.1282, 0.1337, 0.1557, 0.1129, 0.1299, 0.1561,\n",
      "        0.1413, 0.137 , 0.1896, 0.1028, 0.1184, 0.1712, 0.1392, 0.1543,\n",
      "        0.191 , 0.1177, 0.1446, 0.152 , 0.1683, 0.1354, 0.1037, 0.1532,\n",
      "        0.1441, 0.1563, 0.1848, 0.1741, 0.166 , 0.1597, 0.1078, 0.2239,\n",
      "        0.1527, 0.0955, 0.1861, 0.1567, 0.2019, 0.1275, 0.1093, 0.1562,\n",
      "        0.1542, 0.184 , 0.1219, 0.1959, 0.1481, 0.1766, 0.1578, 0.143 ,\n",
      "        0.2198, 0.2105, 0.1844, 0.2054, 0.1889, 0.1139, 0.103 , 0.1022,\n",
      "        0.1669, 0.2283, 0.1316, 0.1447, 0.1505, 0.1887, 0.1279, 0.0958,\n",
      "        0.0881, 0.1856, 0.1668, 0.1125, 0.119 , 0.1769, 0.1281, 0.1989,\n",
      "        0.1504, 0.1146, 0.1313, 0.1308, 0.1409, 0.2798, 0.1597, 0.1675,\n",
      "        0.1721, 0.1053, 0.1578, 0.1531, 0.1361, 0.1369, 0.2141, 0.1571,\n",
      "        0.1266, 0.1064, 0.1453, 0.1554, 0.1703, 0.1495, 0.2422, 0.1289,\n",
      "        0.15  , 0.1631, 0.2695, 0.1465, 0.1444, 0.1489, 0.1614, 0.1547,\n",
      "        0.1344, 0.1476, 0.1093, 0.1828, 0.2011, 0.169 , 0.1263, 0.1621,\n",
      "        0.1498, 0.1807, 0.1022, 0.1645, 0.1677, 0.1723, 0.1571, 0.2324,\n",
      "        0.1985, 0.1461, 0.3476, 0.123 , 0.1228, 0.1374, 0.1673, 0.1523,\n",
      "        0.2077, 0.2623, 0.1294, 0.1748, 0.1302, 0.1602, 0.1041, 0.1228,\n",
      "        0.0997, 0.1149, 0.1552, 0.1439, 0.1622, 0.1671, 0.0985, 0.1488,\n",
      "        0.1181, 0.1709, 0.1895, 0.1235, 0.1276, 0.114 , 0.1046, 0.1358,\n",
      "        0.1491, 0.1544, 0.2044, 0.1783, 0.1491, 0.1713, 0.1178, 0.1058,\n",
      "        0.1278, 0.179 , 0.2325, 0.1678, 0.1421, 0.1459, 0.1825, 0.2041,\n",
      "        0.1445, 0.1312, 0.1715, 0.1542, 0.1282, 0.1114, 0.117 , 0.1497,\n",
      "        0.1261, 0.1394, 0.1977, 0.1539, 0.147 , 0.2272, 0.1574, 0.1632,\n",
      "        0.1181, 0.1718, 0.1315, 0.2457, 0.2018, 0.2019, 0.1512, 0.1714,\n",
      "        0.1449, 0.0911, 0.1712, 0.1796, 0.1451, 0.1316, 0.199 , 0.1353,\n",
      "        0.1544, 0.1468, 0.1266, 0.0825, 0.1412, 0.2598, 0.1536, 0.2729,\n",
      "        0.1412, 0.211 , 0.2067, 0.224 , 0.1702, 0.1785, 0.1267, 0.1445,\n",
      "        0.1485, 0.1362, 0.1215, 0.2406, 0.1487, 0.1042, 0.1821, 0.209 ,\n",
      "        0.1796, 0.1932, 0.3555, 0.1236, 0.1331, 0.1268, 0.1634, 0.1447,\n",
      "        0.2021, 0.2093, 0.2042, 0.1188, 0.1385, 0.1919, 0.1271, 0.1719,\n",
      "        0.2448, 0.1518, 0.1598, 0.2033, 0.1715, 0.1123, 0.1689, 0.1208,\n",
      "        0.2269, 0.1177, 0.1036, 0.2095, 0.1083, 0.096 , 0.1843, 0.1141,\n",
      "        0.1226, 0.1238, 0.1348])\n",
      " array([0.0601, 0.0819, 0.0837, ..., 0.0924, 0.0809, 0.1448])\n",
      " array([0.1575, 0.1787, 0.1819, 0.1528, 0.1845, 0.1577, 0.1842, 0.1769,\n",
      "        0.1717, 0.1465, 0.1775, 0.1942, 0.1443, 0.1398, 0.1552, 0.1426,\n",
      "        0.143 , 0.1935, 0.157 , 0.1582, 0.1471, 0.1434, 0.1901, 0.1938,\n",
      "        0.1198, 0.181 , 0.1769, 0.1729, 0.1952, 0.1647, 0.1775, 0.1633,\n",
      "        0.141 , 0.2259, 0.186 , 0.1897, 0.2151, 0.1678, 0.1773, 0.1854,\n",
      "        0.2024, 0.1745, 0.1615, 0.1662, 0.1788, 0.1571, 0.1364, 0.1678,\n",
      "        0.1713, 0.2067, 0.1862, 0.231 , 0.1994, 0.1858, 0.2066, 0.1683,\n",
      "        0.1701, 0.1668, 0.1857, 0.1759, 0.1591, 0.1419, 0.1402, 0.1675,\n",
      "        0.1501, 0.1855, 0.1766, 0.1645, 0.132 , 0.1498, 0.1729, 0.2394,\n",
      "        0.155 , 0.1631, 0.2337, 0.1864, 0.1784, 0.1724, 0.1995, 0.1774,\n",
      "        0.1802, 0.1459, 0.1718, 0.1666, 0.1602, 0.1733, 0.1569, 0.2143,\n",
      "        0.1788, 0.2102, 0.1125, 0.1482, 0.1171, 0.1533, 0.1789, 0.1667,\n",
      "        0.1675, 0.1764, 0.1172, 0.1631, 0.1371, 0.1671, 0.1491, 0.157 ,\n",
      "        0.1974, 0.1584, 0.1384, 0.1972, 0.2066, 0.1608, 0.1564, 0.1663,\n",
      "        0.1825, 0.16  , 0.197 , 0.1741, 0.2165, 0.1692, 0.1551, 0.1535,\n",
      "        0.1526, 0.1513, 0.1942, 0.2158, 0.1718, 0.18  , 0.1142, 0.138 ,\n",
      "        0.1476, 0.1849, 0.1542, 0.1272, 0.1451, 0.1058, 0.2107, 0.1542,\n",
      "        0.137 , 0.1451, 0.1646, 0.155 , 0.1616, 0.1491, 0.1662, 0.177 ,\n",
      "        0.1623, 0.1759, 0.1243, 0.1571, 0.1229, 0.1442, 0.1787, 0.1698,\n",
      "        0.1616, 0.1723, 0.1667, 0.1592, 0.1543, 0.1391, 0.1535, 0.163 ,\n",
      "        0.1932, 0.1847, 0.1619, 0.1672, 0.1712, 0.1722, 0.1622, 0.1702,\n",
      "        0.2155, 0.1806, 0.1545, 0.2075, 0.1763, 0.1264, 0.1411, 0.1655,\n",
      "        0.1875, 0.2509, 0.1481, 0.16  , 0.1591, 0.1664, 0.1313, 0.185 ,\n",
      "        0.1897, 0.1675, 0.1664, 0.1805, 0.1766, 0.1329, 0.1047, 0.123 ,\n",
      "        0.1483, 0.1626, 0.1611, 0.1471, 0.1653, 0.132 , 0.1545, 0.1671,\n",
      "        0.1351, 0.1786, 0.16  , 0.16  , 0.1579, 0.181 , 0.1535, 0.1768,\n",
      "        0.1626, 0.149 , 0.1666, 0.1692, 0.1589, 0.2155, 0.1488, 0.1591,\n",
      "        0.2244, 0.154 , 0.1528, 0.2025, 0.1936, 0.1631, 0.1568, 0.1414,\n",
      "        0.2007, 0.1571, 0.1351, 0.1421, 0.1498, 0.1849, 0.1814, 0.1582,\n",
      "        0.1361, 0.1726, 0.1444, 0.1334, 0.174 , 0.1946, 0.1455, 0.1675,\n",
      "        0.1935, 0.1636, 0.1327, 0.1353, 0.1501, 0.2072, 0.1637, 0.1857,\n",
      "        0.1511, 0.1468, 0.1823, 0.176 , 0.1729, 0.1421, 0.1277, 0.2298,\n",
      "        0.1452, 0.1437, 0.1907, 0.1841, 0.145 , 0.1286, 0.1784, 0.1597,\n",
      "        0.1666, 0.2328, 0.1238, 0.2226, 0.1786, 0.164 , 0.1321, 0.1535,\n",
      "        0.1412, 0.1662, 0.1803, 0.1533, 0.2076, 0.1703, 0.1624, 0.1533,\n",
      "        0.1847, 0.1468, 0.2271, 0.2122, 0.1372, 0.1845, 0.2108, 0.1623,\n",
      "        0.1843, 0.1738, 0.1409, 0.1591, 0.1181, 0.1572, 0.1704, 0.1444,\n",
      "        0.1141, 0.154 , 0.1549, 0.1716, 0.1589, 0.1447, 0.1561, 0.1499,\n",
      "        0.2101, 0.1621, 0.2193, 0.2033, 0.18  , 0.1713, 0.1543, 0.2156,\n",
      "        0.1488, 0.1493, 0.1657, 0.2149, 0.2281, 0.2019, 0.187 , 0.1416,\n",
      "        0.1326, 0.1632])\n",
      " array([0.1297, 0.0997, 0.1728, ..., 0.0784, 0.0886, 0.0742])\n",
      " array([0.1   , 0.0856, 0.0988, ..., 0.0758, 0.0627, 0.0757])\n",
      " array([0.0514, 0.0358, 0.0455, ..., 0.0475, 0.0638, 0.0529])\n",
      " array([0.0657, 0.17  , 0.1631, ..., 0.113 , 0.0523, 0.0654])\n",
      " array([0.1285, 0.1785, 0.1813, ..., 0.1618, 0.1065, 0.0801])\n",
      " array([0.1074, 0.2225, 0.1763, ..., 0.1165, 0.1371, 0.0742])\n",
      " array([0.267 , 0.1814, 0.1664, 0.1925, 0.1543, 0.1411, 0.1565, 0.1461,\n",
      "        0.1239, 0.154 , 0.2385, 0.1314, 0.1436, 0.1768, 0.1938, 0.141 ,\n",
      "        0.1368, 0.1188, 0.1449, 0.1733, 0.2657, 0.1782, 0.1691, 0.1602,\n",
      "        0.1564, 0.1507, 0.1795, 0.157 , 0.1586, 0.1764, 0.1571, 0.1616,\n",
      "        0.2009, 0.1521, 0.1382, 0.1854, 0.1564, 0.1623, 0.1666, 0.1577,\n",
      "        0.1466, 0.1767, 0.2423, 0.1892, 0.1447, 0.1544, 0.2168, 0.24  ,\n",
      "        0.1664, 0.2069, 0.1939, 0.1581, 0.1125, 0.1442, 0.1712, 0.1617,\n",
      "        0.1187, 0.1379, 0.1572, 0.1615, 0.1359, 0.215 , 0.1136, 0.1493,\n",
      "        0.1946, 0.1318, 0.1797, 0.1667, 0.1725, 0.0934, 0.1497, 0.1275,\n",
      "        0.1777, 0.1402, 0.1304, 0.1659, 0.167 , 0.1587, 0.1968, 0.1744,\n",
      "        0.1564, 0.1315, 0.1187, 0.1707, 0.1671, 0.1793, 0.1489, 0.184 ,\n",
      "        0.1386, 0.1461, 0.1664, 0.2034, 0.1961, 0.157 , 0.2135, 0.1402,\n",
      "        0.1734, 0.1648, 0.2139, 0.1473, 0.1653, 0.165 , 0.1651, 0.2438,\n",
      "        0.2084, 0.1655, 0.1575, 0.1948, 0.148 , 0.1515, 0.1596, 0.1433,\n",
      "        0.1568, 0.2127, 0.1521, 0.1601, 0.2279, 0.1182, 0.1375, 0.1637,\n",
      "        0.1796, 0.1387, 0.1411, 0.1522, 0.1846, 0.192 , 0.1266, 0.1773,\n",
      "        0.1547, 0.2155, 0.1486, 0.124 , 0.1392, 0.1622, 0.1762, 0.2142,\n",
      "        0.1588, 0.1585, 0.1796, 0.1652, 0.1713, 0.1572, 0.175 , 0.1442,\n",
      "        0.1542, 0.1886, 0.1534, 0.1809, 0.1391, 0.1408, 0.1312, 0.1764,\n",
      "        0.205 , 0.1771, 0.1386, 0.1393, 0.1563, 0.1968, 0.2066, 0.1702,\n",
      "        0.1311, 0.1591, 0.1748, 0.1356, 0.1487, 0.2105, 0.1671, 0.1588,\n",
      "        0.1537, 0.2081, 0.1484, 0.1473, 0.1873, 0.1609, 0.1457, 0.1293,\n",
      "        0.1902, 0.1594, 0.3332, 0.1837, 0.1636, 0.1976, 0.1712, 0.1535,\n",
      "        0.1493, 0.1996, 0.1451, 0.1485, 0.1847, 0.1483, 0.1395, 0.1853,\n",
      "        0.149 , 0.1601, 0.1677, 0.1896, 0.1345, 0.113 , 0.1571, 0.1851,\n",
      "        0.1392, 0.1395, 0.1367, 0.1314, 0.1719, 0.1394, 0.1485, 0.1846,\n",
      "        0.1585, 0.139 , 0.1954, 0.1517, 0.1648, 0.1655, 0.1562, 0.1823,\n",
      "        0.1981, 0.1472, 0.1972, 0.2095, 0.1359, 0.1567, 0.1469, 0.1448,\n",
      "        0.1879, 0.1425, 0.1524, 0.1885, 0.1666, 0.1841, 0.21  , 0.162 ,\n",
      "        0.2196, 0.1567, 0.1184, 0.1753, 0.1591, 0.1484, 0.1619, 0.1419,\n",
      "        0.1694, 0.1668, 0.1892, 0.1744, 0.1521, 0.1245, 0.1637, 0.1796,\n",
      "        0.179 , 0.1592, 0.1698, 0.1419, 0.1777, 0.112 , 0.2107, 0.1751,\n",
      "        0.2067, 0.1495, 0.16  , 0.235 , 0.1485, 0.1669, 0.1137, 0.1418,\n",
      "        0.1606, 0.0917, 0.1819, 0.1878, 0.1187, 0.1536, 0.1713, 0.163 ,\n",
      "        0.1187, 0.1267, 0.1253, 0.1974, 0.1825, 0.1299, 0.1746, 0.1403,\n",
      "        0.1509, 0.1577, 0.1294, 0.1967, 0.1398, 0.1994, 0.1341, 0.1671,\n",
      "        0.1949, 0.1466, 0.1802, 0.1193, 0.1819, 0.1545, 0.2004, 0.1402,\n",
      "        0.1693, 0.2027, 0.1254, 0.1424, 0.1425, 0.2627, 0.1877, 0.2016,\n",
      "        0.1944, 0.172 , 0.159 , 0.1579, 0.2075, 0.1347, 0.1489, 0.1462,\n",
      "        0.1844, 0.197 , 0.149 , 0.1673, 0.1167, 0.1316, 0.1698, 0.1791,\n",
      "        0.1526, 0.1896, 0.1845, 0.1664, 0.1767, 0.175 , 0.1343, 0.2051,\n",
      "        0.2224, 0.2373, 0.078 , 0.1792, 0.2098, 0.1901, 0.1907, 0.1844,\n",
      "        0.2133, 0.0893, 0.2216, 0.1778, 0.1248, 0.1255, 0.1562, 0.1909,\n",
      "        0.1828, 0.1933, 0.1857, 0.1477, 0.1661, 0.2408, 0.157 , 0.1594,\n",
      "        0.1694, 0.16  , 0.1523, 0.1736, 0.175 , 0.1555, 0.169 , 0.1859,\n",
      "        0.13  , 0.1788, 0.1354, 0.1948, 0.1778, 0.1381, 0.216 , 0.1368,\n",
      "        0.1602, 0.2068, 0.197 , 0.225 , 0.2371, 0.1928, 0.1443, 0.1582,\n",
      "        0.2238, 0.1444, 0.1389, 0.1892, 0.2018, 0.2023, 0.2062, 0.2033,\n",
      "        0.1825, 0.1691, 0.1406, 0.1587, 0.1716, 0.1428, 0.2252, 0.1759,\n",
      "        0.1637, 0.1692, 0.2051, 0.1475, 0.1294, 0.1759, 0.1664, 0.1797,\n",
      "        0.1645, 0.1677, 0.1346, 0.1525, 0.1416, 0.167 , 0.1928, 0.2906,\n",
      "        0.194 , 0.1355, 0.1395, 0.1803, 0.1765, 0.155 , 0.1898, 0.1764,\n",
      "        0.1944, 0.2157, 0.1669, 0.1451, 0.1418, 0.137 , 0.1624, 0.2755,\n",
      "        0.2073, 0.1669, 0.1723, 0.2148, 0.1495, 0.1775, 0.1951, 0.1806,\n",
      "        0.1462, 0.1639, 0.1621, 0.2025, 0.1194, 0.1497, 0.0907, 0.1615,\n",
      "        0.2417, 0.1395, 0.1708, 0.1319, 0.1321, 0.1674, 0.1571, 0.1973,\n",
      "        0.1863, 0.1725, 0.1589, 0.1972, 0.1317, 0.1623, 0.1755, 0.2714,\n",
      "        0.1357, 0.1713, 0.1594, 0.185 , 0.1115, 0.1284, 0.1144, 0.167 ,\n",
      "        0.1749, 0.1847, 0.1852, 0.138 , 0.1719, 0.2149, 0.1623, 0.1433,\n",
      "        0.1591, 0.1789, 0.1877, 0.1648, 0.1846, 0.2457, 0.1896, 0.1296,\n",
      "        0.1318, 0.1398, 0.1664, 0.2   , 0.3137, 0.1976, 0.2129, 0.1168,\n",
      "        0.206 , 0.2168, 0.1569, 0.1872, 0.1847, 0.1471, 0.1506, 0.0973,\n",
      "        0.1572, 0.1856, 0.1622, 0.1209, 0.2001, 0.2072, 0.139 , 0.1762,\n",
      "        0.2374, 0.1409, 0.0962, 0.107 , 0.1367, 0.2454, 0.1748, 0.2506,\n",
      "        0.1907, 0.1722, 0.1731, 0.1921, 0.1734, 0.1775, 0.1651, 0.1902,\n",
      "        0.1574, 0.1547, 0.167 , 0.1558, 0.1441, 0.1299, 0.1528, 0.1868,\n",
      "        0.1893, 0.1865, 0.1645, 0.2055, 0.1428, 0.165 , 0.1385, 0.1209,\n",
      "        0.1584, 0.1872, 0.1924, 0.1584, 0.1215, 0.1245, 0.2057, 0.1631,\n",
      "        0.1747, 0.1673, 0.1703, 0.2101, 0.1187, 0.1721, 0.2156, 0.1751,\n",
      "        0.1492, 0.1153, 0.1445, 0.1922, 0.1667, 0.149 , 0.1317, 0.1537,\n",
      "        0.2017, 0.1625, 0.1413, 0.1487, 0.2327, 0.1813, 0.17  , 0.2022,\n",
      "        0.1624, 0.167 , 0.1754, 0.1664, 0.1491, 0.1276, 0.1236, 0.1083,\n",
      "        0.1118, 0.1798, 0.1543, 0.1482, 0.1761, 0.1888, 0.1969, 0.1387,\n",
      "        0.1664, 0.1591, 0.1538, 0.1497, 0.1805, 0.1357, 0.1622, 0.1488,\n",
      "        0.2013, 0.1946, 0.1789, 0.1589, 0.1699, 0.1016, 0.1388, 0.1164,\n",
      "        0.1775, 0.2525, 0.1294, 0.1476, 0.1552, 0.1876, 0.1045, 0.1555,\n",
      "        0.1525, 0.1724, 0.1256, 0.1526, 0.139 , 0.186 , 0.1431, 0.2129,\n",
      "        0.175 , 0.1697, 0.1145, 0.1703, 0.1911, 0.155 , 0.1796, 0.2566,\n",
      "        0.1696, 0.1647, 0.1351, 0.1686, 0.2053, 0.1984, 0.196 , 0.1831,\n",
      "        0.1843, 0.1619, 0.1101, 0.1668, 0.1972, 0.1312, 0.1486, 0.1394,\n",
      "        0.1448, 0.1426, 0.1617, 0.1679, 0.1393, 0.1475, 0.2122, 0.1628,\n",
      "        0.1616, 0.1543, 0.1816, 0.1404, 0.1539, 0.1843, 0.1585, 0.2022,\n",
      "        0.194 , 0.1972, 0.1412, 0.1287, 0.1041, 0.2018, 0.1668, 0.2015,\n",
      "        0.1797, 0.2355, 0.2053, 0.2151, 0.2281, 0.1238, 0.1742, 0.1697,\n",
      "        0.2454, 0.1292, 0.1723, 0.1478, 0.1764, 0.1792, 0.1933, 0.2062,\n",
      "        0.1486, 0.1519, 0.1622, 0.2316, 0.1924, 0.1943, 0.1711, 0.2291,\n",
      "        0.1411, 0.2326, 0.1578, 0.2101, 0.1823, 0.1424, 0.1737, 0.1792,\n",
      "        0.1583, 0.181 , 0.1322, 0.1646, 0.157 , 0.1519, 0.1777, 0.1992,\n",
      "        0.1374, 0.1805, 0.1828, 0.147 , 0.1157, 0.155 , 0.1904, 0.2228,\n",
      "        0.1602, 0.1389, 0.1337, 0.1532, 0.1489, 0.1498, 0.2455, 0.2134,\n",
      "        0.1972, 0.1526, 0.1601, 0.1796, 0.1881, 0.1342, 0.1995, 0.2073,\n",
      "        0.1294, 0.1692, 0.1636, 0.2017, 0.1238, 0.1967, 0.1896, 0.1322,\n",
      "        0.1516, 0.1639, 0.1467, 0.1552, 0.1558, 0.1375, 0.1298, 0.1679,\n",
      "        0.334 , 0.1457, 0.1867, 0.1853, 0.1515, 0.1898, 0.1825])\n",
      " array([0.0843, 0.0695, 0.0467, 0.0689, 0.1526, 0.0695, 0.0916, 0.1083,\n",
      "        0.1296, 0.0914, 0.1458, 0.0999, 0.1352, 0.0763, 0.0991, 0.1526,\n",
      "        0.1083, 0.1328, 0.1012, 0.1538, 0.0908, 0.1886, 0.0794, 0.0699,\n",
      "        0.0622, 0.1421, 0.1055, 0.1786, 0.1455, 0.0731, 0.0761, 0.1024,\n",
      "        0.1327, 0.0748, 0.0781, 0.0895, 0.109 , 0.0819, 0.1121, 0.1433,\n",
      "        0.1638, 0.0754, 0.0963, 0.0782, 0.0959, 0.083 , 0.0777, 0.0904,\n",
      "        0.0961, 0.123 , 0.1512, 0.0749, 0.0683, 0.0741, 0.0793, 0.119 ,\n",
      "        0.0852, 0.1524, 0.251 , 0.0587, 0.1247, 0.0819, 0.0993, 0.1142,\n",
      "        0.0814, 0.0914, 0.1303, 0.082 , 0.1091, 0.0801, 0.087 , 0.1082,\n",
      "        0.121 , 0.0747, 0.0859, 0.1496, 0.0833, 0.1145, 0.1149, 0.1313,\n",
      "        0.0881, 0.0686, 0.1007, 0.1671, 0.0837, 0.0957, 0.1438, 0.0924,\n",
      "        0.1083, 0.088 , 0.0472, 0.1151, 0.0521, 0.0926, 0.1223, 0.0596,\n",
      "        0.0956, 0.1123, 0.085 , 0.1076, 0.08  , 0.0756, 0.0702, 0.0657,\n",
      "        0.1003, 0.129 , 0.1657, 0.0636, 0.0859, 0.0946, 0.1359, 0.0913,\n",
      "        0.109 , 0.1391, 0.0916, 0.0865, 0.1023, 0.103 , 0.0894, 0.1473,\n",
      "        0.0981, 0.0595, 0.1246, 0.0925, 0.1037, 0.066 , 0.141 , 0.1268,\n",
      "        0.1761, 0.1019, 0.0481, 0.0598, 0.0537, 0.0648, 0.11  , 0.1259,\n",
      "        0.1106, 0.1618, 0.1286, 0.0582, 0.0825, 0.1126, 0.0963, 0.0751,\n",
      "        0.118 , 0.1141, 0.1168, 0.1007, 0.0717, 0.0705, 0.162 , 0.0659,\n",
      "        0.078 , 0.1258, 0.0805, 0.1326, 0.0804, 0.0651, 0.1136, 0.0674,\n",
      "        0.0978, 0.0826, 0.1389, 0.0959, 0.0789, 0.0931, 0.1458, 0.1408,\n",
      "        0.075 , 0.0638, 0.0701, 0.1008, 0.0735, 0.1435, 0.1054, 0.1231,\n",
      "        0.0977, 0.1486, 0.0959, 0.0861, 0.0863, 0.1416, 0.1522, 0.0774,\n",
      "        0.0951, 0.1083, 0.1613, 0.0892, 0.1103, 0.1311, 0.1227, 0.1311,\n",
      "        0.0831, 0.1238, 0.1209, 0.1054, 0.0954, 0.1408, 0.0556, 0.0795,\n",
      "        0.0935, 0.1299, 0.086 , 0.1359, 0.0787, 0.1431, 0.0613, 0.0648,\n",
      "        0.1165, 0.0691, 0.127 , 0.1214, 0.0733, 0.0873, 0.1049, 0.062 ,\n",
      "        0.0893, 0.0837, 0.1055, 0.0959, 0.0623, 0.0704, 0.0821, 0.1104,\n",
      "        0.1063, 0.0609, 0.1026, 0.0982, 0.1228, 0.0756, 0.136 , 0.1064,\n",
      "        0.0659, 0.0832, 0.1185, 0.0831, 0.0868, 0.0908, 0.097 , 0.0903,\n",
      "        0.0825, 0.1007, 0.1059, 0.0708, 0.0669, 0.0702, 0.0553, 0.0754,\n",
      "        0.065 , 0.0822, 0.1056, 0.113 , 0.0914, 0.0776, 0.071 , 0.1191,\n",
      "        0.0964, 0.1181, 0.1148, 0.0627, 0.0812, 0.1461, 0.157 , 0.0589,\n",
      "        0.1171, 0.1314, 0.0835, 0.081 , 0.0748, 0.1059, 0.1353, 0.1366,\n",
      "        0.173 , 0.1038, 0.1245, 0.0578, 0.0543, 0.1247, 0.0759, 0.1427,\n",
      "        0.091 , 0.0935, 0.1611, 0.0796, 0.1356, 0.09  , 0.1488, 0.0865,\n",
      "        0.1376, 0.0893, 0.1243, 0.0552, 0.0758, 0.0842, 0.1113, 0.1191,\n",
      "        0.0737, 0.0987, 0.1312, 0.0756, 0.0814, 0.0878, 0.1312, 0.1055,\n",
      "        0.1353, 0.0724, 0.0553, 0.1278, 0.0656, 0.0702, 0.0763, 0.0825,\n",
      "        0.1304, 0.117 , 0.1111, 0.0555, 0.0808, 0.1086, 0.0782, 0.0961,\n",
      "        0.1308, 0.0645, 0.0988, 0.1376, 0.0764, 0.0877, 0.1025, 0.1345,\n",
      "        0.0805, 0.0457, 0.0995, 0.0722, 0.0689, 0.0806, 0.0866, 0.0814,\n",
      "        0.0958, 0.1354, 0.1039, 0.0627, 0.0458, 0.067 , 0.0523, 0.0905,\n",
      "        0.1186, 0.0645, 0.0688, 0.0645, 0.1344, 0.0694, 0.0873, 0.0701,\n",
      "        0.0669, 0.1125, 0.1082, 0.0723, 0.0919, 0.1405, 0.0777, 0.1133,\n",
      "        0.1078, 0.0714, 0.0668, 0.0662, 0.0659, 0.1303, 0.0725, 0.0695,\n",
      "        0.0823, 0.0552, 0.0877, 0.1348, 0.0749, 0.0861, 0.1311, 0.1262,\n",
      "        0.0699, 0.1357, 0.0743, 0.0697, 0.0702, 0.1002, 0.1165, 0.0736,\n",
      "        0.1131, 0.1182, 0.131 , 0.1278, 0.0962, 0.1107, 0.1234, 0.1061,\n",
      "        0.0732, 0.1435, 0.0922, 0.1007, 0.1182, 0.1155, 0.0606, 0.0905,\n",
      "        0.057 , 0.1309, 0.1013, 0.104 , 0.1204, 0.074 , 0.0698, 0.0676,\n",
      "        0.1055, 0.0874, 0.0784, 0.1107, 0.1009, 0.0834, 0.0615, 0.0929,\n",
      "        0.1008, 0.1011, 0.1267, 0.0782, 0.0795, 0.0704, 0.0824, 0.056 ,\n",
      "        0.0834, 0.0881, 0.083 , 0.121 , 0.1005, 0.1266, 0.0704, 0.1323,\n",
      "        0.0787, 0.1343, 0.1119, 0.1088, 0.1068, 0.0641, 0.0642, 0.0737,\n",
      "        0.1706, 0.0613, 0.0733, 0.0938, 0.1356, 0.0632, 0.0788, 0.0831,\n",
      "        0.0825, 0.1362, 0.1403, 0.0692, 0.0652, 0.0817, 0.0801, 0.1409,\n",
      "        0.0725, 0.0828, 0.0609, 0.0692, 0.0578, 0.0648, 0.1104, 0.1081,\n",
      "        0.0972, 0.0952, 0.1133, 0.1001, 0.0803, 0.0782, 0.0963, 0.1138,\n",
      "        0.1309, 0.0723, 0.1517, 0.2042, 0.06  , 0.073 , 0.0711, 0.0989,\n",
      "        0.0764, 0.1154, 0.0741, 0.1063, 0.094 , 0.0515, 0.0827, 0.0634,\n",
      "        0.1177, 0.0799, 0.1305, 0.1039, 0.1108, 0.066 , 0.1008, 0.1364,\n",
      "        0.0878, 0.0753, 0.1323, 0.0913, 0.0691, 0.0876, 0.1142, 0.0607,\n",
      "        0.0952, 0.1053, 0.1284, 0.0608, 0.0837, 0.0955, 0.0906, 0.0877,\n",
      "        0.1265, 0.1135, 0.0853, 0.0914, 0.1146, 0.065 , 0.1129, 0.1096,\n",
      "        0.0605, 0.0834, 0.1311, 0.1019, 0.1049, 0.1356, 0.0748, 0.1253,\n",
      "        0.0833, 0.0525, 0.0655, 0.1005, 0.1341, 0.0826, 0.1276, 0.0698,\n",
      "        0.0877, 0.0701, 0.1309, 0.0776, 0.1338, 0.0877, 0.1421, 0.0777,\n",
      "        0.1483, 0.0802, 0.1187, 0.0583, 0.1089, 0.0909, 0.0879, 0.1065,\n",
      "        0.0911, 0.124 , 0.0729, 0.0828, 0.1066, 0.1114, 0.086 , 0.1086,\n",
      "        0.1439, 0.0748, 0.1207, 0.0797, 0.0699, 0.1267, 0.0707, 0.1309,\n",
      "        0.0787, 0.1051, 0.1059, 0.0836, 0.1076, 0.0921, 0.0787, 0.0816,\n",
      "        0.0626, 0.0867, 0.123 , 0.0869, 0.1008, 0.104 , 0.0878, 0.1228,\n",
      "        0.0778, 0.1264, 0.0445, 0.1058, 0.0571, 0.0573, 0.0698, 0.1003,\n",
      "        0.0876, 0.1282, 0.1607, 0.0702, 0.1055, 0.1304, 0.1005, 0.0752,\n",
      "        0.0833, 0.0922, 0.0993, 0.109 , 0.0953, 0.1127, 0.1515, 0.0574,\n",
      "        0.0785, 0.106 , 0.1212, 0.078 , 0.0729, 0.0554, 0.0701, 0.1107,\n",
      "        0.085 , 0.0983, 0.0581, 0.0608, 0.0652, 0.0837, 0.1136, 0.0795,\n",
      "        0.0888, 0.0974, 0.0833, 0.1079, 0.0797, 0.0649, 0.0788, 0.0651,\n",
      "        0.1434, 0.0689, 0.0685, 0.1091, 0.1149, 0.0616, 0.0698, 0.0605,\n",
      "        0.1007, 0.148 , 0.0561, 0.0708, 0.066 , 0.1168, 0.115 , 0.1289,\n",
      "        0.0623, 0.0755, 0.0846, 0.1146, 0.0823, 0.0702, 0.0863, 0.1308,\n",
      "        0.1091, 0.0607, 0.0995, 0.0645, 0.0649, 0.0901, 0.1516, 0.083 ,\n",
      "        0.1355, 0.0523, 0.0512, 0.0737, 0.1205, 0.067 , 0.0684, 0.1479,\n",
      "        0.0701, 0.0595, 0.0578, 0.0567, 0.1223, 0.0717, 0.0817, 0.1216,\n",
      "        0.0513, 0.0895, 0.0986, 0.056 , 0.0639, 0.0565, 0.0721, 0.1301,\n",
      "        0.0692, 0.0695, 0.152 , 0.0976, 0.0915, 0.0983, 0.1608, 0.1421,\n",
      "        0.0717, 0.0958, 0.0886, 0.1049, 0.0624, 0.0781, 0.1534, 0.0726,\n",
      "        0.1132, 0.1141, 0.1488, 0.0651, 0.0704, 0.0679, 0.1003, 0.0484,\n",
      "        0.065 , 0.1404, 0.1219, 0.1014, 0.1743, 0.1089, 0.116 , 0.066 ,\n",
      "        0.069 , 0.0991, 0.1339, 0.2071, 0.1362, 0.0536, 0.1149, 0.1067,\n",
      "        0.0832, 0.1258, 0.1456, 0.1175, 0.0953, 0.1869, 0.111 , 0.1122,\n",
      "        0.1433, 0.1011, 0.1665, 0.1423, 0.0647, 0.1437, 0.1007, 0.1462,\n",
      "        0.0729, 0.1189, 0.0627, 0.0733, 0.0708, 0.1145, 0.1009, 0.0926,\n",
      "        0.1059, 0.0952, 0.1057, 0.1285, 0.0672, 0.1006, 0.1103, 0.1195,\n",
      "        0.1008, 0.1488, 0.0528, 0.0708, 0.0738, 0.1515, 0.088 , 0.0709,\n",
      "        0.0659, 0.0605, 0.0607, 0.0588, 0.0682, 0.1073, 0.1547, 0.0686,\n",
      "        0.091 , 0.1348, 0.069 , 0.137 , 0.1474, 0.0608, 0.0915, 0.1069,\n",
      "        0.1199, 0.0685, 0.0734, 0.061 , 0.1039, 0.0876, 0.0729, 0.0757,\n",
      "        0.1455, 0.0623, 0.0694, 0.1004, 0.1335, 0.1456, 0.0731, 0.1061,\n",
      "        0.0583, 0.0742, 0.0673, 0.0637, 0.1258, 0.0762, 0.0756, 0.089 ,\n",
      "        0.1207, 0.0927, 0.1004, 0.1297, 0.1805, 0.0781, 0.1153, 0.09  ,\n",
      "        0.0931, 0.1373, 0.0862, 0.0613, 0.2047, 0.0703, 0.0796, 0.1005,\n",
      "        0.1055])\n",
      " array([0.1713, 0.1432, 0.0655, ..., 0.074 , 0.1655, 0.0827])\n",
      " array([0.0895, 0.0954, 0.1738, ..., 0.1805, 0.1844, 0.1446])\n",
      " array([0.1119, 0.1345, 0.3396, ..., 0.1775, 0.3274, 0.1813])\n",
      " array([0.1194, 0.1337, 0.1847, ..., 0.1554, 0.1494, 0.1183])\n",
      " array([0.1528, 0.208 , 0.2084, ..., 0.1945, 0.2004, 0.1557])\n",
      " array([0.1269, 0.1766, 0.1843, ..., 0.1686, 0.1313, 0.1855])\n",
      " array([0.1516, 0.2453, 0.1762, ..., 0.0798, 0.2365, 0.131 ])\n",
      " array([0.0838, 0.0876, 0.0978, ..., 0.0867, 0.1354, 0.2532])\n",
      " array([0.0557, 0.0758, 0.0704, ..., 0.0835, 0.0757, 0.0533])\n",
      " array([0.1753, 0.1835, 0.1707, ..., 0.2444, 0.1715, 0.1709])\n",
      " array([0.1428, 0.2186, 0.1533, ..., 0.1401, 0.0962, 0.1062])\n",
      " array([0.0577, 0.066 , 0.0654, ..., 0.0284, 0.0513, 0.064 ])\n",
      " array([0.1342, 0.1804, 0.2044, ..., 0.5156, 0.3767, 0.1868])\n",
      " array([0.143 , 0.128 , 0.1128, ..., 0.0523, 0.0611, 0.0683])\n",
      " array([0.1342, 0.1258, 0.0939, ..., 0.1575, 0.1324, 0.1571])\n",
      " array([0.1441, 0.1045, 0.0895, ..., 0.2377, 0.1111, 0.1338])\n",
      " array([0.1233, 0.078 , 0.0901, ..., 0.065 , 0.0603, 0.1015])\n",
      " array([0.1048, 0.088 , 0.1005, ..., 0.1149, 0.1002, 0.0826])\n",
      " array([0.0803, 0.0725, 0.124 , ..., 0.0871, 0.0982, 0.053 ])\n",
      " array([0.0713, 0.1311, 0.0792, ..., 0.161 , 0.0749, 0.0716])\n",
      " array([0.1366, 0.1065, 0.1074, ..., 0.0865, 0.0661, 0.093 ])\n",
      " array([0.0906, 0.0726, 0.1019, ..., 0.0996, 0.0855, 0.0951])\n",
      " array([0.0633, 0.0753, 0.1626, ..., 0.1117, 0.1076, 0.1399])\n",
      " array([0.1792, 0.1546, 0.1283, ..., 0.1421, 0.1025, 0.111 ])\n",
      " array([0.1584, 0.2889, 0.1568, ..., 0.091 , 0.2622, 0.0582])\n",
      " array([0.0888, 0.0947, 0.1346, ..., 0.1205, 0.199 , 0.1121])\n",
      " array([0.0823, 0.1657, 0.1174, ..., 0.0918, 0.0915, 0.1214])\n",
      " array([0.158 , 0.0621, 0.1037, ..., 0.1259, 0.0742, 0.0351])\n",
      " array([0.2817, 0.1244, 0.1983, ..., 0.1619, 0.1839, 0.1009])\n",
      " array([0.1759, 0.1583, 0.293 , ..., 0.3192, 0.3687, 0.1052])\n",
      " array([0.1122, 0.0822, 0.0872, ..., 0.0773, 0.0941, 0.0737])]\n",
      "[1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 1 0 0 1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "C:\\Users\\micoa\\AppData\\Local\\Temp\\ipykernel_11016\\4195563876.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_filtered = np.array(X_filtered)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X1,X2),axis=0)\n",
    "y=  np.concatenate((early_stage['gt'],de_novo[\"gt\"]),axis=0)\n",
    "X_filtered =[]\n",
    "y_filtered =[]\n",
    "\n",
    "\n",
    "for i,e in enumerate(X):\n",
    "    if len(e)>1:\n",
    "        X_filtered.append(e)\n",
    "        y_filtered.append(y[i])\n",
    "min = len(X_filtered[0])\n",
    "for i,e in enumerate(X_filtered):\n",
    "    if len(e)<min:\n",
    "        min = len(e)\n",
    "        print(i)\n",
    "X_filtered = np.array(X_filtered)\n",
    "y_filtered = np.array(y_filtered)\n",
    "X_lens = [len(e) for e in X_filtered]\n",
    "avg_len = np.mean(X_lens)\n",
    "print(avg_len)\n",
    "print(min)\n",
    "print(len(X_filtered),len(y_filtered))\n",
    "print(X_filtered)\n",
    "print(y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91950d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11633377817478088\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "flat_list = list(itertools.chain(*X_filtered))\n",
    "value = sum(flat_list) / len(flat_list)\n",
    "print(value)\n",
    "X_filtered =sequence.pad_sequences(X_filtered,dtype='float32',padding='post',maxlen=6000,value =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5c7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622a0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_filtered,y_filtered,test_size=0.17,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae63e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 6000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0794f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.46910757]\n",
      "  [0.12045828]\n",
      "  [0.21827409]\n",
      "  ...\n",
      "  [1.        ]\n",
      "  [1.        ]\n",
      "  [1.        ]]\n",
      "\n",
      " [[0.06598015]\n",
      "  [0.04680851]\n",
      "  [0.03299493]\n",
      "  ...\n",
      "  [1.        ]\n",
      "  [1.        ]\n",
      "  [1.        ]]\n",
      "\n",
      " [[0.44088477]\n",
      "  [0.2752864 ]\n",
      "  [0.05301748]\n",
      "  ...\n",
      "  [1.        ]\n",
      "  [1.        ]\n",
      "  [1.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.46109837]\n",
      "  [0.35090014]\n",
      "  [0.31754088]\n",
      "  ...\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.16895501]\n",
      "  [0.08674306]\n",
      "  [0.1469261 ]\n",
      "  ...\n",
      "  [1.        ]\n",
      "  [1.        ]\n",
      "  [1.        ]]\n",
      "\n",
      " [[0.01678108]\n",
      "  [0.07463175]\n",
      "  [0.10434292]\n",
      "  ...\n",
      "  [1.        ]\n",
      "  [1.        ]\n",
      "  [1.        ]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], int(X_train.shape[1]),1))\n",
    "print(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], int(X_test.shape[1]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c51390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afdac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc816cc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 6000, 100)         40800     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6000, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 6000, 100)         80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6000, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 6000, 100)         80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6000, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 282,101\n",
      "Trainable params: 282,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.6942 - accuracy: 0.5286\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6933 - accuracy: 0.4714\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6935 - accuracy: 0.4857\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6940 - accuracy: 0.4857\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6966 - accuracy: 0.4714\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6894 - accuracy: 0.5571\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6937 - accuracy: 0.4714\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6968 - accuracy: 0.5714\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6946 - accuracy: 0.5143\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6941 - accuracy: 0.5143\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6879 - accuracy: 0.5286\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6894 - accuracy: 0.6000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6904 - accuracy: 0.4857\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6938 - accuracy: 0.4857\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6826 - accuracy: 0.5571\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6930 - accuracy: 0.5143\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6885 - accuracy: 0.5000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7137 - accuracy: 0.4143\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6990 - accuracy: 0.4857\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6828 - accuracy: 0.6143\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6981 - accuracy: 0.4429\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7011 - accuracy: 0.4714\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6999 - accuracy: 0.4429\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6882 - accuracy: 0.5143\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7081 - accuracy: 0.4429\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6843 - accuracy: 0.5429\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6855 - accuracy: 0.5143\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6911 - accuracy: 0.5286\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6872 - accuracy: 0.6000\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6869 - accuracy: 0.5429\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.7027 - accuracy: 0.4286\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6970 - accuracy: 0.4429\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6874 - accuracy: 0.6143\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6959 - accuracy: 0.4714\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6871 - accuracy: 0.5857\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6923 - accuracy: 0.5286\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6958 - accuracy: 0.5143\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6941 - accuracy: 0.5429\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6976 - accuracy: 0.4857\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6846 - accuracy: 0.5000\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6902 - accuracy: 0.5429\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6939 - accuracy: 0.4714\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6915 - accuracy: 0.5286\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6885 - accuracy: 0.5000\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6862 - accuracy: 0.4857\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6849 - accuracy: 0.5857\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6889 - accuracy: 0.5143\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6816 - accuracy: 0.5714\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6898 - accuracy: 0.5000\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6822 - accuracy: 0.5857\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6811 - accuracy: 0.5429\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6896 - accuracy: 0.4857\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6866 - accuracy: 0.5286\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6857 - accuracy: 0.5143\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6831 - accuracy: 0.5000\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6850 - accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6946 - accuracy: 0.4714\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6824 - accuracy: 0.4857\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6862 - accuracy: 0.5143\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6811 - accuracy: 0.6000\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6846 - accuracy: 0.5286\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6867 - accuracy: 0.5286\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6943 - accuracy: 0.4143\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6872 - accuracy: 0.5143\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6865 - accuracy: 0.5000\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6857 - accuracy: 0.5286\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6831 - accuracy: 0.5714\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6829 - accuracy: 0.5000\n",
      "Epoch 69/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 0.6879 - accuracy: 0.4143\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6824 - accuracy: 0.5429\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6883 - accuracy: 0.4857\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6839 - accuracy: 0.5143\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6825 - accuracy: 0.4714\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6793 - accuracy: 0.5571\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6822 - accuracy: 0.5714\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6797 - accuracy: 0.5714\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6870 - accuracy: 0.5000\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6834 - accuracy: 0.5286\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6823 - accuracy: 0.5143\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6783 - accuracy: 0.5857\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6825 - accuracy: 0.5286\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6836 - accuracy: 0.5143\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6804 - accuracy: 0.5429\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6780 - accuracy: 0.5429\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6925 - accuracy: 0.4857\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6876 - accuracy: 0.4286\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6882 - accuracy: 0.5000\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6765 - accuracy: 0.6000\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6868 - accuracy: 0.4857\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6871 - accuracy: 0.5286\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6872 - accuracy: 0.5143\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6874 - accuracy: 0.5429\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6801 - accuracy: 0.5143\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6857 - accuracy: 0.5429\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6842 - accuracy: 0.5143\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6817 - accuracy: 0.5000\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6845 - accuracy: 0.5000\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6832 - accuracy: 0.5429\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6811 - accuracy: 0.5000\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6822 - accuracy: 0.5143\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6845 - accuracy: 0.4714\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6797 - accuracy: 0.5143\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6885 - accuracy: 0.5000\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6812 - accuracy: 0.5571\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6904 - accuracy: 0.4429\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6805 - accuracy: 0.5571\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6856 - accuracy: 0.5143\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6886 - accuracy: 0.5143\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6855 - accuracy: 0.5286\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6837 - accuracy: 0.4571\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6783 - accuracy: 0.6000\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6819 - accuracy: 0.5571\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6859 - accuracy: 0.4571\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6818 - accuracy: 0.5429\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6802 - accuracy: 0.5429\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6847 - accuracy: 0.5571\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6860 - accuracy: 0.4857\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6804 - accuracy: 0.5000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6854 - accuracy: 0.5143\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6843 - accuracy: 0.5571\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6872 - accuracy: 0.5000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6816 - accuracy: 0.5429\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6869 - accuracy: 0.4429\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6856 - accuracy: 0.4571\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6807 - accuracy: 0.5571\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6796 - accuracy: 0.5714\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6835 - accuracy: 0.5143\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6831 - accuracy: 0.5429\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6904 - accuracy: 0.4286\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6830 - accuracy: 0.5143\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6826 - accuracy: 0.5000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6857 - accuracy: 0.4857\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6858 - accuracy: 0.5571\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6803 - accuracy: 0.5286\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6810 - accuracy: 0.6000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6839 - accuracy: 0.5286\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6828 - accuracy: 0.5571\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6827 - accuracy: 0.4714\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6825 - accuracy: 0.5571\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6865 - accuracy: 0.4714\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6818 - accuracy: 0.5714\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6887 - accuracy: 0.4571\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6857 - accuracy: 0.4857\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6895 - accuracy: 0.4143\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6855 - accuracy: 0.4000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6842 - accuracy: 0.4571\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6800 - accuracy: 0.5571\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6849 - accuracy: 0.4857\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6832 - accuracy: 0.5571\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6832 - accuracy: 0.5286\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6885 - accuracy: 0.5286\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6823 - accuracy: 0.5429\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 0.6838 - accuracy: 0.5143\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6817 - accuracy: 0.5714\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6808 - accuracy: 0.5857\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6803 - accuracy: 0.5857\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6824 - accuracy: 0.5857\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6837 - accuracy: 0.4714\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6861 - accuracy: 0.4857\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6826 - accuracy: 0.5000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6811 - accuracy: 0.5571\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6828 - accuracy: 0.5000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6877 - accuracy: 0.4000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6848 - accuracy: 0.4857\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6815 - accuracy: 0.5286\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6788 - accuracy: 0.4714\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6791 - accuracy: 0.5714\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6840 - accuracy: 0.5286\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6798 - accuracy: 0.5857\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6831 - accuracy: 0.4857\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6855 - accuracy: 0.4714\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6848 - accuracy: 0.4571\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6829 - accuracy: 0.4857\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6877 - accuracy: 0.4429\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6796 - accuracy: 0.5571\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6962 - accuracy: 0.4000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6859 - accuracy: 0.4714\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6889 - accuracy: 0.4857\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6826 - accuracy: 0.5714\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6871 - accuracy: 0.5000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6833 - accuracy: 0.5000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6830 - accuracy: 0.4714\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6848 - accuracy: 0.5000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6871 - accuracy: 0.4714\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6795 - accuracy: 0.5857\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6840 - accuracy: 0.5000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6856 - accuracy: 0.4571\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6861 - accuracy: 0.4857\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6848 - accuracy: 0.4857\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6849 - accuracy: 0.4857\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6844 - accuracy: 0.4857\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6864 - accuracy: 0.5143\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6814 - accuracy: 0.5286\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6831 - accuracy: 0.5143\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6865 - accuracy: 0.4143\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6832 - accuracy: 0.5286\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6863 - accuracy: 0.4714\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6808 - accuracy: 0.5857\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6803 - accuracy: 0.5857\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6809 - accuracy: 0.5429\n",
      "Accuracy: 46.67%\n",
      "Train: 0.514, Test: 0.467\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "clf = Sequential()\n",
    "clf.add(LSTM(units = 100,return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units = 100,return_sequences = True))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units = 100,return_sequences = True))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units =100 ))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "adm = tf.keras.optimizers.Adam()\n",
    "clf.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "print(clf.summary())\n",
    "\n",
    "\n",
    "\n",
    "history=clf.fit(X_train, y_train, epochs=200, batch_size=70)\n",
    "scores = clf.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "train_acc = clf.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = clf.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train loss')\n",
    "pyplot.plot(history.history['accuracy'], label='train accuracy')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e44e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "def LSTM_Network(neurons=30,dropout=0.2):\n",
    "\n",
    "    clf = Sequential()\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units =neurons))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(Dense(units = 100,activation='relu'))\n",
    "    clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "    adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    clf.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "    \n",
    "    print(clf.summary())\n",
    "    return clf\n",
    "lstm_clf = KerasClassifier(build_fn=LSTM_Network, epochs=300, batch_size=70, verbose=0)\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a575c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores1=evaluate_model(lstm_clf,X_filtered,y_filtered)\n",
    "# results = [scores1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d0752b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pyplot.boxplot(results, labels=['RNN'], showmeans=True)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d084232f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#     clf.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#     clf.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#     clf.add(Dropout(0.25))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#     clf.add(MaxPooling1D(pool_size=2))\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     clf\u001b[38;5;241m.\u001b[39madd(LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,input_shape\u001b[38;5;241m=\u001b[39m(X_traincv\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m---> 26\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     clf\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.2\u001b[39m))\n\u001b[0;32m     28\u001b[0m     clf\u001b[38;5;241m.\u001b[39madd(LSTM(units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,return_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py:530\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 530\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\engine\\sequential.py:217\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_explicit_input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[0;32m    215\u001b[0m   \u001b[38;5;66;03m# If the model is being built continuously on top of an input layer:\u001b[39;00m\n\u001b[0;32m    216\u001b[0m   \u001b[38;5;66;03m# refresh its output.\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   output_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(output_tensor)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\layers\\recurrent.py:659\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m inputs, initial_state, constants \u001b[38;5;241m=\u001b[39m _standardize_args(inputs,\n\u001b[0;32m    654\u001b[0m                                                      initial_state,\n\u001b[0;32m    655\u001b[0m                                                      constants,\n\u001b[0;32m    656\u001b[0m                                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_constants)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m constants \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 659\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(RNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;66;03m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m additional_inputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\engine\\base_layer.py:976\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _in_functional_construction_mode(\u001b[38;5;28mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 976\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functional_construction_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m call_context \u001b[38;5;241m=\u001b[39m base_layer_utils\u001b[38;5;241m.\u001b[39mcall_context()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\engine\\base_layer.py:1114\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     training_arg_passed_by_framework \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m call_context\u001b[38;5;241m.\u001b[39menter(\n\u001b[0;32m   1112\u001b[0m     layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs, build_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39mtraining_value):\n\u001b[0;32m   1113\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1114\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_tensor_symbolic_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA layer\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms `call` method should return a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1119\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensor or a list of Tensors, not None \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1120\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(layer: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\engine\\base_layer.py:848\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(keras_tensor\u001b[38;5;241m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 848\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_output_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_masks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\engine\\base_layer.py:886\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_scope()):  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    881\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    882\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;66;03m# Build layer if applicable (if the `build` method has been\u001b[39;00m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# overridden).\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[39;00m\n\u001b[1;32m--> 886\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m    888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\engine\\base_layer.py:2633\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2630\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_build\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m   2631\u001b[0m   \u001b[38;5;66;03m# Check input assumptions set before layer building, e.g. input rank.\u001b[39;00m\n\u001b[0;32m   2632\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m-> 2633\u001b[0m     \u001b[43minput_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_input_compatibility\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2635\u001b[0m     input_list \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m   2636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype_policy\u001b[38;5;241m.\u001b[39mcompute_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\classifiertest\\lib\\site-packages\\keras\\engine\\input_spec.py:214\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    212\u001b[0m   ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[0;32m    213\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(input_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    215\u001b[0m                      layer_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is incompatible with the layer: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    216\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(spec\u001b[38;5;241m.\u001b[39mndim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    217\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(ndim) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Full shape received: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    218\u001b[0m                      \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(shape)))\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m   ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 50)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X_filtered, y_filtered):\n",
    "  # create model\n",
    "    X_traincv =X_filtered[train]\n",
    "    y_traincv  = y_filtered[train]\n",
    "    X_testcv = X_filtered[test]\n",
    "    y_testcv = y_filtered[test]\n",
    "    X_traincv = np.log10(X_traincv)\n",
    "    X_traincv = sc.fit_transform(X_traincv)\n",
    "    X_traincv = np.reshape(X_traincv, (X_traincv.shape[0], int(X_traincv.shape[1]),1))\n",
    "    X_testcv = np.log10(X_testcv)\n",
    "    X_testcv = sc.transform(X_testcv)\n",
    "    X_testcv = np.reshape(X_testcv, (X_testcv.shape[0], int(X_testcv.shape[1]),1))\n",
    "    clf = Sequential()\n",
    "#     clf.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "#     clf.add(Conv1D(filters=16, kernel_size=3, padding='same', activation='relu'))\n",
    "#     clf.add(Dropout(0.25))\n",
    "#     clf.add(MaxPooling1D(pool_size=2))\n",
    "    clf.add(LSTM(units=50,input_shape=(X_traincv.shape[1],1)))\n",
    "    clf.add(LSTM(units = 30,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units = 50,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units = 50,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units =50))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(Dense(units = 100,activation='relu'))\n",
    "    clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "    adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    ada = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.00001, rho=0.95, epsilon=1e-07)\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(reduction='sum_over_batch_size')\n",
    "    clf.compile(loss=bce, optimizer=adm, metrics=['accuracy']) \n",
    "    clf.fit(X_traincv, y_traincv, epochs=300, batch_size=10)\n",
    "    # evaluate the model\n",
    "    scores = clf.evaluate(X_testcv, y_testcv, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (clf.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "pyplot.boxplot([cvscores], labels=['RNN'], showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbff9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
