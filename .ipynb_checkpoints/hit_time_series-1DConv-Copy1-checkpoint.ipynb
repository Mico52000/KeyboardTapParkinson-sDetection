{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "705d3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nqDataLoader as nq #data loading library\n",
    "from keras.preprocessing import sequence\n",
    "# np.random.seed(0)\n",
    "# cnn model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6a7affcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>gt</th>\n",
       "      <th>updrs108</th>\n",
       "      <th>afTap</th>\n",
       "      <th>sTap</th>\n",
       "      <th>nqScore</th>\n",
       "      <th>typingSpeed</th>\n",
       "      <th>file_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>79.0</td>\n",
       "      <td>184.5</td>\n",
       "      <td>0.107179</td>\n",
       "      <td>56.866667</td>\n",
       "      <td>1424946827.1000_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>96.5</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.056286</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1427279751.1001_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>140.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>119.037037</td>\n",
       "      <td>1426676689.1002_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>83.5</td>\n",
       "      <td>191.5</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>74.266667</td>\n",
       "      <td>1429866367.1004_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>68.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>74.969697</td>\n",
       "      <td>1430134526.1005_001_014.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pID  gt  updrs108  afTap   sTap   nqScore  typingSpeed  \\\n",
       "0  1000   1        27   79.0  184.5  0.107179    56.866667   \n",
       "1  1001   1        16   96.5  189.0  0.056286   118.000000   \n",
       "2  1002   0         5  140.0  158.0  0.039519   119.037037   \n",
       "3  1004   1        22   83.5  191.5  0.034853    74.266667   \n",
       "4  1005   1        17   68.0  150.0  0.048307    74.969697   \n",
       "\n",
       "                        file_1  \n",
       "0  1424946827.1000_001_014.csv  \n",
       "1  1427279751.1001_001_014.csv  \n",
       "2  1426676689.1002_001_014.csv  \n",
       "3  1429866367.1004_001_014.csv  \n",
       "4  1430134526.1005_001_014.csv  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the early stage dataset \n",
    "early_stage = pd.read_csv('GT_DataPD_MIT-CS2PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "early_stage[\"gt\"] = early_stage[\"gt\"].astype(int)\n",
    "early_stage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1ea5e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_time_series = []\n",
    "for index, row in early_stage.iterrows():\n",
    "    fileloc = row.file_1\n",
    "    keyPressed, htArr, pressArr, releaseArr =  nq.getDataFiltHelper( \"data_MIT-CS2PD/\" + early_stage.loc[index]['file_1'])\n",
    "    htArr =np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "\n",
    "X1 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6f32abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>gt</th>\n",
       "      <th>updrs108</th>\n",
       "      <th>afTap</th>\n",
       "      <th>sTap</th>\n",
       "      <th>nqScore</th>\n",
       "      <th>typingSpeed</th>\n",
       "      <th>file_1</th>\n",
       "      <th>file_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>14.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.25</td>\n",
       "      <td>0.117543</td>\n",
       "      <td>189.372549</td>\n",
       "      <td>1402930351.011_001_014.csv</td>\n",
       "      <td>1403706430.011_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.25</td>\n",
       "      <td>0.070350</td>\n",
       "      <td>60.533333</td>\n",
       "      <td>1402932300.060_001_014.csv</td>\n",
       "      <td>1403708258.060_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>25.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.75</td>\n",
       "      <td>0.223411</td>\n",
       "      <td>54.333333</td>\n",
       "      <td>1401117235.067_001_014.csv</td>\n",
       "      <td>1401978395.067_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159.00</td>\n",
       "      <td>0.074973</td>\n",
       "      <td>71.800000</td>\n",
       "      <td>1401114972.068_001_014.csv</td>\n",
       "      <td>1401980765.068_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>26.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.50</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>39.614035</td>\n",
       "      <td>1404311419.070_001_014.csv</td>\n",
       "      <td>1404743687.070_003_014.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pID  gt  updrs108  afTap    sTap   nqScore  typingSpeed  \\\n",
       "0   11   1     14.25    NaN  162.25  0.117543   189.372549   \n",
       "1   60   0      2.00    NaN  162.25  0.070350    60.533333   \n",
       "2   67   1     25.25    NaN  133.75  0.223411    54.333333   \n",
       "3   68   0      6.00    NaN  159.00  0.074973    71.800000   \n",
       "4   70   1     26.25    NaN  113.50  0.175751    39.614035   \n",
       "\n",
       "                       file_1                      file_2  \n",
       "0  1402930351.011_001_014.csv  1403706430.011_003_014.csv  \n",
       "1  1402932300.060_001_014.csv  1403708258.060_003_014.csv  \n",
       "2  1401117235.067_001_014.csv  1401978395.067_003_014.csv  \n",
       "3  1401114972.068_001_014.csv  1401980765.068_003_014.csv  \n",
       "4  1404311419.070_001_014.csv  1404743687.070_003_014.csv  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the de-novo dataset \n",
    "de_novo = pd.read_csv('GT_DataPD_MIT-CS1PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "print(len(de_novo))\n",
    "de_novo[\"gt\"] = de_novo[\"gt\"].astype(int)\n",
    "de_novo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5259add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##using both files \n",
    "hit_time_series = []\n",
    "for index, row in de_novo.iterrows():\n",
    "    fileloc1 = row.file_1\n",
    "    keyPressed, htArr1, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_1'])\n",
    "    htArr1 = np.array(htArr1)\n",
    "    keyPressed, htArr2, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_2'])\n",
    "    htArr2 = np.array(htArr2)\n",
    "    htArr =np.concatenate((htArr1,htArr2),axis =0)\n",
    "    htArr=np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "X2 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "83b99f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(85,)\n",
      "[1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 1 0 0 1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X1,X2),axis=0)\n",
    "y=  np.concatenate((early_stage['gt'],de_novo[\"gt\"]),axis=0)\n",
    "print(X.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "91950d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11633377817478088\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "flat_list = list(itertools.chain(*X))\n",
    "value = sum(flat_list) / len(flat_list)\n",
    "print(value)\n",
    "X_padded =sequence.pad_sequences(X,dtype='float32',padding='post',maxlen=6000,value =0)\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_3D,y,test_size=0.17,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4fcea601",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(trainX, trainy, testX, testy,n_filters):\n",
    "    verbose, epochs, batch_size = 0, 100, 10\n",
    "    n_timesteps, n_features = trainX.shape[1], trainX.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Conv1D(filters=n_filters, kernel_size=3, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4216c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a0aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">p=8 #1: 64.706\n",
      ">p=8 #2: 64.706\n",
      ">p=8 #3: 58.824\n",
      ">p=8 #4: 52.941\n",
      ">p=8 #5: 52.941\n",
      ">p=8 #6: 52.941\n",
      ">p=8 #7: 64.706\n",
      ">p=8 #8: 58.824\n",
      ">p=8 #9: 58.824\n",
      ">p=8 #10: 52.941\n",
      ">p=16 #1: 47.059\n",
      ">p=16 #2: 52.941\n",
      ">p=16 #3: 64.706\n",
      ">p=16 #4: 52.941\n",
      ">p=16 #5: 58.824\n",
      ">p=16 #6: 64.706\n",
      ">p=16 #7: 52.941\n",
      ">p=16 #8: 64.706\n",
      ">p=16 #9: 58.824\n",
      ">p=16 #10: 58.824\n",
      ">p=32 #1: 52.941\n",
      ">p=32 #2: 64.706\n",
      ">p=32 #3: 52.941\n",
      ">p=32 #4: 52.941\n",
      ">p=32 #5: 58.824\n",
      ">p=32 #6: 64.706\n",
      ">p=32 #7: 58.824\n",
      ">p=32 #8: 58.824\n",
      ">p=32 #9: 64.706\n",
      ">p=32 #10: 52.941\n",
      ">p=64 #1: 52.941\n",
      ">p=64 #2: 64.706\n",
      ">p=64 #3: 64.706\n",
      ">p=64 #4: 52.941\n",
      ">p=64 #5: 64.706\n",
      ">p=64 #6: 64.706\n",
      ">p=64 #7: 64.706\n",
      ">p=64 #8: 64.706\n",
      ">p=64 #9: 70.588\n",
      ">p=64 #10: 58.824\n",
      ">p=128 #1: 58.824\n",
      ">p=128 #2: 58.824\n",
      ">p=128 #3: 64.706\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# run an experiment\n",
    "def run_experiment(params, repeats=10):\n",
    "    # load data\n",
    "    trainX,testX,trainy,testy = train_test_split(X_padded,y,test_size = 0.2,stratify=y)\n",
    "    trainX = np.array(trainX)\n",
    "    testX = np.array(testX)\n",
    "    sc = StandardScaler()\n",
    "    trainX = sc.fit_transform(trainX)\n",
    "    trainX = np.reshape(trainX,(trainX.shape[0],trainX.shape[1],1))\n",
    "    testX = sc.transform(testX)\n",
    "    testX = np.reshape(testX,(testX.shape[0],testX.shape[1],1))\n",
    "    # test each parameter\n",
    "    all_scores = list()\n",
    "    for p in params:\n",
    "        # repeat experiment\n",
    "        scores = list()\n",
    "        for r in range(repeats):\n",
    "            score = evaluate_model(trainX, trainy, testX, testy, p)\n",
    "            score = score * 100.0\n",
    "            print('>p=%d #%d: %.3f' % (p, r+1, score))\n",
    "            scores.append(score)\n",
    "        all_scores.append(scores)\n",
    "    # summarize results\n",
    "    summarize_results(all_scores, params)\n",
    "# summarize scores\n",
    "def summarize_results(scores, params):\n",
    "    print(scores, params)\n",
    "    # summarize mean and standard deviation\n",
    "    for i in range(len(scores)):\n",
    "        m, s = mean(scores[i]), std(scores[i])\n",
    "        print('Param=%d: %.3f%% (+/-%.3f)' % (params[i], m, s))\n",
    "    # boxplot of scores\n",
    "    pyplot.boxplot(scores, labels=params)\n",
    "# run the experiment\n",
    "n_params = [8, 16, 32, 64, 128, 256]\n",
    "run_experiment(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c51390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of each variable in the dataset\n",
    "def plot_variable_distributions(trainX):\n",
    "    pyplot.figure()\n",
    "    xaxis = None\n",
    "    for i in range(trainX.shape[1]):\n",
    "        ax = pyplot.subplot(trainX.shape[1], 1, i+1, sharex=xaxis)\n",
    "        ax.set_xlim(-1, 1)\n",
    "        if i == 0:\n",
    "            xaxis = ax\n",
    "        pyplot.hist(trainX[:, i], bins=100)\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afdac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_variable_distributions(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7fb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cbdf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc816cc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Dropout\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from matplotlib import pyplot\n",
    "# from keras import optimizers\n",
    "# import tensorflow as tf\n",
    "# clf = Sequential()\n",
    "# clf.add(LSTM(units = 30,return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "# clf.add(Dropout(0.2))\n",
    "# clf.add(LSTM(units = 30,return_sequences = True))\n",
    "# clf.add(Dropout(0.2))\n",
    "# clf.add(LSTM(units = 30,return_sequences = True))\n",
    "# clf.add(Dropout(0.2))\n",
    "# clf.add(LSTM(units =30 ))\n",
    "# clf.add(Dropout(0.2))\n",
    "# clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "# es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "# adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "# clf.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "# print(clf.summary())\n",
    "# history=clf.fit(X_train, y_train, epochs=200, batch_size=35)\n",
    "# scores = clf.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# train_acc = clf.evaluate(X_train, y_train, verbose=0)\n",
    "# test_acc = clf.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Train: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))\n",
    "# # plot training history\n",
    "# pyplot.plot(history.history['loss'], label='train loss')\n",
    "# pyplot.plot(history.history['accuracy'], label='train accuracy')\n",
    "# # pyplot.plot(history.history['val_loss'], label='test')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
