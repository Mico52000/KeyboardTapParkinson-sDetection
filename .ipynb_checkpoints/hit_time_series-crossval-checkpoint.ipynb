{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705d3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nqDataLoader as nq #data loading library\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7affcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>gt</th>\n",
       "      <th>updrs108</th>\n",
       "      <th>afTap</th>\n",
       "      <th>sTap</th>\n",
       "      <th>nqScore</th>\n",
       "      <th>typingSpeed</th>\n",
       "      <th>file_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>79.0</td>\n",
       "      <td>184.5</td>\n",
       "      <td>0.107179</td>\n",
       "      <td>56.866667</td>\n",
       "      <td>1424946827.1000_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>96.5</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.056286</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>1427279751.1001_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>140.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.039519</td>\n",
       "      <td>119.037037</td>\n",
       "      <td>1426676689.1002_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>83.5</td>\n",
       "      <td>191.5</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>74.266667</td>\n",
       "      <td>1429866367.1004_001_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>68.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.048307</td>\n",
       "      <td>74.969697</td>\n",
       "      <td>1430134526.1005_001_014.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pID  gt  updrs108  afTap   sTap   nqScore  typingSpeed  \\\n",
       "0  1000   1        27   79.0  184.5  0.107179    56.866667   \n",
       "1  1001   1        16   96.5  189.0  0.056286   118.000000   \n",
       "2  1002   0         5  140.0  158.0  0.039519   119.037037   \n",
       "3  1004   1        22   83.5  191.5  0.034853    74.266667   \n",
       "4  1005   1        17   68.0  150.0  0.048307    74.969697   \n",
       "\n",
       "                        file_1  \n",
       "0  1424946827.1000_001_014.csv  \n",
       "1  1427279751.1001_001_014.csv  \n",
       "2  1426676689.1002_001_014.csv  \n",
       "3  1429866367.1004_001_014.csv  \n",
       "4  1430134526.1005_001_014.csv  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the early stage dataset \n",
    "early_stage = pd.read_csv('GT_DataPD_MIT-CS2PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "early_stage[\"gt\"] = early_stage[\"gt\"].astype(int)\n",
    "early_stage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea5e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_time_series = []\n",
    "for index, row in early_stage.iterrows():\n",
    "    fileloc = row.file_1\n",
    "    keyPressed, htArr, pressArr, releaseArr =  nq.getDataFiltHelper( \"data_MIT-CS2PD/\" + early_stage.loc[index]['file_1'])\n",
    "    htArr =np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "\n",
    "X1 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f32abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pID</th>\n",
       "      <th>gt</th>\n",
       "      <th>updrs108</th>\n",
       "      <th>afTap</th>\n",
       "      <th>sTap</th>\n",
       "      <th>nqScore</th>\n",
       "      <th>typingSpeed</th>\n",
       "      <th>file_1</th>\n",
       "      <th>file_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>14.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.25</td>\n",
       "      <td>0.117543</td>\n",
       "      <td>189.372549</td>\n",
       "      <td>1402930351.011_001_014.csv</td>\n",
       "      <td>1403706430.011_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.25</td>\n",
       "      <td>0.070350</td>\n",
       "      <td>60.533333</td>\n",
       "      <td>1402932300.060_001_014.csv</td>\n",
       "      <td>1403708258.060_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>25.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133.75</td>\n",
       "      <td>0.223411</td>\n",
       "      <td>54.333333</td>\n",
       "      <td>1401117235.067_001_014.csv</td>\n",
       "      <td>1401978395.067_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159.00</td>\n",
       "      <td>0.074973</td>\n",
       "      <td>71.800000</td>\n",
       "      <td>1401114972.068_001_014.csv</td>\n",
       "      <td>1401980765.068_003_014.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>26.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.50</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>39.614035</td>\n",
       "      <td>1404311419.070_001_014.csv</td>\n",
       "      <td>1404743687.070_003_014.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pID  gt  updrs108  afTap    sTap   nqScore  typingSpeed  \\\n",
       "0   11   1     14.25    NaN  162.25  0.117543   189.372549   \n",
       "1   60   0      2.00    NaN  162.25  0.070350    60.533333   \n",
       "2   67   1     25.25    NaN  133.75  0.223411    54.333333   \n",
       "3   68   0      6.00    NaN  159.00  0.074973    71.800000   \n",
       "4   70   1     26.25    NaN  113.50  0.175751    39.614035   \n",
       "\n",
       "                       file_1                      file_2  \n",
       "0  1402930351.011_001_014.csv  1403706430.011_003_014.csv  \n",
       "1  1402932300.060_001_014.csv  1403708258.060_003_014.csv  \n",
       "2  1401117235.067_001_014.csv  1401978395.067_003_014.csv  \n",
       "3  1401114972.068_001_014.csv  1401980765.068_003_014.csv  \n",
       "4  1404311419.070_001_014.csv  1404743687.070_003_014.csv  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing the de-novo dataset \n",
    "de_novo = pd.read_csv('GT_DataPD_MIT-CS1PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "print(len(de_novo))\n",
    "de_novo[\"gt\"] = de_novo[\"gt\"].astype(int)\n",
    "de_novo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5259add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##using both files \n",
    "hit_time_series = []\n",
    "for index, row in de_novo.iterrows():\n",
    "    fileloc1 = row.file_1\n",
    "    keyPressed, htArr1, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_1'])\n",
    "    htArr1 = np.array(htArr1)\n",
    "    keyPressed, htArr2, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_2'])\n",
    "    htArr2 = np.array(htArr2)\n",
    "    htArr =np.concatenate((htArr1,htArr2),axis =0)\n",
    "    htArr=np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "X2 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282ec3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "40\n",
      "43\n",
      "1975.5882352941176\n",
      "299\n",
      "85 85\n",
      "[array([0.3179, 0.1892, 0.1641, 0.2597, 0.1994, 0.1016, 0.122 , 0.139 ,\n",
      "        0.147 , 0.2011, 0.0758, 0.1902, 0.1341, 0.1766, 0.0912, 0.2218,\n",
      "        0.1295, 0.1171, 0.1057, 0.077 , 0.0969, 0.2141, 0.1101, 0.1355,\n",
      "        0.1697, 0.1038, 0.1234, 0.1405, 0.1393, 0.1209, 0.1359, 0.1342,\n",
      "        0.1935, 0.0775, 0.1834, 0.178 , 0.2392, 0.1381, 0.1584, 0.1741,\n",
      "        0.1349, 0.2007, 0.1402, 0.1964, 0.2102, 0.1575, 0.1527, 0.0831,\n",
      "        0.1709, 0.1588, 0.1162, 0.1702, 0.1259, 0.1004, 0.1751, 0.1479,\n",
      "        0.1835, 0.1342, 0.1687, 0.1576, 0.1129, 0.2006, 0.1176, 0.1315,\n",
      "        0.2539, 0.1244, 0.1316, 0.1911, 0.1904, 0.1438, 0.1757, 0.1734,\n",
      "        0.1517, 0.18  , 0.1208, 0.1003, 0.1092, 0.1556, 0.1204, 0.2012,\n",
      "        0.1609, 0.214 , 0.1929, 0.3871, 0.1575, 0.118 , 0.1306, 0.2313,\n",
      "        0.1446, 0.1225, 0.1309, 0.1261, 0.1912, 0.1307, 0.1356, 0.0955,\n",
      "        0.1739, 0.1349, 0.1399, 0.1971, 0.1133, 0.1099, 0.1257, 0.1404,\n",
      "        0.153 , 0.1179, 0.1884, 0.1261, 0.1448, 0.1837, 0.148 , 0.2308,\n",
      "        0.1405, 0.1566, 0.1071, 0.1095, 0.1273, 0.1338, 0.1787, 0.151 ,\n",
      "        0.2489, 0.1791, 0.1969, 0.1134, 0.1006, 0.125 , 0.1261, 0.1352,\n",
      "        0.1489, 0.1178, 0.1349, 0.1314, 0.1438, 0.1481, 0.1841, 0.1176,\n",
      "        0.1885, 0.1657, 0.2059, 0.1557, 0.201 , 0.188 , 0.1076, 0.1782,\n",
      "        0.1177, 0.1532, 0.1354, 0.179 , 0.1178, 0.1275, 0.1098, 0.1273,\n",
      "        0.1378, 0.1512, 0.1176, 0.12  , 0.149 , 0.1001, 0.1131, 0.1209,\n",
      "        0.1003, 0.1403, 0.131 , 0.1359, 0.2185, 0.1105, 0.146 , 0.1692,\n",
      "        0.1282, 0.1394, 0.1206, 0.1631, 0.1745, 0.1068, 0.1622, 0.0923,\n",
      "        0.1843, 0.1177, 0.1832, 0.1105, 0.1582, 0.1654, 0.1301, 0.1889,\n",
      "        0.148 , 0.1483, 0.1206, 0.179 , 0.1306, 0.1172, 0.1685, 0.1449,\n",
      "        0.1535, 0.1378, 0.1785, 0.1533, 0.1274, 0.1077, 0.1412, 0.1873,\n",
      "        0.1494, 0.1836, 0.1278, 0.1275, 0.1277, 0.1566, 0.1764, 0.1759,\n",
      "        0.1517, 0.1176, 0.0998, 0.1558, 0.134 , 0.1357, 0.1258, 0.2407,\n",
      "        0.1884, 0.161 , 0.1087, 0.2094, 0.1277, 0.0958, 0.0964, 0.1264,\n",
      "        0.1359, 0.1587, 0.1488, 0.1537, 0.2109, 0.1911, 0.1611, 0.1986,\n",
      "        0.1484, 0.1137, 0.167 , 0.1758, 0.179 , 0.1352, 0.1843, 0.1609,\n",
      "        0.1046, 0.1669, 0.1306, 0.0871, 0.1247, 1.2744, 0.463 , 0.1119,\n",
      "        0.0279, 0.0804, 0.0539, 0.0737, 0.1188, 0.1014, 0.0696, 0.0699,\n",
      "        0.041 , 0.0307, 0.1175, 0.0493, 0.0583, 0.0737, 0.0597, 0.0447,\n",
      "        0.0577, 0.036 , 0.0701, 0.0815, 0.0311, 0.0831, 0.0778, 0.1539,\n",
      "        0.1758, 0.1035, 0.1485, 0.0782, 0.1163, 0.1276, 0.1783, 0.1081,\n",
      "        0.1175, 0.1784, 0.1129, 0.1249, 0.2037, 0.1015, 0.1763, 0.2361,\n",
      "        0.1156, 0.1158, 0.1076, 0.1471, 0.1471, 0.1637, 0.1497, 0.151 ,\n",
      "        0.1465, 0.15  , 0.1036, 0.1507, 0.1681, 0.1511, 0.1703, 0.1686,\n",
      "        0.124 , 0.1916, 0.2113, 0.3489, 0.0986, 0.101 , 0.1766, 0.1469,\n",
      "        0.1591, 0.1464, 0.1773, 0.1672, 0.1485, 0.1278, 0.161 , 0.1175,\n",
      "        0.1658, 0.1714, 0.201 , 0.1509, 0.1582, 0.1137, 0.1177, 0.161 ,\n",
      "        0.1308, 0.1734, 0.1721, 0.1493, 0.1547, 0.1157, 0.1729, 0.1595,\n",
      "        0.361 , 0.1259, 0.1228, 0.1014, 0.1128, 0.136 , 0.2081, 0.1004,\n",
      "        0.1135, 0.1405, 0.1335, 0.1453, 0.149 , 0.1351, 0.0814, 0.1309,\n",
      "        0.1788, 0.184 , 0.1081, 0.1386, 0.1485, 0.1963, 0.1307, 0.1232,\n",
      "        0.1842, 0.1009, 0.1845, 0.1354, 0.1767, 0.1571, 0.1136, 0.0958,\n",
      "        0.0505, 0.1715, 0.1396, 0.2031, 0.0951, 0.1231, 0.1179, 0.1185,\n",
      "        0.1303, 0.1967, 0.1991, 0.3322, 0.123 , 0.1203, 0.1004, 0.1034,\n",
      "        0.2039, 0.1407, 0.2046, 0.2132, 0.1585, 0.1055, 0.1878, 0.1526,\n",
      "        0.1969, 0.1262, 0.2316, 0.1706, 0.1318, 0.1491, 0.1611, 0.1307,\n",
      "        0.1401, 0.1179, 0.0829, 0.1755, 0.0997, 0.1281, 0.1584, 0.1429,\n",
      "        0.1175, 0.1042, 0.0938, 0.1337, 0.1707, 0.1005, 0.1256, 0.2259,\n",
      "        0.1307, 0.1183, 0.0828, 0.1691, 0.2216, 0.1509, 0.1442, 0.1356,\n",
      "        0.1977, 0.113 , 0.1481, 0.1346, 0.147 , 0.0753, 0.0986, 0.1007,\n",
      "        0.2092, 0.1308, 0.1001, 0.1968, 0.14  , 0.1384, 0.1059, 0.1583,\n",
      "        0.1307, 0.1383, 0.1333, 0.1002, 0.2183, 0.1532, 0.1957, 0.1948,\n",
      "        0.1558, 0.179 , 0.1439, 0.1658, 0.1564, 0.1077, 0.109 , 0.1006,\n",
      "        0.1332, 0.1279, 0.1296, 0.099 , 0.1362, 0.1176, 0.1145, 0.158 ,\n",
      "        0.0952, 0.1332, 0.131 , 0.0967, 0.2088, 0.1053, 0.1003, 0.1869,\n",
      "        0.1232, 0.1391, 0.1793, 0.1923, 0.1489, 0.1027, 0.1088, 0.1793,\n",
      "        0.1361, 0.1527, 0.1081, 0.118 , 0.1163, 0.1308, 0.1211, 0.0962,\n",
      "        0.1823, 0.147 , 0.1004, 0.0836, 0.1264, 0.1481, 0.11  , 0.1537,\n",
      "        0.1607, 0.2207, 0.1248, 0.3244, 0.1194, 0.1354, 0.1572, 0.1208,\n",
      "        0.129 , 0.1335, 0.1336, 0.1208, 0.1293, 0.094 , 0.155 , 0.1392,\n",
      "        0.1161, 0.1119, 0.1244, 0.1596, 0.0984, 0.157 , 0.1015, 0.1388,\n",
      "        0.1421, 0.1295, 0.1761, 0.148 , 0.3358, 0.5507, 0.3517, 0.1509,\n",
      "        0.1784, 0.1553, 0.1291, 0.1477, 0.2233, 0.1454, 0.197 , 0.1448,\n",
      "        0.1033, 0.1324, 0.1996, 0.1387, 0.139 , 0.1528, 0.1134, 0.1979,\n",
      "        0.1881, 0.1752, 0.1758, 0.127 , 0.1733, 0.1681, 0.1305, 0.1711,\n",
      "        0.1175, 0.1309, 0.2714, 0.1639, 0.0936, 0.1761, 0.1464, 0.1646,\n",
      "        0.1115, 0.1198, 0.1633, 0.1431, 0.1196, 0.1292, 0.1244, 0.2436,\n",
      "        0.1514, 0.177 , 0.1082, 0.1088, 0.1523, 0.1294, 0.1677, 0.2547,\n",
      "        0.1947, 0.1634, 0.2234, 0.1056, 0.1359, 0.1784, 0.1435, 0.1308,\n",
      "        0.1312, 0.1266, 0.1314, 0.1135, 0.1171, 0.1758, 0.1136, 0.0885,\n",
      "        0.0826, 0.1453, 0.2107, 0.1567, 0.1347, 0.0926, 0.14  , 0.1097,\n",
      "        0.1175, 0.1825, 0.1255, 0.1688, 0.1306, 0.1006, 0.1552, 0.1705,\n",
      "        0.1349, 0.1003, 0.1172, 0.1755, 0.1987, 0.1683, 0.0912, 0.1007,\n",
      "        0.1003, 0.1254, 0.0821, 0.1399, 0.0999, 0.1183, 0.1127, 0.1179,\n",
      "        0.0977, 0.1203, 0.1611, 0.1678, 0.0359, 0.1361, 0.0833, 0.171 ,\n",
      "        0.1131, 0.136 , 0.1541, 0.1278, 0.1487, 0.1744, 0.2218, 0.1357,\n",
      "        0.1128, 0.1558, 0.1439, 0.0738, 0.1967, 0.1441, 0.1309, 0.1838,\n",
      "        0.1307, 0.1842, 0.2155, 0.1938, 0.1136, 0.1614, 0.1262, 0.1004,\n",
      "        0.1272, 0.1089, 0.1246, 0.1599, 0.1458, 0.1661, 0.1314, 0.1763,\n",
      "        0.0885, 0.0845, 0.1967, 0.1408, 0.1637, 0.1055, 0.1534, 0.1832,\n",
      "        0.1356, 0.1484, 0.1308, 0.1518, 0.0961, 0.1788, 0.1542, 0.1364,\n",
      "        0.1612, 0.131 , 0.1913, 0.1656, 0.4023, 0.1368, 0.1172, 0.1097,\n",
      "        0.148 , 0.1615, 0.1486, 0.1312, 0.1059, 0.1668, 0.1745, 0.1628,\n",
      "        0.1262, 0.1212, 0.1153, 0.1561, 0.1263, 0.1982, 0.11  , 0.1555,\n",
      "        0.1044, 0.2086, 0.1339, 0.2739, 0.1463, 0.2717, 0.2868, 0.2113,\n",
      "        0.241 , 0.1829, 0.1179, 0.2128, 0.2046, 0.0864, 0.0768, 0.158 ,\n",
      "        0.1335, 0.1644, 0.1172, 0.112 , 0.1818, 0.1256, 0.0892, 0.0937,\n",
      "        0.1087, 0.1653, 0.1424, 0.1216, 0.1243, 0.1662, 0.1863, 0.2188,\n",
      "        0.1616, 0.1381, 0.1277, 0.1606, 0.1358, 0.1171, 0.2128, 0.18  ,\n",
      "        0.1273, 0.2261, 0.1263, 0.1308, 0.1229, 0.1533, 0.1977, 0.1038,\n",
      "        0.1003, 0.1207, 0.1159, 0.1081, 0.1462, 0.1133, 0.1352, 0.1321,\n",
      "        0.1054, 0.1669, 0.1604, 0.1581, 0.189 , 0.1008, 0.0806, 0.1362,\n",
      "        0.1691, 0.1244, 0.1246, 0.1412, 0.1789, 0.1131, 0.1522, 0.135 ,\n",
      "        0.151 , 0.1954, 0.1319, 0.2262, 0.1452, 0.1274, 0.1132, 0.1302,\n",
      "        0.1306, 0.1487, 0.1561, 0.1446, 0.1525, 0.1524, 0.1059, 0.1315,\n",
      "        0.1487, 0.1544, 0.2104, 0.1238, 0.119 , 0.1127, 0.1113, 0.115 ,\n",
      "        0.1768, 0.1748, 0.1367, 0.0956, 0.1307, 0.1326, 0.152 , 0.183 ,\n",
      "        0.1478, 0.1764, 0.115 , 0.1194, 0.1466, 0.169 , 0.1678, 0.1363,\n",
      "        0.2435, 0.1777, 0.1472, 0.1788, 0.1338, 0.1244, 0.1525, 0.1418,\n",
      "        0.1454, 0.1429, 0.1508, 0.2744, 0.1443, 0.1762, 0.1525, 0.1384,\n",
      "        0.1411, 0.2527, 0.1418, 0.1948, 0.14  , 0.123 , 0.1866, 0.1245,\n",
      "        0.1644, 0.1089, 0.1652, 0.1166, 0.1261, 0.1332, 0.1131, 0.1397,\n",
      "        0.1818, 0.12  , 0.1246, 0.2272, 0.1015, 0.0946, 0.0514])\n",
      " array([0.1135, 0.1269, 0.1317, ..., 0.0978, 0.0979, 0.1241])\n",
      " array([0.0619, 0.0598, 0.1309, ..., 0.0707, 0.0747, 0.0869])\n",
      " array([0.1093, 0.0939, 0.1115, ..., 0.1407, 0.036 , 0.1439])\n",
      " array([0.1002, 0.1057, 0.0784, ..., 0.085 , 0.1167, 0.0967])\n",
      " array([0.166 , 0.1032, 0.0502, ..., 0.07  , 0.0731, 0.0927])\n",
      " array([0.1542, 0.1139, 0.1873, ..., 0.1514, 0.0842, 0.1007])\n",
      " array([0.1245, 0.1519, 0.1872, ..., 0.0959, 0.1429, 0.1171])\n",
      " array([0.0913, 0.0784, 0.0875, ..., 0.0857, 0.0693, 0.0776])\n",
      " array([0.0639, 0.0698, 0.1483, ..., 0.0779, 0.091 , 0.0828])\n",
      " array([0.0855, 0.0753, 0.0773, ..., 0.0608, 0.0774, 0.0793])\n",
      " array([0.2108, 0.1698, 0.1414, ..., 0.1527, 0.1333, 0.1432])\n",
      " array([0.1515, 0.1756, 0.0994, ..., 0.1742, 0.2873, 0.178 ])\n",
      " array([0.1128, 0.1038, 0.1295, ..., 0.0874, 0.0707, 0.0748])\n",
      " array([0.1139, 0.0822, 0.0607, ..., 0.0532, 0.0481, 0.0411])\n",
      " array([0.2359, 0.1929, 0.4013, 0.2103, 0.183 , 0.2391, 0.2081, 0.2162,\n",
      "        0.2703, 0.1732, 0.2609, 0.3407, 0.2966, 0.1762, 0.2005, 0.2436,\n",
      "        0.2081, 0.1903, 0.2833, 0.2001, 0.2138, 0.2162, 0.2382, 0.2482,\n",
      "        0.2057, 0.2628, 0.2129, 0.2304, 0.1378, 0.1786, 0.2556, 0.1477,\n",
      "        0.0985, 0.2182, 0.1814, 0.2715, 0.1093, 0.1527, 0.2069, 0.1834,\n",
      "        0.2147, 0.1692, 0.2492, 0.1342, 0.2394, 0.2313, 0.2384, 0.2411,\n",
      "        0.2268, 0.2304, 0.1734, 0.2518, 0.1542, 0.1869, 0.3955, 0.2704,\n",
      "        0.2382, 0.1746, 0.1801, 0.213 , 0.2134, 0.2177, 0.216 , 0.1926,\n",
      "        0.2223, 0.1585, 0.2706, 0.175 , 0.2659, 0.1601, 0.2489, 0.219 ,\n",
      "        0.2151, 0.2343, 0.2433, 0.2617, 0.1273, 0.1617, 0.1306, 0.1739,\n",
      "        0.204 , 0.2305, 0.1905, 0.2156, 0.1942, 0.2273, 0.2089, 0.1779,\n",
      "        0.2301, 0.2177, 0.0385, 0.2525, 0.1735, 0.1958, 0.2828, 0.2266,\n",
      "        0.1955, 0.2742, 0.2226, 0.1928, 0.2259, 0.1821, 0.1958, 0.2748,\n",
      "        0.1924, 0.1707, 0.1876, 0.2012, 0.2015, 0.2362, 0.1898, 0.1773,\n",
      "        0.1342, 0.1499, 0.2176, 0.1894, 0.1955, 0.2127, 0.3181, 0.1697,\n",
      "        0.2587, 0.1493, 0.1203, 0.1473, 0.1632, 0.1786, 0.1632, 0.2233,\n",
      "        0.2478, 0.2056, 0.2049, 0.1957, 0.21  , 0.1735, 0.2561, 0.2654,\n",
      "        0.2534, 0.2224, 0.1346, 0.0951, 0.1735, 0.2005, 0.183 , 0.181 ,\n",
      "        0.1826, 0.2176, 0.1933, 0.2279, 0.2511, 0.1999, 0.306 , 0.246 ,\n",
      "        0.2362, 0.2396, 0.2049, 0.2306, 0.1735, 0.1449, 0.2264, 0.2004,\n",
      "        0.2173, 0.3058, 0.1875, 0.1473, 0.134 , 0.17  , 0.1876, 0.1872,\n",
      "        0.2486, 0.1252, 0.2228, 0.2514, 0.1914, 0.3389, 0.2385, 0.2119,\n",
      "        0.1996, 0.3345, 0.1554, 0.3493, 0.2211, 0.2233, 0.2337, 0.2003,\n",
      "        0.1954, 0.2082, 0.2828, 0.2183, 0.175 , 0.1484, 0.2006, 0.2254,\n",
      "        0.1854, 0.2348, 0.2005, 0.165 , 0.1675, 0.1812, 0.3107, 0.23  ,\n",
      "        0.2381, 0.2403, 0.2807, 0.1532, 0.2785, 0.3321, 0.1677, 0.2117,\n",
      "        0.2034, 0.2361, 0.1729, 0.1401, 0.1558, 0.2078, 0.3339, 0.176 ,\n",
      "        0.2416, 0.2169, 0.3699, 0.2088, 0.2082, 0.1395, 0.1917, 0.206 ,\n",
      "        0.1356, 0.1198, 0.2475, 0.2294, 0.2101, 0.2554, 0.1945, 0.1969,\n",
      "        0.1305, 0.1705, 0.1337, 0.1883, 0.2968, 0.2609, 0.3009, 0.2029,\n",
      "        0.2007, 0.2363, 0.2188, 0.179 , 0.1856, 0.2522, 0.1614, 0.1708,\n",
      "        0.1625, 0.2183, 0.2082, 0.2813, 0.223 , 0.262 , 0.2308, 0.2354,\n",
      "        0.2274, 0.2525, 0.1862, 0.2328, 0.1898, 0.1985, 0.1991, 0.2594,\n",
      "        0.2069, 0.1468, 0.3296, 0.1731, 0.2728, 0.1348, 0.1906, 0.2082,\n",
      "        0.1969, 0.2088, 0.2349, 0.2158, 0.1826, 0.2403, 0.1911, 0.2133,\n",
      "        0.2135, 0.2323, 0.2959, 0.2739, 0.3264, 0.2919, 0.1909, 0.2383,\n",
      "        0.2136, 0.3004, 0.1643, 0.2213, 0.2502, 0.1701, 0.1692, 0.2134,\n",
      "        0.138 , 0.195 , 0.188 , 0.2881, 0.2703, 0.1704, 0.1888, 0.2406,\n",
      "        0.2403, 0.2909, 0.2851, 0.183 , 0.1827, 0.3518, 0.2787, 0.2667,\n",
      "        0.248 , 0.2931, 0.2437, 0.1601, 0.1354, 0.1694, 0.1703, 0.2137,\n",
      "        0.2033, 0.2796, 0.2488, 0.1828, 0.2964, 0.2048, 0.3041, 0.2395,\n",
      "        0.1697, 0.266 , 0.2202, 0.2269, 0.2179, 0.2325, 0.2557, 0.4865,\n",
      "        0.2814, 0.2649, 0.3514, 0.2727, 0.2617, 0.3348, 0.3313, 0.2165,\n",
      "        0.199 , 0.2561, 0.3268, 0.1545, 0.2482, 0.2004, 0.2235, 0.1946,\n",
      "        0.0518, 0.2402, 0.224 , 0.2627, 0.2404, 0.1552, 0.16  , 0.1964,\n",
      "        0.2015, 0.3004, 0.1947, 0.3526, 0.2821, 0.1919, 0.1302, 0.2188,\n",
      "        0.1479, 0.1758, 0.1965, 0.2359, 0.218 , 0.3693, 0.1477, 0.2671,\n",
      "        0.3106, 0.1354, 0.1757, 0.2193, 0.1913, 0.2678, 0.1653, 0.2193,\n",
      "        0.2592, 0.216 , 0.1457, 0.2371, 0.2068, 0.2419, 0.1793, 0.3083,\n",
      "        0.2099, 0.1656, 0.2436, 0.1735, 0.2182, 0.2314, 0.2698, 0.1836,\n",
      "        0.1527, 0.2619, 0.1707, 0.2062, 0.254 , 0.2593, 0.1364, 0.1968,\n",
      "        0.3446, 0.2441, 0.2394, 0.2315, 0.1966, 0.2443, 0.2489, 0.231 ,\n",
      "        0.2167, 0.2116, 0.2144, 0.1967, 0.2368, 0.2111, 0.1568, 0.2231,\n",
      "        0.2443, 0.1803, 0.2696, 0.1841, 0.1364, 0.297 , 0.267 , 0.2538,\n",
      "        0.2446, 0.1536, 0.1385, 0.2274, 0.176 , 0.1795, 0.274 , 0.1628,\n",
      "        0.3095, 0.3448, 0.2539, 0.2573, 0.1396, 0.2048, 0.1991, 0.1808,\n",
      "        0.2318, 0.262 , 0.2515, 0.2482, 0.2762, 0.1809, 0.2272, 0.1395,\n",
      "        0.1276, 0.1653, 0.1424, 0.1809, 0.1747, 0.1999, 0.3057, 0.2749,\n",
      "        0.1876, 0.2207, 0.3183, 0.2697, 0.1985, 0.1832, 0.2526, 0.2307,\n",
      "        0.2189, 0.2308, 0.1646, 0.2704, 0.1703, 0.2138, 0.2304, 0.2853,\n",
      "        0.1639, 0.2606, 0.1335, 0.1918, 0.256 ])\n",
      " array([0.1787, 0.0959, 0.1241, ..., 0.1585, 0.0577, 0.1055])\n",
      " array([0.1465, 0.152 , 0.1184, ..., 0.2147, 0.1237, 0.1188])\n",
      " array([0.0701, 0.1527, 0.064 , ..., 0.0735, 0.0649, 0.1129])\n",
      " array([0.1205, 0.1329, 0.1888, ..., 0.1829, 0.1506, 0.1803])\n",
      " array([0.083 , 0.2014, 0.0783, ..., 0.1386, 0.0902, 0.174 ])\n",
      " array([0.1487, 0.114 , 0.1281, ..., 0.1239, 0.1216, 0.1662])\n",
      " array([0.2446, 0.3646, 0.2989, 0.0448, 0.1181, 0.0699, 0.2322, 0.0785,\n",
      "        0.1177, 0.2227, 0.071 , 0.0878, 0.2577, 0.2931, 0.0737, 0.2152,\n",
      "        0.1054, 0.1311, 0.1878, 0.1659, 0.1236, 0.2321, 0.0702, 0.0779,\n",
      "        0.0751, 0.2122, 0.0683, 0.2373, 0.3101, 0.2743, 0.0616, 0.2926,\n",
      "        0.1739, 0.1045, 0.0781, 0.0886, 0.1229, 0.2376, 0.0719, 0.1302,\n",
      "        0.2731, 0.1356, 0.2009, 0.2488, 0.1352, 0.1371, 0.2896, 0.1028,\n",
      "        0.2723, 0.0906, 0.208 , 0.1151, 0.0767, 0.0835, 0.2196, 0.1357,\n",
      "        0.2642, 0.1076, 0.0835, 0.0797, 0.1449, 0.3018, 0.1666, 0.0603,\n",
      "        0.0693, 0.143 , 0.1342, 0.074 , 0.2619, 0.1117, 0.0822, 0.0696,\n",
      "        0.3192, 0.2091, 0.262 , 0.0957, 0.1704, 0.2624, 0.2237, 0.1269,\n",
      "        0.1522, 0.1577, 0.0898, 0.0866, 0.2313, 0.0904, 0.231 , 0.0604,\n",
      "        0.1048, 0.2085, 0.0605, 0.1139, 0.3075, 0.0783, 0.1031, 0.2419,\n",
      "        0.0904, 0.0627, 0.149 , 0.0873, 0.1574, 0.2411, 0.1458, 0.2666,\n",
      "        0.1171, 0.117 , 0.1025, 0.2626, 0.2278, 0.1285, 0.3127, 0.3333,\n",
      "        0.2852, 0.1178, 0.0825, 0.0774, 0.0587, 0.154 , 0.2296, 0.071 ,\n",
      "        0.1265, 0.0875, 0.0878, 0.0914, 0.0832, 0.0529, 0.1526, 0.0622,\n",
      "        0.1234, 0.2415, 0.0672, 0.0895, 0.0785, 0.0628, 0.2151, 0.0966,\n",
      "        0.0709, 0.1234, 0.0877, 0.087 , 0.2204, 0.0976, 0.1836, 0.2235,\n",
      "        0.108 , 0.065 , 0.0782, 0.2368, 0.0683, 0.2196, 0.0657, 0.2151,\n",
      "        0.1966, 0.2625, 0.071 , 0.0609, 0.0792, 0.2204, 0.0995, 0.0648,\n",
      "        0.2183, 0.2285, 0.3223, 0.0859, 0.1422, 0.1457, 0.2964, 0.2005,\n",
      "        0.3453, 0.2324, 0.0866, 0.1534, 0.0947, 0.0946, 0.1778, 0.1568,\n",
      "        0.0849, 0.1933, 0.2393, 0.1605, 0.2754, 0.2185, 0.0785, 0.0822,\n",
      "        0.2132, 0.1543, 0.1339, 0.0951, 0.0608, 0.0744, 0.2424, 0.0664,\n",
      "        0.0999, 0.1481, 0.2473, 0.1133, 0.2055, 0.2686, 0.2467, 0.0759,\n",
      "        0.0849, 0.0886, 0.0649, 0.2476, 0.0839, 0.0704, 0.0706, 0.1699,\n",
      "        0.2417, 0.0817, 0.0755, 0.0831, 0.0883, 0.1721, 0.1546, 0.1506,\n",
      "        0.1002, 0.0687, 0.0822, 0.1712, 0.1031, 0.0846, 0.1526, 0.1492,\n",
      "        0.1559, 0.2566, 0.1169, 0.3073, 0.2139, 0.1141, 0.2437, 0.2438,\n",
      "        0.1212, 0.2564, 0.087 , 0.1149, 0.0952, 0.2009, 0.2966, 0.3315,\n",
      "        0.0813, 0.2533, 0.0848, 0.2534, 0.1065, 0.1267, 0.2536, 0.0716,\n",
      "        0.2221, 0.1247, 0.0759, 0.051 , 0.1342, 0.2447, 0.0866, 0.1586,\n",
      "        0.2879, 0.07  , 0.0733, 0.0927, 0.0626, 0.1452, 0.3101, 0.0884,\n",
      "        0.0963, 0.0708, 0.0689, 0.2279, 0.1046, 0.1089, 0.0793, 0.1637,\n",
      "        0.0981, 0.0885, 0.075 , 0.1076, 0.1984, 0.1355, 0.1804, 0.3105,\n",
      "        0.0685, 0.1484, 0.1367, 0.0835, 0.0863, 0.0726, 0.1096, 0.2722,\n",
      "        0.2623, 0.1786, 0.0831, 0.1083, 0.2398, 0.1007, 0.2016, 0.0922,\n",
      "        0.2274, 0.2596, 0.1111, 0.096 , 0.0717, 0.2325, 0.0888, 0.2371,\n",
      "        0.1561, 0.2833, 0.0972, 0.0706, 0.0728, 0.1999, 0.2448, 0.0835,\n",
      "        0.228 , 0.0563, 0.0652, 0.0778, 0.1844, 0.2195, 0.1632, 0.2574,\n",
      "        0.2889, 0.0939, 0.0733, 0.1659, 0.1104, 0.0935, 0.2682, 0.0831,\n",
      "        0.0521, 0.1483, 0.0733, 0.2681, 0.0819, 0.1707, 0.2542, 0.091 ,\n",
      "        0.0559, 0.0548, 0.2362, 0.1135, 0.2611, 0.4017, 0.3568, 0.2314,\n",
      "        0.2465, 0.1216, 0.0834, 0.107 , 0.0826, 0.2622, 0.101 , 0.0653,\n",
      "        0.1222, 0.2199, 0.0705, 0.1138, 0.229 , 0.0631, 0.092 , 0.1566,\n",
      "        0.0946, 0.0945, 0.0875, 0.0786, 0.1573, 0.1121, 0.1471, 0.2425,\n",
      "        0.0783, 0.1105, 0.1121, 0.0607, 0.1135, 0.1103, 0.1612, 0.2611,\n",
      "        0.1193, 0.0775, 0.2517, 0.0536, 0.296 , 0.2965, 0.1049, 0.1429,\n",
      "        0.288 , 0.2407, 0.1332, 0.1746, 0.2274, 0.2441, 0.1174, 0.0778,\n",
      "        0.0744, 0.1399, 0.236 , 0.1092, 0.0831, 0.139 , 0.1345, 0.1394,\n",
      "        0.2486, 0.0869, 0.0689, 0.0568, 0.1742, 0.2091, 0.2853, 0.1315,\n",
      "        0.1791, 0.1109, 0.2784, 0.0707, 0.0525, 0.0976, 0.2573, 0.101 ,\n",
      "        0.078 , 0.1347, 0.2421, 0.1537, 0.0758, 0.0706, 0.0692, 0.245 ,\n",
      "        0.245 , 0.072 , 0.0834, 0.2975, 0.119 , 0.3032, 0.1137, 0.0861,\n",
      "        0.0729, 0.1572, 0.2592, 0.0825, 0.126 , 0.0658, 0.061 , 0.0557,\n",
      "        0.1284, 0.0925, 0.0683, 0.2672, 0.113 , 0.0862, 0.0626, 0.2071,\n",
      "        0.1931, 0.2673, 0.0753, 0.0932, 0.134 , 0.1846, 0.0877, 0.0811,\n",
      "        0.2548, 0.0758, 0.1281, 0.2935, 0.2351, 0.1236, 0.0755, 0.0735,\n",
      "        0.2368, 0.0705, 0.2319, 0.0527, 0.0927, 0.0736, 0.0673, 0.224 ,\n",
      "        0.0827, 0.2452, 0.0607, 0.2089, 0.1715, 0.0832, 0.1056, 0.0827,\n",
      "        0.0831, 0.0789, 0.2209, 0.1933, 0.2371, 0.3201, 0.0876, 0.1246,\n",
      "        0.078 , 0.0725, 0.0692, 0.0574, 0.4951, 0.2549, 0.1343, 0.0834,\n",
      "        0.2171, 0.2625, 0.1484, 0.0599, 0.0752, 0.0653, 0.149 , 0.2496,\n",
      "        0.0757, 0.1609, 0.1777, 0.1175, 0.2463, 0.1347, 0.2362, 0.0942,\n",
      "        0.0952, 0.0689, 0.2227, 0.1341, 0.2514, 0.0642, 0.1509, 0.2287,\n",
      "        0.0838, 0.0497, 0.0865, 0.2274, 0.0795, 0.1647, 0.266 , 0.0833,\n",
      "        0.0725, 0.1176, 0.2661, 0.2726, 0.3139, 0.124 , 0.236 , 0.0963,\n",
      "        0.2103, 0.0872, 0.2328, 0.1197, 0.2199, 0.2314, 0.2899, 0.0871,\n",
      "        0.2099, 0.1395, 0.0964, 0.2768, 0.097 , 0.1843, 0.2444, 0.0831,\n",
      "        0.1936, 0.1187, 0.0888, 0.0791, 0.118 , 0.0949, 0.2224, 0.5013,\n",
      "        0.248 , 0.0848, 0.0869, 0.2737, 0.1172, 0.2072, 0.0885, 0.1703,\n",
      "        0.0791, 0.0577, 0.1494, 0.1075, 0.0801, 0.0795, 0.0702, 0.0526,\n",
      "        0.1804, 0.2602, 0.2954, 0.1157, 0.0912, 0.1067, 0.2212, 0.2584,\n",
      "        0.1422, 0.1631, 0.2114, 0.2284, 0.1639, 0.1172, 0.0747, 0.2638,\n",
      "        0.1333, 0.2549, 0.1015, 0.2161, 0.0961, 0.1629, 0.2459, 0.2603,\n",
      "        0.0956, 0.1677, 0.2808, 0.092 , 0.2327, 0.2234, 0.086 , 0.2431,\n",
      "        0.088 , 0.1818, 0.2986, 0.2077, 0.3035, 0.0963, 0.1224, 0.2678,\n",
      "        0.2146, 0.0839, 0.0703, 0.1501, 0.2506, 0.1148, 0.1057, 0.1018,\n",
      "        0.1411, 0.0786, 0.2245, 0.1498, 0.0964, 0.0783, 0.2372, 0.2936,\n",
      "        0.115 , 0.1739, 0.2754, 0.1037, 0.0781, 0.2902, 0.1154, 0.0785,\n",
      "        0.1464, 0.2845, 0.1004, 0.1237, 0.0658, 0.053 , 0.1039, 0.2672,\n",
      "        0.0913, 0.1055, 0.1092, 0.0741, 0.0756, 0.0595, 0.0688, 0.2534,\n",
      "        0.061 , 0.1233, 0.1008, 0.0788, 0.0685, 0.0586, 0.0654, 0.1106,\n",
      "        0.2051, 0.1446, 0.2717, 0.1586, 0.0861, 0.0686, 0.2246, 0.1213,\n",
      "        0.0924, 0.061 , 0.1034, 0.258 , 0.1088, 0.229 , 0.2195, 0.146 ,\n",
      "        0.2446, 0.1312, 0.2066, 0.2445, 0.2624, 0.1665, 0.1311, 0.207 ,\n",
      "        0.1135, 0.105 , 0.0834, 0.1522, 0.2579, 0.1439, 0.0897, 0.1496,\n",
      "        0.1102, 0.1212, 0.2503, 0.2592, 0.0908, 0.1256, 0.2577, 0.3104,\n",
      "        0.1322, 0.3204, 0.1258, 0.0789, 0.0798, 0.0831, 0.1361, 0.2723,\n",
      "        0.0659, 0.0825, 0.1108, 0.0658, 0.0606, 0.1695, 0.0831, 0.0706,\n",
      "        0.1229, 0.3329, 0.0777, 0.0886, 0.2582, 0.1247, 0.078 , 0.0794,\n",
      "        0.0578, 0.1369, 0.0658, 0.1545, 0.276 , 0.0796, 0.2284, 0.0654,\n",
      "        0.0628, 0.0762, 0.0758, 0.0582, 0.0942, 0.2771, 0.098 , 0.2076,\n",
      "        0.3086, 0.0786, 0.0831, 0.0681, 0.0833, 0.0658, 0.1148, 0.1137,\n",
      "        0.2855, 0.1209, 0.0773, 0.0759, 0.1495, 0.2858, 0.1088, 0.1072,\n",
      "        0.2888, 0.0877, 0.2781, 0.0924, 0.1437, 0.2894, 0.3193, 0.1781,\n",
      "        0.0882, 0.0833, 0.2912, 0.1499, 0.2338, 0.1003, 0.176 , 0.2246,\n",
      "        0.1492, 0.1188, 0.0753, 0.0787, 0.2706, 0.1493, 0.2228, 0.071 ,\n",
      "        0.2467, 0.1635, 0.2583, 0.2473, 0.0943, 0.273 , 0.1533, 0.2653,\n",
      "        0.1197, 0.0945, 0.0683, 0.061 , 0.1598, 0.2888, 0.1717, 0.2736,\n",
      "        0.1333, 0.0784, 0.0935, 0.1149, 0.2085, 0.2643, 0.2559, 0.2465,\n",
      "        0.2579, 0.1117, 0.247 , 0.2588, 0.0909, 0.0682, 0.2246, 0.2359,\n",
      "        0.1894, 0.2599, 0.2575, 0.3412, 0.1614, 0.0801, 0.1666, 0.0804,\n",
      "        0.0609, 0.232 , 0.1715, 0.2024, 0.2146, 0.2771, 0.1879, 0.1344,\n",
      "        0.1778, 0.1455, 0.1009, 0.0881, 0.0701, 0.0912, 0.1706, 0.2116,\n",
      "        0.1485, 0.0921, 0.1477, 0.2258, 0.0644, 0.241 , 0.2109, 0.0735,\n",
      "        0.2363, 0.2605, 0.1234, 0.1272, 0.1947, 0.2898, 0.1264, 0.0959,\n",
      "        0.1919, 0.1458, 0.1137, 0.2566, 0.0576, 0.1006, 0.1857, 0.2066,\n",
      "        0.267 , 0.1493, 0.1005, 0.0687, 0.1002, 0.1962, 0.2974, 0.2797,\n",
      "        0.1446, 0.122 , 0.0807, 0.078 , 0.0825, 0.2543, 0.218 , 0.0607,\n",
      "        0.0756, 0.2271, 0.065 , 0.0902, 0.2316, 0.0928, 0.0832, 0.1387,\n",
      "        0.1006, 0.0782, 0.0704, 0.0578, 0.106 ])\n",
      " array([0.1469, 0.1643, 0.1058, ..., 0.1189, 0.075 , 0.0564])\n",
      " array([0.073 , 0.0734, 0.0584, ..., 0.1482, 0.1081, 0.1395])\n",
      " array([0.0933, 0.1265, 0.1098, ..., 0.1246, 0.1262, 0.1456])\n",
      " array([0.1353, 0.099 , 0.1179, ..., 0.0994, 0.1045, 0.1399])\n",
      " array([0.0638, 0.11  , 0.1146, ..., 0.1033, 0.0755, 0.0914])\n",
      " array([0.237 , 0.0607, 0.1674, ..., 0.0655, 0.0956, 0.0672])\n",
      " array([0.073 , 0.0828, 0.1006, ..., 0.0834, 0.0649, 0.0964])\n",
      " array([0.0964, 0.0963, 0.0879, ..., 0.0745, 0.1052, 0.1146])\n",
      " array([0.0596, 0.0924, 0.171 , ..., 0.0831, 0.1013, 0.1184])\n",
      " array([0.0892, 0.0591, 0.0665, ..., 0.0837, 0.0665, 0.0815])\n",
      " array([0.2195, 0.2326, 0.1748, 0.1544, 0.2424, 0.22  , 0.1974, 0.1966,\n",
      "        0.2506, 0.1039, 0.1655, 0.176 , 0.1933, 0.2068, 0.2019, 0.1686,\n",
      "        0.2104, 0.1811, 0.2136, 0.231 , 0.2717, 0.1969, 0.2092, 0.1668,\n",
      "        0.2768, 0.1815, 0.254 , 0.1573, 0.1792, 0.2098, 0.407 , 0.2114,\n",
      "        0.2807, 0.219 , 0.2279, 0.1944, 0.2247, 0.2203, 0.2327, 0.2725,\n",
      "        0.2099, 0.2846, 0.2148, 0.1795, 0.2238, 0.2038, 0.1892, 0.2329,\n",
      "        0.1855, 0.2165, 0.238 , 0.186 , 0.2199, 0.2026, 0.2212, 0.2549,\n",
      "        0.295 , 0.2624, 0.2067, 0.3626, 0.2506, 0.2667, 0.1912, 0.233 ,\n",
      "        0.1719, 0.189 , 0.1794, 0.3466, 0.334 , 0.2632, 0.228 , 0.2502,\n",
      "        0.1486, 0.2849, 0.1832, 0.2497, 0.2102, 0.3814, 0.485 , 0.245 ,\n",
      "        0.2416, 0.2286, 0.1883, 0.1416, 0.181 , 0.2031, 0.262 , 0.1738,\n",
      "        0.2271, 0.3765, 0.2012, 0.2011, 0.2567, 0.1759, 0.2859, 0.2002,\n",
      "        0.1929, 0.2253, 0.1912, 0.3199, 0.2698, 0.1751, 0.2182, 0.1523,\n",
      "        0.14  , 0.1219, 0.222 , 0.1684, 0.2045, 0.2007, 0.2389, 0.2055,\n",
      "        0.1727, 0.2633, 0.2624, 0.3774, 0.1475, 0.2442, 0.2132, 0.3621,\n",
      "        0.2088, 0.2309, 0.2338, 0.2357, 0.2649, 0.2315, 0.1478, 0.2668,\n",
      "        0.2443, 0.3321, 0.1699, 0.2313, 0.1914, 0.1596, 0.1902, 0.1923,\n",
      "        0.1681, 0.2255, 0.191 , 0.1816, 0.2094, 0.2056, 0.2079, 0.3399,\n",
      "        0.1687, 0.2354, 0.1908, 0.1879, 0.2441, 0.1923, 0.2165, 0.1692,\n",
      "        0.1873, 0.1701, 0.1417, 0.2012, 0.1999, 0.1695, 0.3924, 0.1609,\n",
      "        0.196 , 0.3221, 0.2191, 0.1308, 0.207 , 0.2605, 0.2055, 0.2094,\n",
      "        0.1388, 0.2578, 0.3707, 0.126 , 0.1921, 0.2258, 0.1311, 0.2046,\n",
      "        0.2091, 0.1666, 0.2492, 0.1607, 0.1721, 0.1963, 0.21  , 0.2205,\n",
      "        0.196 , 0.2138, 0.1597, 0.3269, 0.2936, 0.204 , 0.2045, 0.1926,\n",
      "        0.2337, 0.2426, 0.1714, 0.2074, 0.1966, 0.2544, 0.2179, 0.2125,\n",
      "        0.2459, 0.4673, 0.1559, 0.2188, 0.2   , 0.191 , 0.1888, 0.3152,\n",
      "        0.2635, 0.2253, 0.2824, 0.1482, 0.2047, 0.1445, 0.1741, 0.2781,\n",
      "        0.2969, 0.2655, 0.2081, 0.2508, 0.2108, 0.2255, 0.3322, 0.1296,\n",
      "        0.227 , 0.1449, 0.1967, 0.1937, 0.1396, 0.1823, 0.2155, 0.1967,\n",
      "        0.196 , 0.2106, 0.2075, 0.1693, 0.1877, 0.1905, 0.1574, 0.2087,\n",
      "        0.2553, 0.205 , 0.2616, 0.3616, 0.1699, 0.1774, 0.2307, 0.1731,\n",
      "        0.2636, 0.1363, 0.1293, 0.2171, 0.223 , 0.1258, 0.1753, 0.2141,\n",
      "        0.2022, 0.2646, 0.179 , 0.2489, 0.1122, 0.2281, 0.1917, 0.1794,\n",
      "        0.196 , 0.1759, 0.1644, 0.1255, 0.2031, 0.1448, 0.2025, 0.2117,\n",
      "        0.2116, 0.1971, 0.1482, 0.1636, 0.2223, 0.2706, 0.2613, 0.1672,\n",
      "        0.1644, 0.192 , 0.1791, 0.2115, 0.1999, 0.2275, 0.2064, 0.1366,\n",
      "        0.2144, 0.1834, 0.1407, 0.2119, 0.2221, 0.2192, 0.1786, 0.1158,\n",
      "        0.1822, 0.2476, 0.1249, 0.1919, 0.2001, 0.1479, 0.2041, 0.1904,\n",
      "        0.187 , 0.2051, 0.213 , 0.2527, 0.1635, 0.1617, 0.2253, 0.1993,\n",
      "        0.1784, 0.1829, 0.1755, 0.2125, 0.1817, 0.4613, 0.1954, 0.1232,\n",
      "        0.2545, 0.2064, 0.1836, 0.188 , 0.2179, 0.1561, 0.1921, 0.3003,\n",
      "        0.2394, 0.1489, 0.1916, 0.1487, 0.1808, 0.1884, 0.1965, 0.2045,\n",
      "        0.2926, 0.0308, 0.1233, 0.1802, 0.1652, 0.1644, 0.189 , 0.1536,\n",
      "        0.1458, 0.1749, 0.2324, 0.2127, 0.1457, 0.1542, 0.2143, 0.1608,\n",
      "        0.2267, 0.2571, 0.1621, 0.1838, 0.2103, 0.2312, 0.2743, 0.1661,\n",
      "        0.1431, 0.1881, 0.2396, 0.136 , 0.1758, 0.1615, 0.148 , 0.1762,\n",
      "        0.1891, 0.1483, 0.1658, 0.1915, 0.179 , 0.224 , 0.1613, 0.1427,\n",
      "        0.3901, 0.1997, 0.2491, 0.1712, 0.2097, 0.2194, 0.1874, 0.2009,\n",
      "        0.1665, 0.1889, 0.2276, 0.2051, 0.2672, 0.2363, 0.1284, 0.2145,\n",
      "        0.191 , 0.2147, 0.1815, 0.1453, 0.1284, 0.1534, 0.1913, 0.1654,\n",
      "        0.1869, 0.1922, 0.2191, 0.2271, 0.215 , 0.1708, 0.1909, 0.1834,\n",
      "        0.1988, 0.1385, 0.2012, 0.2135, 0.1696, 0.2046, 0.2434, 0.2049,\n",
      "        0.1997, 0.1561, 0.378 , 0.1547, 0.1948, 0.1511, 0.1647, 0.1597,\n",
      "        0.2128, 0.223 , 0.2893, 0.1693, 0.1297, 0.1606, 0.1555, 0.1953,\n",
      "        0.1827, 0.1996, 0.2225, 0.2002, 0.215 , 0.1999, 0.1779, 0.2127,\n",
      "        0.1926, 0.1826, 0.1564, 0.1479, 0.1296, 0.2184, 0.2002, 0.2175,\n",
      "        0.2262, 0.2358, 0.192 , 0.1588, 0.2081, 0.1658, 0.2115, 0.1719,\n",
      "        0.1716, 0.2022, 0.2228, 0.1794, 0.2194, 0.113 , 0.1846, 0.1532,\n",
      "        0.1901, 0.1623, 0.1921, 0.162 , 0.2018, 0.1673, 0.1698, 0.1674,\n",
      "        0.2402, 0.1667, 0.1787, 0.2574, 0.1894, 0.1849, 0.1783, 0.1615,\n",
      "        0.1631, 0.1302, 0.174 , 0.1503, 0.2017, 0.1533, 0.201 , 0.1918,\n",
      "        0.1655, 0.1767, 0.2094, 0.1444, 0.166 , 0.2064, 0.1889, 0.1935,\n",
      "        0.1714, 0.1923, 0.1703, 0.1998, 0.1577, 0.2081, 0.1562, 0.1309,\n",
      "        0.1604, 0.1469, 0.2138, 0.1774, 0.163 , 0.148 , 0.1875, 0.2058,\n",
      "        0.1795, 0.1856, 0.1467, 0.2085, 0.3789, 0.1735, 0.2297, 0.1736,\n",
      "        0.1406, 0.2141, 0.1962, 0.1767, 0.2168, 0.2015, 0.2176, 0.154 ,\n",
      "        0.2093, 0.1959, 0.2047, 0.144 , 0.1452, 0.1913, 0.2198, 0.1657,\n",
      "        0.1305, 0.1309, 0.2571, 0.1959, 0.1835, 0.342 , 0.1653, 0.1435,\n",
      "        0.1827, 0.1656, 0.1613, 0.1531, 0.2106, 0.1836, 0.1433, 0.202 ,\n",
      "        0.1793, 0.1692, 0.219 , 0.1836, 0.1928, 0.1892, 0.2097, 0.1613,\n",
      "        0.184 , 0.1719, 0.2565, 0.1988, 0.292 , 0.2177, 0.1394, 0.148 ,\n",
      "        0.1088, 0.1556, 0.1481, 0.1567, 0.2256, 0.1081, 0.1741, 0.2089,\n",
      "        0.1976, 0.2127, 0.1827, 0.1915, 0.1959, 0.1915, 0.1307, 0.2007,\n",
      "        0.1967, 0.2034, 0.1791, 0.2085, 0.2485, 0.154 , 0.1186, 0.1658,\n",
      "        0.1176, 0.1283, 0.1256, 0.1005, 0.1664, 0.1705, 0.1515, 0.1398,\n",
      "        0.2068, 0.1721, 0.1667, 0.2238, 0.1971, 0.1502, 0.1319, 0.2327,\n",
      "        0.1615, 0.1267, 0.1208, 0.1438, 0.1664, 0.1148, 0.2143, 0.1786,\n",
      "        0.2361, 0.222 , 0.1328, 0.1661, 0.1612, 0.2674, 0.1579, 0.2239,\n",
      "        0.1863, 0.2622, 0.2289, 0.1535, 0.1278, 0.1352, 0.1793, 0.1937,\n",
      "        0.2153, 0.1541, 0.1838, 0.1958, 0.1864, 0.1972, 0.1836, 0.1425,\n",
      "        0.1913, 0.1844, 0.1614, 0.2114, 0.1433, 0.1309, 0.1835, 0.1711,\n",
      "        0.1909, 0.1421, 0.1745, 0.192 , 0.1968, 0.1786, 0.1836, 0.1997,\n",
      "        0.1576, 0.2573, 0.2466, 0.2129, 0.1615, 0.1545, 0.2047, 0.1622,\n",
      "        0.1744, 0.1301])\n",
      " array([0.1494, 0.1281, 0.1263, ..., 0.0883, 0.1066, 0.055 ])\n",
      " array([0.0828, 0.0656, 0.0657, ..., 0.0747, 0.0878, 0.0705])\n",
      " array([0.1311, 0.1301, 0.0829, ..., 0.1498, 0.1014, 0.1101])\n",
      " array([0.2252, 0.1104, 0.1003, 0.1011, 0.1715, 0.101 , 0.1038, 0.1853,\n",
      "        0.2375, 0.1215, 0.2156, 0.122 , 0.2886, 0.0818, 0.1429, 0.1914,\n",
      "        0.1101, 0.2318, 0.1103, 0.2415, 0.1006, 0.2281, 0.098 , 0.1008,\n",
      "        0.0883, 0.1749, 0.1812, 0.2567, 0.274 , 0.1169, 0.1068, 0.1083,\n",
      "        0.2606, 0.0919, 0.113 , 0.0946, 0.0959, 0.0734, 0.1412, 0.1692,\n",
      "        0.1664, 0.1936, 0.1735, 0.1323, 0.1359, 0.2153, 0.1482, 0.2142,\n",
      "        0.1105, 0.1131, 0.2041, 0.2142, 0.1254, 0.1381, 0.1307, 0.1709,\n",
      "        0.3099, 0.1575, 0.127 , 0.2312, 0.1956, 0.1098, 0.1408, 0.1929,\n",
      "        0.1154, 0.1976, 0.1166, 0.2008, 0.2709, 0.1772, 0.3454, 0.2186,\n",
      "        0.1388, 0.171 , 0.1313, 0.205 , 0.152 , 0.2844, 0.1274, 0.1877,\n",
      "        0.1435, 0.126 , 0.2172, 0.166 , 0.1617, 0.1712, 0.1355, 0.1391,\n",
      "        0.1176, 0.2569, 0.167 , 0.1491, 0.2314, 0.1757, 0.2146, 0.1357,\n",
      "        0.1592, 0.1255, 0.3089, 0.1477, 0.1551, 0.1655, 0.1957, 0.1229,\n",
      "        0.2086, 0.1659, 0.1403, 0.1386, 0.139 , 0.1398, 0.1602, 0.1183,\n",
      "        0.1343, 0.1296, 0.1336, 0.2784, 0.1715, 0.1264, 0.1241, 0.1169,\n",
      "        0.1789, 0.1353, 0.1305, 0.1616, 0.1957, 0.2509, 0.0995, 0.1379,\n",
      "        0.1252, 0.2326, 0.1222, 0.212 , 0.1336, 0.1217, 0.1516, 0.2013,\n",
      "        0.2649, 0.131 , 0.2793, 0.1788, 0.1567, 0.1584, 0.2963, 0.1908,\n",
      "        0.2586, 0.2049, 0.2444, 0.1174, 0.1531, 0.2036, 0.1404, 0.1364,\n",
      "        0.1399, 0.1425, 0.3009, 0.1298, 0.2419, 0.1209, 0.2039, 0.1306,\n",
      "        0.123 , 0.1358, 0.1837, 0.1379, 0.1305, 0.1333, 0.1179, 0.2501,\n",
      "        0.1356, 0.1171, 0.2575, 0.1355, 0.1093, 0.1048, 0.217 , 0.1487,\n",
      "        0.4899, 0.1254, 0.0683, 0.2011, 0.1071, 0.1517, 0.125 , 0.1233,\n",
      "        0.2058, 0.2961, 0.1913, 0.1736, 0.1072, 0.2381, 0.1297, 0.1099,\n",
      "        0.2036, 0.1452, 0.1044, 0.1178, 0.1226, 0.0916, 0.2313, 0.1489,\n",
      "        0.142 , 0.1271, 0.1002, 0.2746, 0.1572, 0.1256, 0.0901, 0.1189,\n",
      "        0.1223, 0.1209, 0.1406, 0.3714, 0.3319, 0.1248, 0.1176, 0.1054,\n",
      "        0.1035, 0.1734, 0.125 , 0.1849, 0.1175, 0.105 , 0.1075, 0.1032,\n",
      "        0.1307, 0.1055, 0.2455, 0.1031, 0.154 , 0.1426, 0.2406, 0.196 ,\n",
      "        0.2973, 0.1709, 0.139 , 0.2269, 0.4134, 0.1572, 0.1132, 0.1417,\n",
      "        0.1474, 0.2357, 0.1783, 0.1957, 0.1311, 0.1304, 0.13  , 0.2136,\n",
      "        0.1257, 0.1179, 0.1746, 0.1536, 0.2718, 0.167 , 0.1279, 0.334 ,\n",
      "        0.1214, 0.2368, 0.1257, 0.1513, 0.2266, 0.1193, 0.1141, 0.1009,\n",
      "        0.1831, 0.2449, 0.0998, 0.118 , 0.3198, 0.1058, 0.1239, 0.1226,\n",
      "        0.1538, 0.161 , 0.1456, 0.1083, 0.1351, 0.1329, 0.1557, 0.1965,\n",
      "        0.1525, 0.1342, 0.1269, 0.1221, 0.2037, 0.1574, 0.2615, 0.1909,\n",
      "        0.1393, 0.1434, 0.1178, 0.3315, 0.1027, 0.1193, 0.1734, 0.2272,\n",
      "        0.1491, 0.1212, 0.1466, 0.1736, 0.2312, 0.1255, 0.1514, 0.1477,\n",
      "        0.1314, 0.3436, 0.1128, 0.0918, 0.1701, 0.1432, 0.1464, 0.1001,\n",
      "        0.0879, 0.1004, 0.1257, 0.1545, 0.1135, 0.2655, 0.0872, 0.1306,\n",
      "        0.32  , 0.1575, 0.1082, 0.2628, 0.1352, 0.1483, 0.2119, 0.1123,\n",
      "        0.1204, 0.1119, 0.145 , 0.1357, 0.2352, 0.1391, 0.1591, 0.2599,\n",
      "        0.1456, 0.1211, 0.3203, 0.1183, 0.0887, 0.2247, 0.1239, 0.1134,\n",
      "        0.1617, 0.2575, 0.1051, 0.2475, 0.128 , 0.1265, 0.1323, 0.1138,\n",
      "        0.1139, 0.18  , 0.1082, 0.0898, 0.0996, 0.0871, 0.1957, 0.1224,\n",
      "        0.0862, 0.0993, 0.1067, 0.2003, 0.1956, 0.1144, 0.1049, 0.1   ,\n",
      "        0.148 , 0.2269, 0.1212, 0.2407, 0.1129, 0.2307, 0.2632, 0.1477,\n",
      "        0.1607, 0.1396, 0.2809, 0.1227, 0.245 , 0.1308, 0.1163, 0.2836,\n",
      "        0.1329, 0.14  , 0.1607, 0.2919, 0.2029, 0.1663, 0.1643, 0.2414,\n",
      "        0.1375, 0.0992, 0.1475, 0.2307, 0.13  , 0.2393, 0.2052, 0.1125,\n",
      "        0.1428, 0.1301, 0.234 , 0.1227, 0.143 , 0.1783, 0.1123, 0.1874,\n",
      "        0.2489, 0.1096, 0.105 , 0.2193, 0.1032, 0.158 , 0.1956, 0.1572,\n",
      "        0.135 , 0.2386, 0.1434, 0.1526, 0.1477, 0.2715, 0.1435, 0.1304,\n",
      "        0.1355, 0.1965, 0.1436, 0.1002, 0.1352, 0.1403, 0.2104, 0.1124,\n",
      "        0.1208, 0.1306, 0.138 , 0.249 , 0.1359, 0.2085, 0.1406, 0.1402,\n",
      "        0.161 , 0.2274, 0.0898, 0.143 , 0.17  , 0.1401, 0.1064, 0.2656,\n",
      "        0.153 , 0.1306, 0.5206, 0.2591, 0.1433, 0.1724, 0.1786, 0.1053,\n",
      "        0.1794, 0.1314, 0.1414, 0.2929, 0.2042, 0.3645, 0.2331, 0.178 ,\n",
      "        0.1194, 0.1234, 0.2011, 0.178 , 0.1878, 0.1622, 0.1559, 0.2175,\n",
      "        0.1223, 0.1208, 0.2063, 0.135 , 0.2792, 0.2109, 0.153 , 0.1472,\n",
      "        0.1303, 0.2662, 0.1301, 0.1093, 0.1949, 0.1434, 0.2454, 0.1174,\n",
      "        0.1315, 0.179 , 0.2798, 0.157 , 0.1482, 0.2449, 0.0925, 0.2023,\n",
      "        0.1315, 0.2244, 0.1709, 0.1361, 0.1184, 0.1659, 0.2226, 0.2108,\n",
      "        0.1172, 0.1362, 0.1256, 0.0941, 0.1089, 0.2927, 0.1365, 0.1188,\n",
      "        0.119 , 0.1413, 0.145 , 0.1414, 0.134 , 0.211 , 0.2353, 0.1311,\n",
      "        0.1714, 0.151 , 0.2523, 0.1481, 0.2131, 0.1481, 0.1558, 0.2635,\n",
      "        0.1493, 0.1222, 0.1086, 0.3101, 0.1891, 0.1246, 0.16  , 0.1888,\n",
      "        0.1215, 0.1926, 0.1407, 0.0964, 0.1974, 0.2477, 0.1402, 0.149 ,\n",
      "        0.2641, 0.1447, 0.1745, 0.211 , 0.128 , 0.1202, 0.1588, 0.1491,\n",
      "        0.2348, 0.1256, 0.1399, 0.0645, 0.1765, 0.2311, 0.132 , 0.118 ,\n",
      "        0.2892, 0.1387, 0.1599, 0.2041, 0.1535, 0.1437, 0.1488, 0.1991,\n",
      "        0.2016, 0.0697, 0.1446, 0.35  , 0.1231, 0.1227, 0.1171, 0.1531,\n",
      "        0.1755, 0.1655, 0.1255, 0.1376, 0.1305, 0.1559, 0.225 , 0.1383,\n",
      "        0.1052, 0.1184, 0.0972, 0.1956, 0.1433, 0.1907, 0.2441, 0.1924,\n",
      "        0.1376, 0.1279, 0.1078, 0.1483, 0.1226, 0.1307, 0.1654, 0.2435,\n",
      "        0.0951, 0.0947, 0.1477, 0.153 , 0.2561, 0.1202, 0.1612, 0.1397,\n",
      "        0.1303, 0.2737, 0.1381, 0.1259, 0.1125, 0.1357, 0.1204, 0.1489,\n",
      "        0.3011, 0.1426, 0.1256, 0.2438, 0.2458, 0.1185, 0.1429, 0.1439,\n",
      "        0.3109, 0.1433, 0.2442, 0.2362, 0.1697, 0.1272, 0.1427, 0.2358,\n",
      "        0.1273, 0.2535, 0.148 , 0.2531, 0.1306, 0.1351, 0.1476, 0.1478,\n",
      "        0.1861, 0.1585, 0.1177, 0.2679, 0.1564, 0.1381, 0.2228, 0.156 ,\n",
      "        0.1747, 0.2357, 0.1562, 0.1399, 0.1461, 0.1273, 0.1172, 0.2217,\n",
      "        0.2289, 0.1358, 0.1225, 0.126 , 0.1229, 0.289 , 0.2088, 0.1123,\n",
      "        0.126 , 0.1384, 0.1258, 0.2312, 0.1406, 0.3443, 0.2482, 0.1307,\n",
      "        0.1521, 0.2441, 0.1476, 0.195 , 0.1804, 0.1051, 0.2571, 0.1132,\n",
      "        0.1151, 0.1185, 0.1652, 0.1955, 0.1187, 0.1818, 0.1704, 0.1203,\n",
      "        0.1427, 0.2475, 0.1424, 0.231 , 0.1609, 0.2661, 0.1158, 0.1694,\n",
      "        0.1355, 0.274 , 0.1584, 0.1624, 0.2468, 0.1448, 0.191 , 0.1174,\n",
      "        0.1527, 0.2789, 0.2126, 0.1037, 0.2307, 0.1832, 0.1139, 0.2129,\n",
      "        0.1405, 0.1403, 0.1963, 0.1403, 0.2994, 0.1708, 0.2437, 0.1481,\n",
      "        0.191 , 0.1696, 0.1399, 0.196 , 0.1661, 0.1076, 0.113 , 0.1258,\n",
      "        0.1185, 0.1659, 0.1307, 0.2735, 0.0361, 0.153 , 0.1002, 0.2156,\n",
      "        0.1389, 0.1708, 0.1609, 0.1122, 0.1388, 0.1677, 0.1239, 0.2143,\n",
      "        0.1986, 0.1432, 0.1612, 0.1967, 0.1305, 0.1177, 0.1207, 0.1176,\n",
      "        0.1221, 0.2514, 0.191 , 0.2455, 0.1839, 0.1177, 0.2042, 0.2567,\n",
      "        0.1448, 0.1134, 0.1631, 0.1787, 0.2181, 0.1306, 0.1393, 0.2083,\n",
      "        0.2888, 0.1439, 0.3565, 0.1402, 0.2536, 0.3018, 0.1491, 0.2184,\n",
      "        0.1443, 0.1119, 0.1833, 0.2792, 0.1383, 0.1403, 0.1626, 0.1804,\n",
      "        0.1351, 0.1307, 0.1275, 0.2039, 0.2313, 0.1398, 0.1314, 0.1349,\n",
      "        0.1429, 0.2336, 0.1361, 0.1177, 0.1662, 0.0967, 0.2393, 0.1448,\n",
      "        0.1654, 0.3241, 0.1144, 0.1363, 0.2219, 0.1372, 0.1344, 0.1398,\n",
      "        0.2654, 0.1403, 0.1142, 0.1277, 0.1092, 0.1253, 0.2967, 0.127 ,\n",
      "        0.1314, 0.0954, 0.1241, 0.135 , 0.1437, 0.1107, 0.0958, 0.2662,\n",
      "        0.126 , 0.1134, 0.1072, 0.1404, 0.1578, 0.1352, 0.1922, 0.1292,\n",
      "        0.4933, 0.0856, 0.0914, 0.2309, 0.1212, 0.1173, 0.2396, 0.1152,\n",
      "        0.0924, 0.1589, 0.2616, 0.0299, 0.2438, 0.1307, 0.1216, 0.151 ,\n",
      "        0.1271, 0.1222, 0.2835, 0.1482, 0.2517, 0.0663, 0.0827, 0.0957,\n",
      "        0.1136, 0.1213, 0.3805, 0.1733, 0.1101, 0.2838, 0.2743, 0.1311,\n",
      "        0.1401, 0.2236, 0.1079, 0.1205, 0.1936, 0.1256, 0.1262, 0.1215,\n",
      "        0.1336, 0.2182, 0.1122, 0.1293, 0.1278, 0.1272, 0.2409, 0.1279,\n",
      "        0.1104, 0.3035, 0.1139, 0.1311, 0.1917, 0.1088, 0.1222, 0.1839,\n",
      "        0.1904, 0.2166, 0.0649, 0.1305, 0.2788, 0.13  , 0.1173, 0.1032,\n",
      "        0.2496, 0.2438, 0.1429, 0.1075, 0.135 , 0.1747, 0.1307, 0.135 ,\n",
      "        0.1231, 0.1421, 0.0995, 0.0599, 0.2091, 0.1968, 0.2138, 0.2139,\n",
      "        0.1507, 0.1247, 0.1129, 0.2765, 0.1049, 0.1583, 0.1653, 0.24  ,\n",
      "        0.1259, 0.1226, 0.165 , 0.1481, 0.2542, 0.2313, 0.1476, 0.1481])\n",
      " array([0.1112, 0.0983, 0.1204, ..., 0.0875, 0.1498, 0.1663])\n",
      " array([0.0949, 0.1079, 0.0912, 0.0775, 0.0814, 0.0762, 0.1082, 0.1111,\n",
      "        0.0861, 0.1266, 0.1089, 0.0936, 0.0952, 0.1266, 0.1041, 0.0986,\n",
      "        0.1337, 0.1002, 0.135 , 0.1009, 0.088 , 0.0927, 0.1056, 0.1051,\n",
      "        0.1351, 0.1009, 0.1013, 0.0876, 0.0955, 0.1541, 0.1145, 0.0856,\n",
      "        0.0829, 0.1034, 0.1137, 0.0907, 0.0876, 0.0755, 0.0867, 0.0936,\n",
      "        0.1113, 0.0468, 0.0772, 0.0858, 0.0973, 0.0959, 0.096 , 0.0824,\n",
      "        0.0986, 0.103 , 0.105 , 0.0878, 0.0826, 0.0699, 0.0857, 0.0745,\n",
      "        0.0927, 0.1083, 0.1233, 0.1001, 0.1146, 0.118 , 0.1078, 0.0956,\n",
      "        0.1185, 0.0905, 0.1054, 0.1003, 0.1005, 0.1   , 0.1283, 0.0857,\n",
      "        0.0909, 0.0972, 0.0907, 0.0826, 0.0924, 0.0871, 0.0782, 0.0952,\n",
      "        0.1179, 0.0951, 0.0926, 0.0798, 0.0903, 0.096 , 0.1212, 0.0952,\n",
      "        0.1052, 0.1004, 0.0876, 0.0856, 0.1004, 0.0832, 0.0926, 0.0955,\n",
      "        0.1038, 0.0906, 0.0876, 0.0952, 0.0856, 0.1103, 0.0842, 0.0863,\n",
      "        0.1065, 0.082 , 0.0684, 0.1131, 0.0975, 0.0943, 0.1121, 0.0867,\n",
      "        0.0993, 0.0945, 0.0911, 0.0946, 0.1205, 0.0823, 0.094 , 0.1117,\n",
      "        0.0897, 0.0947, 0.0817, 0.095 , 0.0926, 0.104 , 0.127 , 0.0994,\n",
      "        0.0737, 0.0893, 0.0885, 0.1019, 0.0719, 0.0786, 0.074 , 0.0818,\n",
      "        0.0859, 0.0988, 0.0832, 0.0939, 0.0918, 0.0756, 0.0867, 0.0808,\n",
      "        0.0994, 0.0814, 0.0777, 0.1033, 0.121 , 0.1004, 0.0953, 0.1092,\n",
      "        0.0958, 0.1071, 0.0823, 0.0989, 0.091 , 0.0987, 0.1136, 0.1014,\n",
      "        0.0858, 0.0808, 0.0764, 0.0862, 0.108 , 0.147 , 0.0945, 0.1035,\n",
      "        0.0934, 0.0993, 0.1024, 0.112 , 0.1212, 0.0847, 0.0809, 0.0987,\n",
      "        0.1047, 0.0942, 0.0765, 0.0991, 0.0715, 0.1064, 0.0941, 0.105 ,\n",
      "        0.0944, 0.0986, 0.1207, 0.081 , 0.1165, 0.0888, 0.0812, 0.0851,\n",
      "        0.1006, 0.1004, 0.1054, 0.0829, 0.1134, 0.0887, 0.0923, 0.1009,\n",
      "        0.1081, 0.1037, 0.0877, 0.0959, 0.0832, 0.0862, 0.0832, 0.0961,\n",
      "        0.1232, 0.0962, 0.117 , 0.1079, 0.0961, 0.1038, 0.0958, 0.092 ,\n",
      "        0.1436, 0.0952, 0.0982, 0.078 , 0.0943, 0.0993, 0.1003, 0.0861,\n",
      "        0.0976, 0.1085, 0.0882, 0.0713, 0.1038, 0.1057, 0.0779, 0.1007,\n",
      "        0.1328, 0.0919, 0.1003, 0.101 , 0.1004, 0.0884, 0.1007, 0.0777,\n",
      "        0.0861, 0.1101, 0.0914, 0.0968, 0.0859, 0.0938, 0.0954, 0.0813,\n",
      "        0.0812, 0.0761, 0.094 , 0.1044, 0.0884, 0.0864, 0.0819, 0.0917,\n",
      "        0.1012, 0.1185, 0.1054, 0.1208, 0.1014, 0.1139, 0.0729, 0.087 ,\n",
      "        0.1153, 0.1037, 0.0879, 0.1316, 0.0878, 0.1003, 0.1056, 0.097 ,\n",
      "        0.1235, 0.0885, 0.0776, 0.0918, 0.1417, 0.088 , 0.048 , 0.0831,\n",
      "        0.0762, 0.0927, 0.0913, 0.101 , 0.1058, 0.0885, 0.1105, 0.1572,\n",
      "        0.0943, 0.097 , 0.1397, 0.0904, 0.0918, 0.0815, 0.0866, 0.1   ,\n",
      "        0.1267, 0.0909, 0.1109, 0.0918, 0.0911, 0.1004, 0.0912, 0.1522,\n",
      "        0.1053, 0.0829, 0.0955, 0.1006, 0.0881, 0.1089, 0.0886, 0.0912,\n",
      "        0.0955, 0.0743, 0.0833, 0.0964, 0.1008, 0.1054, 0.0877, 0.0968,\n",
      "        0.0868, 0.0959, 0.11  , 0.0801, 0.118 , 0.1039, 0.0878, 0.0836,\n",
      "        0.0782, 0.0933, 0.1003, 0.0933, 0.0798, 0.1053, 0.0878, 0.0835,\n",
      "        0.0657, 0.0843, 0.1103, 0.0965, 0.0947, 0.12  , 0.0819, 0.0829,\n",
      "        0.088 , 0.0977, 0.1037, 0.1069, 0.0911, 0.0889, 0.0826, 0.0821,\n",
      "        0.0982, 0.0835, 0.1088, 0.0981, 0.0832, 0.0915, 0.0985, 0.0746,\n",
      "        0.0809, 0.0865, 0.0767, 0.0836, 0.0761, 0.1006, 0.0917, 0.0784,\n",
      "        0.0878, 0.0867, 0.1078, 0.0838, 0.0819, 0.103 , 0.082 , 0.0882,\n",
      "        0.0804, 0.0878, 0.088 , 0.1009, 0.1107, 0.149 , 0.091 , 0.1007,\n",
      "        0.0953, 0.1098, 0.0907, 0.1131, 0.1038, 0.0968, 0.08  , 0.0794,\n",
      "        0.0952, 0.0989, 0.121 , 0.0917, 0.0844, 0.0893, 0.0784, 0.0708,\n",
      "        0.0886, 0.091 , 0.1012, 0.0964, 0.0915, 0.0822, 0.1012, 0.0753,\n",
      "        0.0773, 0.0801, 0.1016, 0.106 , 0.0848, 0.0768, 0.0907, 0.0832,\n",
      "        0.076 , 0.093 , 0.0729, 0.0957, 0.115 , 0.1135, 0.095 , 0.0878,\n",
      "        0.096 , 0.1207, 0.107 , 0.0865, 0.0863, 0.1644, 0.1036, 0.0816,\n",
      "        0.0814, 0.0889, 0.1239, 0.0905, 0.1134, 0.0915, 0.0979, 0.078 ,\n",
      "        0.0879, 0.0894, 0.0796, 0.0783, 0.0981, 0.0807, 0.1103, 0.0784,\n",
      "        0.0975, 0.1007, 0.0783, 0.096 , 0.0882, 0.0788, 0.0779, 0.0882,\n",
      "        0.0864, 0.0814, 0.0765, 0.0869, 0.0756, 0.0705, 0.0834, 0.0964,\n",
      "        0.0885, 0.0833, 0.1181, 0.0991, 0.0808, 0.068 , 0.1207, 0.0814,\n",
      "        0.0734, 0.0854, 0.0937, 0.137 , 0.1202, 0.1193, 0.1021, 0.1138,\n",
      "        0.0937, 0.1142, 0.0817, 0.0988, 0.0814, 0.1026, 0.027 , 0.0885,\n",
      "        0.0932, 0.1023, 0.0911, 0.0529, 0.0963, 0.0665, 0.0817, 0.1067,\n",
      "        0.0813, 0.0903, 0.0781, 0.0951, 0.0886, 0.0642, 0.0788, 0.0935,\n",
      "        0.0934, 0.0845, 0.0956, 0.0836, 0.101 , 0.0749, 0.1065, 0.0608,\n",
      "        0.1108, 0.0799, 0.0751, 0.0804, 0.087 , 0.078 , 0.096 , 0.0835,\n",
      "        0.1316, 0.061 , 0.0977, 0.0879, 0.0868, 0.0735, 0.0936, 0.0962,\n",
      "        0.0831, 0.1041, 0.0834, 0.0759, 0.109 , 0.0782, 0.1084, 0.078 ,\n",
      "        0.0828, 0.0883, 0.1143, 0.1262, 0.0798, 0.1001, 0.1122, 0.1131,\n",
      "        0.0996, 0.0931, 0.0735, 0.0906, 0.0906, 0.1354, 0.1141, 0.0959,\n",
      "        0.0959, 0.0901, 0.1028, 0.1138, 0.0989, 0.094 , 0.0894, 0.0906,\n",
      "        0.0879, 0.0888, 0.114 , 0.0986, 0.0754, 0.0999, 0.0786, 0.0988,\n",
      "        0.1032, 0.1053, 0.0906, 0.114 , 0.0972, 0.0742, 0.0733, 0.1008,\n",
      "        0.1326, 0.1005, 0.1081, 0.0899, 0.0928, 0.097 , 0.1004, 0.1212,\n",
      "        0.1032, 0.1036, 0.1009, 0.0975, 0.0878, 0.0931, 0.1105, 0.1031,\n",
      "        0.1004, 0.1047, 0.0879, 0.0623, 0.0892, 0.0818, 0.0585, 0.0738,\n",
      "        0.0944, 0.1139, 0.0688, 0.0966, 0.0992, 0.0872, 0.0905, 0.0829,\n",
      "        0.1215, 0.0829, 0.106 , 0.0745, 0.086 , 0.0707, 0.0835, 0.1013,\n",
      "        0.117 , 0.0959, 0.0743, 0.1189, 0.1264, 0.0607, 0.0934, 0.0963,\n",
      "        0.1158, 0.1038, 0.0832, 0.1236, 0.0836, 0.1063, 0.106 , 0.1092,\n",
      "        0.1088, 0.0961, 0.0863, 0.1191, 0.0913, 0.0879, 0.1104, 0.0778,\n",
      "        0.093 , 0.0879, 0.071 , 0.0915, 0.0702, 0.0828, 0.0865, 0.0966,\n",
      "        0.0962, 0.075 , 0.1056, 0.1129, 0.1168, 0.0788, 0.1142, 0.1059,\n",
      "        0.0956, 0.0759, 0.082 , 0.0942, 0.0969, 0.0924, 0.1007, 0.08  ,\n",
      "        0.1106, 0.0798, 0.118 , 0.0833, 0.1005, 0.1005, 0.0926, 0.0907,\n",
      "        0.1215, 0.0958, 0.0833, 0.0892, 0.1003, 0.0828, 0.0884, 0.1147,\n",
      "        0.0783, 0.1   , 0.1276, 0.0827, 0.1124, 0.0832, 0.0903, 0.1179,\n",
      "        0.0869, 0.1177, 0.1006, 0.0959, 0.0518, 0.0862, 0.1053, 0.1051,\n",
      "        0.083 , 0.0826, 0.0951, 0.1003, 0.1261, 0.0953, 0.0973, 0.0906,\n",
      "        0.1179, 0.1036, 0.1137, 0.1001, 0.0877, 0.0831, 0.0778, 0.0952,\n",
      "        0.0775, 0.0827, 0.1179, 0.0974, 0.1002, 0.1078, 0.0982, 0.0976,\n",
      "        0.0924, 0.091 , 0.0907, 0.0961, 0.0821, 0.1085, 0.0797, 0.0761,\n",
      "        0.1007, 0.0962, 0.099 , 0.0841, 0.1015, 0.0911, 0.0856, 0.0805,\n",
      "        0.0868, 0.0805, 0.1294, 0.097 , 0.1035, 0.1168, 0.0854, 0.1039,\n",
      "        0.0909, 0.1014, 0.0556, 0.0989, 0.0944, 0.1028, 0.1157, 0.0813,\n",
      "        0.1052, 0.0938, 0.1133, 0.0838, 0.1014, 0.1318, 0.1015, 0.081 ,\n",
      "        0.0941, 0.0731, 0.0635, 0.0812, 0.1021, 0.077 , 0.0885, 0.1019,\n",
      "        0.0915, 0.136 , 0.0919, 0.0953, 0.0771, 0.0758, 0.073 , 0.1057,\n",
      "        0.1097, 0.1186, 0.0781, 0.0911, 0.1193, 0.0982, 0.08  , 0.0889,\n",
      "        0.0732, 0.0913, 0.0887, 0.1064, 0.0985, 0.1106, 0.0984, 0.1081,\n",
      "        0.0978, 0.0735, 0.0878, 0.0756, 0.0855, 0.0723, 0.0758, 0.1136,\n",
      "        0.1278, 0.0975, 0.1028, 0.0827, 0.0756, 0.0654, 0.0872, 0.1002,\n",
      "        0.0978, 0.0775, 0.093 , 0.0701, 0.0831, 0.0921, 0.083 , 0.0864,\n",
      "        0.0874, 0.0814, 0.0987, 0.072 , 0.0803, 0.0664, 0.098 , 0.1014,\n",
      "        0.0766, 0.1086, 0.103 , 0.0878, 0.0957, 0.1008, 0.0961, 0.0513,\n",
      "        0.1232, 0.0827, 0.115 , 0.096 , 0.0788, 0.0781, 0.0933, 0.089 ,\n",
      "        0.0964, 0.1006, 0.0835, 0.0865])\n",
      " array([0.2584, 0.1783, 0.1469, 0.1827, 0.1996, 0.2537, 0.185 , 0.2619,\n",
      "        0.19  , 0.2226, 0.1662, 0.1966, 0.2243, 0.1802, 0.1895, 0.1775,\n",
      "        0.1973, 0.2134, 0.4399, 0.2284, 0.1909, 0.1949, 0.175 , 0.1793,\n",
      "        0.1623, 0.1534, 0.1961, 0.1874, 0.1619, 0.1805, 0.2042, 0.1746,\n",
      "        0.2089, 0.1938, 0.1406, 0.1662, 0.1889, 0.1839, 0.2009, 0.1655,\n",
      "        0.2053, 0.2026, 0.1526, 0.1914, 0.1711, 0.1724, 0.1946, 0.1607,\n",
      "        0.1576, 0.2439, 0.1689, 0.2106, 0.178 , 0.21  , 0.1734, 0.1704,\n",
      "        0.1834, 0.1479, 0.2058, 0.1748, 0.2008, 0.1425, 0.2228, 0.1994,\n",
      "        0.1477, 0.1626, 0.211 , 0.1724, 0.1648, 0.1609, 0.1819, 0.1513,\n",
      "        0.1759, 0.1914, 0.153 , 0.2191, 0.1539, 0.1795, 0.1731, 0.1528,\n",
      "        0.2019, 0.2046, 0.3809, 0.1711, 0.1613, 0.21  , 0.1665, 0.1657,\n",
      "        0.219 , 0.1623, 0.1831, 0.1916, 0.2143, 0.1789, 0.1487, 0.1825,\n",
      "        0.1585, 0.2361, 0.1709, 0.2722, 0.219 , 0.1408, 0.159 , 0.1843,\n",
      "        0.1363, 0.2143, 0.235 , 0.2099, 0.1395, 0.1537, 0.1731, 0.2046,\n",
      "        0.158 , 0.179 , 0.1922, 0.2055, 0.1869, 0.171 , 0.1745, 0.1484,\n",
      "        0.156 , 0.1985, 0.1659, 0.1938, 0.1839, 0.1846, 0.1867, 0.1585,\n",
      "        0.1927, 0.1396, 0.1828, 0.1678, 0.1538, 0.1574, 0.162 , 0.1647,\n",
      "        0.1702, 0.1861, 0.1618, 0.1449, 0.1973, 0.1596, 0.182 , 0.1544,\n",
      "        0.158 , 0.1632, 0.159 , 0.1486, 0.1969, 0.1464, 0.1595, 0.1746,\n",
      "        0.1495, 0.18  , 0.1782, 0.1714, 0.1586, 0.1408, 0.1576, 0.2706,\n",
      "        0.187 , 0.1648, 0.1319, 0.3065, 0.1595, 0.1572, 0.1997, 0.1427,\n",
      "        0.1896, 0.1606, 0.1592, 0.162 , 0.1956, 0.1878, 0.1601, 0.2193,\n",
      "        0.1609, 0.2491, 0.1652, 0.1581, 0.1569, 0.2312, 0.156 , 0.1489,\n",
      "        0.2798, 0.2402, 0.21  , 0.1739, 0.1628, 0.232 , 0.1541, 0.1397,\n",
      "        0.1895, 0.1849, 0.2323, 0.1917, 0.1745, 0.1958, 0.1794, 0.1795,\n",
      "        0.2622, 0.2099, 0.1876, 0.3889, 0.254 , 0.2545, 0.3199, 0.2415,\n",
      "        0.176 , 0.2087, 0.149 , 0.1487, 0.1614, 0.1791, 0.3234, 0.2107,\n",
      "        0.1969, 0.1452, 0.175 , 0.2068, 0.171 , 0.1939, 0.1578, 0.1612,\n",
      "        0.3098, 0.1874, 0.201 , 0.183 , 0.1845, 0.2625, 0.223 , 0.1385,\n",
      "        0.2326, 0.167 , 0.2023, 0.2176, 0.1847, 0.239 , 0.3162, 0.181 ,\n",
      "        0.1522, 0.2058, 0.2112, 0.2191, 0.21  , 0.1626, 0.192 , 0.1792,\n",
      "        0.2356, 0.1969, 0.1589, 0.2268, 0.1484, 0.1662, 0.2055, 0.2067,\n",
      "        0.2149, 0.1919, 0.2722, 0.1575, 0.1888, 0.1854, 0.2109, 0.2285,\n",
      "        0.1623, 0.1528, 0.2201, 0.1628, 0.1566, 0.1901, 0.1735, 0.2121,\n",
      "        0.1762, 0.1671, 0.1716, 0.174 , 0.166 , 0.2208, 0.1816, 0.1683,\n",
      "        0.1808, 0.1972, 0.2039, 0.1567, 0.1793, 0.1979, 0.2134, 0.2389,\n",
      "        0.1609, 0.1825, 0.1309, 0.1948, 0.1964, 0.1597, 0.2393, 0.1697,\n",
      "        0.1879, 0.1877, 0.1908, 0.1965, 0.1488, 0.202 , 0.1975, 0.275 ,\n",
      "        0.1443, 0.2006, 0.1676, 0.1659, 0.1439, 0.2408, 0.1536, 0.1888,\n",
      "        0.2313, 0.1825, 0.2346, 0.1704, 0.1241, 0.1909, 0.2447, 0.2174,\n",
      "        0.148 , 0.2158, 0.2005, 0.142 , 0.1693, 0.2357, 0.178 , 0.2455,\n",
      "        0.1856, 0.142 , 0.2278, 0.227 , 0.3061, 0.1237, 0.2063, 0.1968,\n",
      "        0.1844, 0.2114, 0.1535, 0.149 , 0.1364, 0.194 , 0.1921, 0.1985,\n",
      "        0.2627, 0.1657, 0.2458, 0.1618, 0.201 , 0.1266, 0.1316, 0.1711,\n",
      "        0.3053, 0.2191, 0.2196, 0.2221, 0.1449, 0.1793, 0.2497, 0.1769,\n",
      "        0.2183, 0.1692, 0.2256, 0.2614, 0.2459, 0.2269, 0.333 , 0.2282,\n",
      "        0.1761, 0.2098, 0.1604, 0.2487, 0.2401, 0.0644, 0.1977, 0.1439,\n",
      "        0.2011])\n",
      " array([0.1059, 0.1236, 0.0705, ..., 0.1465, 0.0595, 0.0685])\n",
      " array([0.073 , 0.1149, 0.1556, 0.1568, 0.1185, 0.157 , 0.1801, 0.1377,\n",
      "        0.1181, 0.165 , 0.1661, 0.1848, 0.1439, 0.1484, 0.1623, 0.1652,\n",
      "        0.1283, 0.1942, 0.1339, 0.1722, 0.138 , 0.1648, 0.1394, 0.1506,\n",
      "        0.1978, 0.1857, 0.1599, 0.1502, 0.1687, 0.1325, 0.167 , 0.1036,\n",
      "        0.1294, 0.1844, 0.1113, 0.1934, 0.1835, 0.154 , 0.1117, 0.1448,\n",
      "        0.1912, 0.1487, 0.1553, 0.1318, 0.1725, 0.1576, 0.1502, 0.1118,\n",
      "        0.1402, 0.1365, 0.3319, 0.1578, 0.0996, 0.1368, 0.1126, 0.1082,\n",
      "        0.1096, 0.1379, 0.1262, 0.1214, 0.119 , 0.1591, 0.0704, 0.1365,\n",
      "        0.1357, 0.1236, 0.1219, 0.1108, 0.1084, 0.1183, 0.1145, 0.0912,\n",
      "        0.1562, 0.1322, 0.14  , 0.156 , 0.1258, 0.1089, 0.146 , 0.1669,\n",
      "        0.1803, 0.0982, 0.1242, 0.0969, 0.1371, 0.1102, 0.1671, 0.1666,\n",
      "        0.1542, 0.1613, 0.163 , 0.1549, 0.1401, 0.1111, 0.1542, 0.1185,\n",
      "        0.123 , 0.152 , 0.1312, 0.1164, 0.1353, 0.1369, 0.1504, 0.1515,\n",
      "        0.1181, 0.1069, 0.1536, 0.114 , 0.101 , 0.1061, 0.1418, 0.174 ,\n",
      "        0.1578, 0.1641, 0.1971, 0.1674, 0.1722, 0.1267, 0.1357, 0.1186,\n",
      "        0.1237, 0.0798, 0.1316, 0.1721, 0.1543, 0.1498, 0.1729, 0.1446,\n",
      "        0.1355, 0.1411, 0.125 , 0.1498, 0.1337, 0.119 , 0.162 , 0.1371,\n",
      "        0.1014, 0.1395, 0.1676, 0.1855, 0.1316, 0.1045, 0.1317, 0.1623,\n",
      "        0.1594, 0.0834, 0.1137, 0.1331, 0.2202, 0.1371, 0.1348, 0.1329,\n",
      "        0.1692, 0.1696, 0.1585, 0.161 , 0.1483, 0.1711, 0.2098, 0.1531,\n",
      "        0.1426, 0.1003, 0.1847, 0.1484, 0.1081, 0.162 , 0.1483, 0.1318,\n",
      "        0.166 , 0.1568, 0.1263, 0.1665, 0.1623, 0.1393, 0.1353, 0.1545,\n",
      "        0.1801, 0.1543, 0.14  , 0.1667, 0.1349, 0.1631, 0.1261, 0.1693,\n",
      "        0.1622, 0.1205, 0.162 , 0.1748, 0.1523, 0.1289, 0.1362, 0.1667,\n",
      "        0.1937, 0.1577, 0.1571, 0.1618, 0.1872, 0.1805, 0.1157, 0.1416,\n",
      "        0.1542, 0.2155, 0.1262, 0.1618, 0.1938, 0.1534, 0.1142, 0.1236,\n",
      "        0.1369, 0.1841, 0.162 , 0.1623, 0.172 , 0.1186, 0.1445, 0.1383,\n",
      "        0.1632, 0.1279, 0.1615, 0.126 , 0.1539, 0.122 , 0.1409, 0.1126,\n",
      "        0.1137, 0.1754, 0.1971, 0.1456, 0.1748, 0.1082, 0.1717, 0.16  ,\n",
      "        0.158 , 0.1446, 0.1318, 0.1751, 0.1551, 0.1268, 0.1217, 0.1616,\n",
      "        0.1317, 0.1286, 0.1472, 0.1663, 0.1616, 0.1887, 0.1618, 0.1657,\n",
      "        0.1568, 0.149 , 0.1662, 0.1745, 0.1435, 0.1232, 0.1441, 0.1593,\n",
      "        0.2141, 0.1669, 0.1498, 0.1392, 0.1185, 0.1804, 0.2064, 0.1364,\n",
      "        0.154 , 0.1802, 0.1589, 0.1131, 0.1661, 0.1847, 0.1412, 0.1617,\n",
      "        0.1591, 0.141 , 0.1041, 0.1631, 0.1966, 0.1791, 0.1142, 0.1846,\n",
      "        0.1442, 0.1178, 0.1254, 0.1663, 0.1313, 0.1615, 0.1672, 0.1945,\n",
      "        0.1671, 0.198 , 0.1986, 0.1837, 0.1566, 0.1509, 0.1701, 0.1131,\n",
      "        0.1541, 0.1313, 0.1593, 0.1497, 0.1428, 0.1979, 0.1358, 0.1744,\n",
      "        0.1669, 0.1594, 0.141 , 0.1983, 0.192 , 0.1137, 0.126 , 0.1742,\n",
      "        0.1715, 0.1576, 0.1576, 0.1969, 0.1591, 0.1452, 0.1316, 0.1794,\n",
      "        0.1262, 0.1717, 0.2022, 0.1577, 0.1928, 0.193 , 0.1584, 0.1843,\n",
      "        0.2076, 0.1718, 0.1725, 0.1981, 0.1851, 0.1255, 0.1498, 0.2007,\n",
      "        0.1509, 0.1743, 0.1501, 0.1587, 0.1977, 0.1309, 0.1883, 0.1796,\n",
      "        0.238 , 0.1407, 0.2201, 0.1522, 0.1705, 0.1752, 0.1461, 0.1323,\n",
      "        0.1504, 0.2045, 0.1895, 0.1812, 0.2131, 0.1524, 0.1313, 0.1313,\n",
      "        0.1542, 0.1634, 0.1663, 0.1613, 0.1353, 0.1846, 0.1511, 0.1845,\n",
      "        0.1462, 0.193 , 0.1666, 0.188 , 0.212 , 0.1316, 0.1316, 0.1579,\n",
      "        0.1447, 0.1849, 0.1747, 0.1642, 0.1677, 0.1623, 0.1108, 0.101 ,\n",
      "        0.1923, 0.1589, 0.1405, 0.1416, 0.1537, 0.1882, 0.1543, 0.14  ,\n",
      "        0.1394, 0.1398, 0.136 , 0.1621, 0.1892, 0.1546, 0.1719, 0.1439,\n",
      "        0.1524, 0.149 , 0.118 , 0.1566, 0.1566, 0.1397, 0.1062, 0.1543,\n",
      "        0.1251, 0.1357, 0.1488, 0.1109, 0.184 , 0.19  , 0.1797, 0.1632,\n",
      "        0.1595, 0.156 , 0.1493, 0.1986, 0.1237, 0.1227, 0.2074, 0.1846,\n",
      "        0.1596, 0.1545, 0.1624, 0.1407, 0.184 , 0.1646, 0.1716, 0.155 ,\n",
      "        0.1016, 0.1174, 0.1867, 0.1648, 0.1609, 0.1947, 0.1573, 0.1409,\n",
      "        0.1752, 0.1719, 0.1363, 0.1199, 0.2047, 0.1658, 0.1839, 0.1346,\n",
      "        0.1969, 0.1581, 0.1221, 0.1278, 0.1749, 0.1428, 0.1655, 0.1425,\n",
      "        0.1392, 0.1492, 0.1383, 0.1887, 0.161 , 0.1388, 0.1114, 0.1033,\n",
      "        0.1295, 0.199 , 0.1684, 0.1792, 0.1745, 0.1482, 0.1195, 0.1163,\n",
      "        0.1718, 0.2547, 0.147 , 0.1679, 0.1762, 0.136 , 0.1313, 0.15  ,\n",
      "        0.1281, 0.1499, 0.1366, 0.1766, 0.1721, 0.1435, 0.1049, 0.1746,\n",
      "        0.1428, 0.174 , 0.1541, 0.1973, 0.118 , 0.1193, 0.1414, 0.1448,\n",
      "        0.1406, 0.2071, 0.1539, 0.1898, 0.1462, 0.1359, 0.1439, 0.1408,\n",
      "        0.1732, 0.118 , 0.1262, 0.1729, 0.1404, 0.1719, 0.1546, 0.184 ,\n",
      "        0.1463, 0.1749, 0.1195, 0.1359, 0.1406, 0.1637, 0.1641, 0.1446,\n",
      "        0.1559, 0.2327, 0.1755, 0.1772, 0.1662, 0.1312, 0.1473, 0.1734,\n",
      "        0.112 , 0.1391, 0.1368, 0.1115, 0.1837, 0.1825, 0.1331, 0.1394,\n",
      "        0.1379, 0.1173, 0.1202, 0.1167, 0.1483, 0.1781, 0.1931, 0.1612,\n",
      "        0.1821, 0.1807, 0.1572, 0.1718, 0.1535, 0.1709, 0.1619, 0.1711,\n",
      "        0.1492, 0.1563, 0.1718, 0.2143, 0.1599, 0.1751, 0.1383, 0.1353,\n",
      "        0.1428, 0.0695, 0.1496, 0.2033, 0.1666, 0.1925, 0.1246, 0.1791,\n",
      "        0.1355, 0.1345, 0.126 , 0.1706, 0.1492, 0.149 , 0.1973, 0.1673,\n",
      "        0.1497, 0.1902, 0.1596, 0.1317, 0.1921, 0.2529, 0.1484, 0.1943,\n",
      "        0.1256, 0.1622, 0.1736, 0.1556, 0.1647, 0.173 , 0.1384, 0.1788,\n",
      "        0.2015, 0.1816, 0.215 , 0.131 , 0.1698, 0.1769, 0.15  , 0.1572,\n",
      "        0.184 , 0.1617, 0.1715, 0.1743, 0.1763, 0.2152, 0.1897, 0.1739,\n",
      "        0.2051, 0.1589, 0.1666, 0.1594, 0.1876, 0.1717, 0.1412, 0.1357,\n",
      "        0.1713, 0.1196, 0.1307, 0.1264, 0.1311, 0.1839, 0.1767, 0.1719,\n",
      "        0.1702, 0.1569, 0.1655, 0.1847, 0.1596, 0.1579, 0.1587, 0.1891,\n",
      "        0.1792, 0.1666, 0.1668, 0.1841, 0.1541, 0.1859, 0.149 , 0.1711,\n",
      "        0.1972, 0.1371, 0.1301, 0.1646, 0.1879, 0.1741, 0.1388, 0.1702,\n",
      "        0.1959, 0.1947, 0.1472, 0.1741, 0.1523, 0.1744, 0.1669, 0.2053,\n",
      "        0.1663, 0.1667, 0.2491, 0.1845, 0.1846, 0.1236, 0.1214, 0.1984,\n",
      "        0.1696])\n",
      " array([0.2473, 0.2035, 0.245 , 0.1773, 0.1653, 0.1676, 0.2046, 0.1724,\n",
      "        0.1302, 0.1969, 0.1781, 0.17  , 0.1213, 0.1835, 0.1406, 0.1753,\n",
      "        0.1598, 0.1653, 0.218 , 0.2965, 0.1955, 0.1607, 0.1698, 0.1123,\n",
      "        0.1612, 0.227 , 0.1422, 0.1428, 0.153 , 0.1885, 0.1828, 0.1374,\n",
      "        0.1596, 0.1418, 0.1297, 0.1657, 0.1779, 0.2359, 0.156 , 0.1374,\n",
      "        0.1207, 0.1247, 0.1282, 0.1337, 0.1557, 0.1129, 0.1299, 0.1561,\n",
      "        0.1413, 0.137 , 0.1896, 0.1028, 0.1184, 0.1712, 0.1392, 0.1543,\n",
      "        0.191 , 0.1177, 0.1446, 0.152 , 0.1683, 0.1354, 0.1037, 0.1532,\n",
      "        0.1441, 0.1563, 0.1848, 0.1741, 0.166 , 0.1597, 0.1078, 0.2239,\n",
      "        0.1527, 0.0955, 0.1861, 0.1567, 0.2019, 0.1275, 0.1093, 0.1562,\n",
      "        0.1542, 0.184 , 0.1219, 0.1959, 0.1481, 0.1766, 0.1578, 0.143 ,\n",
      "        0.2198, 0.2105, 0.1844, 0.2054, 0.1889, 0.1139, 0.103 , 0.1022,\n",
      "        0.1669, 0.2283, 0.1316, 0.1447, 0.1505, 0.1887, 0.1279, 0.0958,\n",
      "        0.0881, 0.1856, 0.1668, 0.1125, 0.119 , 0.1769, 0.1281, 0.1989,\n",
      "        0.1504, 0.1146, 0.1313, 0.1308, 0.1409, 0.2798, 0.1597, 0.1675,\n",
      "        0.1721, 0.1053, 0.1578, 0.1531, 0.1361, 0.1369, 0.2141, 0.1571,\n",
      "        0.1266, 0.1064, 0.1453, 0.1554, 0.1703, 0.1495, 0.2422, 0.1289,\n",
      "        0.15  , 0.1631, 0.2695, 0.1465, 0.1444, 0.1489, 0.1614, 0.1547,\n",
      "        0.1344, 0.1476, 0.1093, 0.1828, 0.2011, 0.169 , 0.1263, 0.1621,\n",
      "        0.1498, 0.1807, 0.1022, 0.1645, 0.1677, 0.1723, 0.1571, 0.2324,\n",
      "        0.1985, 0.1461, 0.3476, 0.123 , 0.1228, 0.1374, 0.1673, 0.1523,\n",
      "        0.2077, 0.2623, 0.1294, 0.1748, 0.1302, 0.1602, 0.1041, 0.1228,\n",
      "        0.0997, 0.1149, 0.1552, 0.1439, 0.1622, 0.1671, 0.0985, 0.1488,\n",
      "        0.1181, 0.1709, 0.1895, 0.1235, 0.1276, 0.114 , 0.1046, 0.1358,\n",
      "        0.1491, 0.1544, 0.2044, 0.1783, 0.1491, 0.1713, 0.1178, 0.1058,\n",
      "        0.1278, 0.179 , 0.2325, 0.1678, 0.1421, 0.1459, 0.1825, 0.2041,\n",
      "        0.1445, 0.1312, 0.1715, 0.1542, 0.1282, 0.1114, 0.117 , 0.1497,\n",
      "        0.1261, 0.1394, 0.1977, 0.1539, 0.147 , 0.2272, 0.1574, 0.1632,\n",
      "        0.1181, 0.1718, 0.1315, 0.2457, 0.2018, 0.2019, 0.1512, 0.1714,\n",
      "        0.1449, 0.0911, 0.1712, 0.1796, 0.1451, 0.1316, 0.199 , 0.1353,\n",
      "        0.1544, 0.1468, 0.1266, 0.0825, 0.1412, 0.2598, 0.1536, 0.2729,\n",
      "        0.1412, 0.211 , 0.2067, 0.224 , 0.1702, 0.1785, 0.1267, 0.1445,\n",
      "        0.1485, 0.1362, 0.1215, 0.2406, 0.1487, 0.1042, 0.1821, 0.209 ,\n",
      "        0.1796, 0.1932, 0.3555, 0.1236, 0.1331, 0.1268, 0.1634, 0.1447,\n",
      "        0.2021, 0.2093, 0.2042, 0.1188, 0.1385, 0.1919, 0.1271, 0.1719,\n",
      "        0.2448, 0.1518, 0.1598, 0.2033, 0.1715, 0.1123, 0.1689, 0.1208,\n",
      "        0.2269, 0.1177, 0.1036, 0.2095, 0.1083, 0.096 , 0.1843, 0.1141,\n",
      "        0.1226, 0.1238, 0.1348])\n",
      " array([0.0601, 0.0819, 0.0837, ..., 0.0924, 0.0809, 0.1448])\n",
      " array([0.1575, 0.1787, 0.1819, 0.1528, 0.1845, 0.1577, 0.1842, 0.1769,\n",
      "        0.1717, 0.1465, 0.1775, 0.1942, 0.1443, 0.1398, 0.1552, 0.1426,\n",
      "        0.143 , 0.1935, 0.157 , 0.1582, 0.1471, 0.1434, 0.1901, 0.1938,\n",
      "        0.1198, 0.181 , 0.1769, 0.1729, 0.1952, 0.1647, 0.1775, 0.1633,\n",
      "        0.141 , 0.2259, 0.186 , 0.1897, 0.2151, 0.1678, 0.1773, 0.1854,\n",
      "        0.2024, 0.1745, 0.1615, 0.1662, 0.1788, 0.1571, 0.1364, 0.1678,\n",
      "        0.1713, 0.2067, 0.1862, 0.231 , 0.1994, 0.1858, 0.2066, 0.1683,\n",
      "        0.1701, 0.1668, 0.1857, 0.1759, 0.1591, 0.1419, 0.1402, 0.1675,\n",
      "        0.1501, 0.1855, 0.1766, 0.1645, 0.132 , 0.1498, 0.1729, 0.2394,\n",
      "        0.155 , 0.1631, 0.2337, 0.1864, 0.1784, 0.1724, 0.1995, 0.1774,\n",
      "        0.1802, 0.1459, 0.1718, 0.1666, 0.1602, 0.1733, 0.1569, 0.2143,\n",
      "        0.1788, 0.2102, 0.1125, 0.1482, 0.1171, 0.1533, 0.1789, 0.1667,\n",
      "        0.1675, 0.1764, 0.1172, 0.1631, 0.1371, 0.1671, 0.1491, 0.157 ,\n",
      "        0.1974, 0.1584, 0.1384, 0.1972, 0.2066, 0.1608, 0.1564, 0.1663,\n",
      "        0.1825, 0.16  , 0.197 , 0.1741, 0.2165, 0.1692, 0.1551, 0.1535,\n",
      "        0.1526, 0.1513, 0.1942, 0.2158, 0.1718, 0.18  , 0.1142, 0.138 ,\n",
      "        0.1476, 0.1849, 0.1542, 0.1272, 0.1451, 0.1058, 0.2107, 0.1542,\n",
      "        0.137 , 0.1451, 0.1646, 0.155 , 0.1616, 0.1491, 0.1662, 0.177 ,\n",
      "        0.1623, 0.1759, 0.1243, 0.1571, 0.1229, 0.1442, 0.1787, 0.1698,\n",
      "        0.1616, 0.1723, 0.1667, 0.1592, 0.1543, 0.1391, 0.1535, 0.163 ,\n",
      "        0.1932, 0.1847, 0.1619, 0.1672, 0.1712, 0.1722, 0.1622, 0.1702,\n",
      "        0.2155, 0.1806, 0.1545, 0.2075, 0.1763, 0.1264, 0.1411, 0.1655,\n",
      "        0.1875, 0.2509, 0.1481, 0.16  , 0.1591, 0.1664, 0.1313, 0.185 ,\n",
      "        0.1897, 0.1675, 0.1664, 0.1805, 0.1766, 0.1329, 0.1047, 0.123 ,\n",
      "        0.1483, 0.1626, 0.1611, 0.1471, 0.1653, 0.132 , 0.1545, 0.1671,\n",
      "        0.1351, 0.1786, 0.16  , 0.16  , 0.1579, 0.181 , 0.1535, 0.1768,\n",
      "        0.1626, 0.149 , 0.1666, 0.1692, 0.1589, 0.2155, 0.1488, 0.1591,\n",
      "        0.2244, 0.154 , 0.1528, 0.2025, 0.1936, 0.1631, 0.1568, 0.1414,\n",
      "        0.2007, 0.1571, 0.1351, 0.1421, 0.1498, 0.1849, 0.1814, 0.1582,\n",
      "        0.1361, 0.1726, 0.1444, 0.1334, 0.174 , 0.1946, 0.1455, 0.1675,\n",
      "        0.1935, 0.1636, 0.1327, 0.1353, 0.1501, 0.2072, 0.1637, 0.1857,\n",
      "        0.1511, 0.1468, 0.1823, 0.176 , 0.1729, 0.1421, 0.1277, 0.2298,\n",
      "        0.1452, 0.1437, 0.1907, 0.1841, 0.145 , 0.1286, 0.1784, 0.1597,\n",
      "        0.1666, 0.2328, 0.1238, 0.2226, 0.1786, 0.164 , 0.1321, 0.1535,\n",
      "        0.1412, 0.1662, 0.1803, 0.1533, 0.2076, 0.1703, 0.1624, 0.1533,\n",
      "        0.1847, 0.1468, 0.2271, 0.2122, 0.1372, 0.1845, 0.2108, 0.1623,\n",
      "        0.1843, 0.1738, 0.1409, 0.1591, 0.1181, 0.1572, 0.1704, 0.1444,\n",
      "        0.1141, 0.154 , 0.1549, 0.1716, 0.1589, 0.1447, 0.1561, 0.1499,\n",
      "        0.2101, 0.1621, 0.2193, 0.2033, 0.18  , 0.1713, 0.1543, 0.2156,\n",
      "        0.1488, 0.1493, 0.1657, 0.2149, 0.2281, 0.2019, 0.187 , 0.1416,\n",
      "        0.1326, 0.1632])\n",
      " array([0.1297, 0.0997, 0.1728, ..., 0.0784, 0.0886, 0.0742])\n",
      " array([0.1   , 0.0856, 0.0988, ..., 0.0758, 0.0627, 0.0757])\n",
      " array([0.0514, 0.0358, 0.0455, ..., 0.0475, 0.0638, 0.0529])\n",
      " array([0.0657, 0.17  , 0.1631, ..., 0.113 , 0.0523, 0.0654])\n",
      " array([0.1285, 0.1785, 0.1813, ..., 0.1618, 0.1065, 0.0801])\n",
      " array([0.1074, 0.2225, 0.1763, ..., 0.1165, 0.1371, 0.0742])\n",
      " array([0.267 , 0.1814, 0.1664, 0.1925, 0.1543, 0.1411, 0.1565, 0.1461,\n",
      "        0.1239, 0.154 , 0.2385, 0.1314, 0.1436, 0.1768, 0.1938, 0.141 ,\n",
      "        0.1368, 0.1188, 0.1449, 0.1733, 0.2657, 0.1782, 0.1691, 0.1602,\n",
      "        0.1564, 0.1507, 0.1795, 0.157 , 0.1586, 0.1764, 0.1571, 0.1616,\n",
      "        0.2009, 0.1521, 0.1382, 0.1854, 0.1564, 0.1623, 0.1666, 0.1577,\n",
      "        0.1466, 0.1767, 0.2423, 0.1892, 0.1447, 0.1544, 0.2168, 0.24  ,\n",
      "        0.1664, 0.2069, 0.1939, 0.1581, 0.1125, 0.1442, 0.1712, 0.1617,\n",
      "        0.1187, 0.1379, 0.1572, 0.1615, 0.1359, 0.215 , 0.1136, 0.1493,\n",
      "        0.1946, 0.1318, 0.1797, 0.1667, 0.1725, 0.0934, 0.1497, 0.1275,\n",
      "        0.1777, 0.1402, 0.1304, 0.1659, 0.167 , 0.1587, 0.1968, 0.1744,\n",
      "        0.1564, 0.1315, 0.1187, 0.1707, 0.1671, 0.1793, 0.1489, 0.184 ,\n",
      "        0.1386, 0.1461, 0.1664, 0.2034, 0.1961, 0.157 , 0.2135, 0.1402,\n",
      "        0.1734, 0.1648, 0.2139, 0.1473, 0.1653, 0.165 , 0.1651, 0.2438,\n",
      "        0.2084, 0.1655, 0.1575, 0.1948, 0.148 , 0.1515, 0.1596, 0.1433,\n",
      "        0.1568, 0.2127, 0.1521, 0.1601, 0.2279, 0.1182, 0.1375, 0.1637,\n",
      "        0.1796, 0.1387, 0.1411, 0.1522, 0.1846, 0.192 , 0.1266, 0.1773,\n",
      "        0.1547, 0.2155, 0.1486, 0.124 , 0.1392, 0.1622, 0.1762, 0.2142,\n",
      "        0.1588, 0.1585, 0.1796, 0.1652, 0.1713, 0.1572, 0.175 , 0.1442,\n",
      "        0.1542, 0.1886, 0.1534, 0.1809, 0.1391, 0.1408, 0.1312, 0.1764,\n",
      "        0.205 , 0.1771, 0.1386, 0.1393, 0.1563, 0.1968, 0.2066, 0.1702,\n",
      "        0.1311, 0.1591, 0.1748, 0.1356, 0.1487, 0.2105, 0.1671, 0.1588,\n",
      "        0.1537, 0.2081, 0.1484, 0.1473, 0.1873, 0.1609, 0.1457, 0.1293,\n",
      "        0.1902, 0.1594, 0.3332, 0.1837, 0.1636, 0.1976, 0.1712, 0.1535,\n",
      "        0.1493, 0.1996, 0.1451, 0.1485, 0.1847, 0.1483, 0.1395, 0.1853,\n",
      "        0.149 , 0.1601, 0.1677, 0.1896, 0.1345, 0.113 , 0.1571, 0.1851,\n",
      "        0.1392, 0.1395, 0.1367, 0.1314, 0.1719, 0.1394, 0.1485, 0.1846,\n",
      "        0.1585, 0.139 , 0.1954, 0.1517, 0.1648, 0.1655, 0.1562, 0.1823,\n",
      "        0.1981, 0.1472, 0.1972, 0.2095, 0.1359, 0.1567, 0.1469, 0.1448,\n",
      "        0.1879, 0.1425, 0.1524, 0.1885, 0.1666, 0.1841, 0.21  , 0.162 ,\n",
      "        0.2196, 0.1567, 0.1184, 0.1753, 0.1591, 0.1484, 0.1619, 0.1419,\n",
      "        0.1694, 0.1668, 0.1892, 0.1744, 0.1521, 0.1245, 0.1637, 0.1796,\n",
      "        0.179 , 0.1592, 0.1698, 0.1419, 0.1777, 0.112 , 0.2107, 0.1751,\n",
      "        0.2067, 0.1495, 0.16  , 0.235 , 0.1485, 0.1669, 0.1137, 0.1418,\n",
      "        0.1606, 0.0917, 0.1819, 0.1878, 0.1187, 0.1536, 0.1713, 0.163 ,\n",
      "        0.1187, 0.1267, 0.1253, 0.1974, 0.1825, 0.1299, 0.1746, 0.1403,\n",
      "        0.1509, 0.1577, 0.1294, 0.1967, 0.1398, 0.1994, 0.1341, 0.1671,\n",
      "        0.1949, 0.1466, 0.1802, 0.1193, 0.1819, 0.1545, 0.2004, 0.1402,\n",
      "        0.1693, 0.2027, 0.1254, 0.1424, 0.1425, 0.2627, 0.1877, 0.2016,\n",
      "        0.1944, 0.172 , 0.159 , 0.1579, 0.2075, 0.1347, 0.1489, 0.1462,\n",
      "        0.1844, 0.197 , 0.149 , 0.1673, 0.1167, 0.1316, 0.1698, 0.1791,\n",
      "        0.1526, 0.1896, 0.1845, 0.1664, 0.1767, 0.175 , 0.1343, 0.2051,\n",
      "        0.2224, 0.2373, 0.078 , 0.1792, 0.2098, 0.1901, 0.1907, 0.1844,\n",
      "        0.2133, 0.0893, 0.2216, 0.1778, 0.1248, 0.1255, 0.1562, 0.1909,\n",
      "        0.1828, 0.1933, 0.1857, 0.1477, 0.1661, 0.2408, 0.157 , 0.1594,\n",
      "        0.1694, 0.16  , 0.1523, 0.1736, 0.175 , 0.1555, 0.169 , 0.1859,\n",
      "        0.13  , 0.1788, 0.1354, 0.1948, 0.1778, 0.1381, 0.216 , 0.1368,\n",
      "        0.1602, 0.2068, 0.197 , 0.225 , 0.2371, 0.1928, 0.1443, 0.1582,\n",
      "        0.2238, 0.1444, 0.1389, 0.1892, 0.2018, 0.2023, 0.2062, 0.2033,\n",
      "        0.1825, 0.1691, 0.1406, 0.1587, 0.1716, 0.1428, 0.2252, 0.1759,\n",
      "        0.1637, 0.1692, 0.2051, 0.1475, 0.1294, 0.1759, 0.1664, 0.1797,\n",
      "        0.1645, 0.1677, 0.1346, 0.1525, 0.1416, 0.167 , 0.1928, 0.2906,\n",
      "        0.194 , 0.1355, 0.1395, 0.1803, 0.1765, 0.155 , 0.1898, 0.1764,\n",
      "        0.1944, 0.2157, 0.1669, 0.1451, 0.1418, 0.137 , 0.1624, 0.2755,\n",
      "        0.2073, 0.1669, 0.1723, 0.2148, 0.1495, 0.1775, 0.1951, 0.1806,\n",
      "        0.1462, 0.1639, 0.1621, 0.2025, 0.1194, 0.1497, 0.0907, 0.1615,\n",
      "        0.2417, 0.1395, 0.1708, 0.1319, 0.1321, 0.1674, 0.1571, 0.1973,\n",
      "        0.1863, 0.1725, 0.1589, 0.1972, 0.1317, 0.1623, 0.1755, 0.2714,\n",
      "        0.1357, 0.1713, 0.1594, 0.185 , 0.1115, 0.1284, 0.1144, 0.167 ,\n",
      "        0.1749, 0.1847, 0.1852, 0.138 , 0.1719, 0.2149, 0.1623, 0.1433,\n",
      "        0.1591, 0.1789, 0.1877, 0.1648, 0.1846, 0.2457, 0.1896, 0.1296,\n",
      "        0.1318, 0.1398, 0.1664, 0.2   , 0.3137, 0.1976, 0.2129, 0.1168,\n",
      "        0.206 , 0.2168, 0.1569, 0.1872, 0.1847, 0.1471, 0.1506, 0.0973,\n",
      "        0.1572, 0.1856, 0.1622, 0.1209, 0.2001, 0.2072, 0.139 , 0.1762,\n",
      "        0.2374, 0.1409, 0.0962, 0.107 , 0.1367, 0.2454, 0.1748, 0.2506,\n",
      "        0.1907, 0.1722, 0.1731, 0.1921, 0.1734, 0.1775, 0.1651, 0.1902,\n",
      "        0.1574, 0.1547, 0.167 , 0.1558, 0.1441, 0.1299, 0.1528, 0.1868,\n",
      "        0.1893, 0.1865, 0.1645, 0.2055, 0.1428, 0.165 , 0.1385, 0.1209,\n",
      "        0.1584, 0.1872, 0.1924, 0.1584, 0.1215, 0.1245, 0.2057, 0.1631,\n",
      "        0.1747, 0.1673, 0.1703, 0.2101, 0.1187, 0.1721, 0.2156, 0.1751,\n",
      "        0.1492, 0.1153, 0.1445, 0.1922, 0.1667, 0.149 , 0.1317, 0.1537,\n",
      "        0.2017, 0.1625, 0.1413, 0.1487, 0.2327, 0.1813, 0.17  , 0.2022,\n",
      "        0.1624, 0.167 , 0.1754, 0.1664, 0.1491, 0.1276, 0.1236, 0.1083,\n",
      "        0.1118, 0.1798, 0.1543, 0.1482, 0.1761, 0.1888, 0.1969, 0.1387,\n",
      "        0.1664, 0.1591, 0.1538, 0.1497, 0.1805, 0.1357, 0.1622, 0.1488,\n",
      "        0.2013, 0.1946, 0.1789, 0.1589, 0.1699, 0.1016, 0.1388, 0.1164,\n",
      "        0.1775, 0.2525, 0.1294, 0.1476, 0.1552, 0.1876, 0.1045, 0.1555,\n",
      "        0.1525, 0.1724, 0.1256, 0.1526, 0.139 , 0.186 , 0.1431, 0.2129,\n",
      "        0.175 , 0.1697, 0.1145, 0.1703, 0.1911, 0.155 , 0.1796, 0.2566,\n",
      "        0.1696, 0.1647, 0.1351, 0.1686, 0.2053, 0.1984, 0.196 , 0.1831,\n",
      "        0.1843, 0.1619, 0.1101, 0.1668, 0.1972, 0.1312, 0.1486, 0.1394,\n",
      "        0.1448, 0.1426, 0.1617, 0.1679, 0.1393, 0.1475, 0.2122, 0.1628,\n",
      "        0.1616, 0.1543, 0.1816, 0.1404, 0.1539, 0.1843, 0.1585, 0.2022,\n",
      "        0.194 , 0.1972, 0.1412, 0.1287, 0.1041, 0.2018, 0.1668, 0.2015,\n",
      "        0.1797, 0.2355, 0.2053, 0.2151, 0.2281, 0.1238, 0.1742, 0.1697,\n",
      "        0.2454, 0.1292, 0.1723, 0.1478, 0.1764, 0.1792, 0.1933, 0.2062,\n",
      "        0.1486, 0.1519, 0.1622, 0.2316, 0.1924, 0.1943, 0.1711, 0.2291,\n",
      "        0.1411, 0.2326, 0.1578, 0.2101, 0.1823, 0.1424, 0.1737, 0.1792,\n",
      "        0.1583, 0.181 , 0.1322, 0.1646, 0.157 , 0.1519, 0.1777, 0.1992,\n",
      "        0.1374, 0.1805, 0.1828, 0.147 , 0.1157, 0.155 , 0.1904, 0.2228,\n",
      "        0.1602, 0.1389, 0.1337, 0.1532, 0.1489, 0.1498, 0.2455, 0.2134,\n",
      "        0.1972, 0.1526, 0.1601, 0.1796, 0.1881, 0.1342, 0.1995, 0.2073,\n",
      "        0.1294, 0.1692, 0.1636, 0.2017, 0.1238, 0.1967, 0.1896, 0.1322,\n",
      "        0.1516, 0.1639, 0.1467, 0.1552, 0.1558, 0.1375, 0.1298, 0.1679,\n",
      "        0.334 , 0.1457, 0.1867, 0.1853, 0.1515, 0.1898, 0.1825])\n",
      " array([0.0843, 0.0695, 0.0467, 0.0689, 0.1526, 0.0695, 0.0916, 0.1083,\n",
      "        0.1296, 0.0914, 0.1458, 0.0999, 0.1352, 0.0763, 0.0991, 0.1526,\n",
      "        0.1083, 0.1328, 0.1012, 0.1538, 0.0908, 0.1886, 0.0794, 0.0699,\n",
      "        0.0622, 0.1421, 0.1055, 0.1786, 0.1455, 0.0731, 0.0761, 0.1024,\n",
      "        0.1327, 0.0748, 0.0781, 0.0895, 0.109 , 0.0819, 0.1121, 0.1433,\n",
      "        0.1638, 0.0754, 0.0963, 0.0782, 0.0959, 0.083 , 0.0777, 0.0904,\n",
      "        0.0961, 0.123 , 0.1512, 0.0749, 0.0683, 0.0741, 0.0793, 0.119 ,\n",
      "        0.0852, 0.1524, 0.251 , 0.0587, 0.1247, 0.0819, 0.0993, 0.1142,\n",
      "        0.0814, 0.0914, 0.1303, 0.082 , 0.1091, 0.0801, 0.087 , 0.1082,\n",
      "        0.121 , 0.0747, 0.0859, 0.1496, 0.0833, 0.1145, 0.1149, 0.1313,\n",
      "        0.0881, 0.0686, 0.1007, 0.1671, 0.0837, 0.0957, 0.1438, 0.0924,\n",
      "        0.1083, 0.088 , 0.0472, 0.1151, 0.0521, 0.0926, 0.1223, 0.0596,\n",
      "        0.0956, 0.1123, 0.085 , 0.1076, 0.08  , 0.0756, 0.0702, 0.0657,\n",
      "        0.1003, 0.129 , 0.1657, 0.0636, 0.0859, 0.0946, 0.1359, 0.0913,\n",
      "        0.109 , 0.1391, 0.0916, 0.0865, 0.1023, 0.103 , 0.0894, 0.1473,\n",
      "        0.0981, 0.0595, 0.1246, 0.0925, 0.1037, 0.066 , 0.141 , 0.1268,\n",
      "        0.1761, 0.1019, 0.0481, 0.0598, 0.0537, 0.0648, 0.11  , 0.1259,\n",
      "        0.1106, 0.1618, 0.1286, 0.0582, 0.0825, 0.1126, 0.0963, 0.0751,\n",
      "        0.118 , 0.1141, 0.1168, 0.1007, 0.0717, 0.0705, 0.162 , 0.0659,\n",
      "        0.078 , 0.1258, 0.0805, 0.1326, 0.0804, 0.0651, 0.1136, 0.0674,\n",
      "        0.0978, 0.0826, 0.1389, 0.0959, 0.0789, 0.0931, 0.1458, 0.1408,\n",
      "        0.075 , 0.0638, 0.0701, 0.1008, 0.0735, 0.1435, 0.1054, 0.1231,\n",
      "        0.0977, 0.1486, 0.0959, 0.0861, 0.0863, 0.1416, 0.1522, 0.0774,\n",
      "        0.0951, 0.1083, 0.1613, 0.0892, 0.1103, 0.1311, 0.1227, 0.1311,\n",
      "        0.0831, 0.1238, 0.1209, 0.1054, 0.0954, 0.1408, 0.0556, 0.0795,\n",
      "        0.0935, 0.1299, 0.086 , 0.1359, 0.0787, 0.1431, 0.0613, 0.0648,\n",
      "        0.1165, 0.0691, 0.127 , 0.1214, 0.0733, 0.0873, 0.1049, 0.062 ,\n",
      "        0.0893, 0.0837, 0.1055, 0.0959, 0.0623, 0.0704, 0.0821, 0.1104,\n",
      "        0.1063, 0.0609, 0.1026, 0.0982, 0.1228, 0.0756, 0.136 , 0.1064,\n",
      "        0.0659, 0.0832, 0.1185, 0.0831, 0.0868, 0.0908, 0.097 , 0.0903,\n",
      "        0.0825, 0.1007, 0.1059, 0.0708, 0.0669, 0.0702, 0.0553, 0.0754,\n",
      "        0.065 , 0.0822, 0.1056, 0.113 , 0.0914, 0.0776, 0.071 , 0.1191,\n",
      "        0.0964, 0.1181, 0.1148, 0.0627, 0.0812, 0.1461, 0.157 , 0.0589,\n",
      "        0.1171, 0.1314, 0.0835, 0.081 , 0.0748, 0.1059, 0.1353, 0.1366,\n",
      "        0.173 , 0.1038, 0.1245, 0.0578, 0.0543, 0.1247, 0.0759, 0.1427,\n",
      "        0.091 , 0.0935, 0.1611, 0.0796, 0.1356, 0.09  , 0.1488, 0.0865,\n",
      "        0.1376, 0.0893, 0.1243, 0.0552, 0.0758, 0.0842, 0.1113, 0.1191,\n",
      "        0.0737, 0.0987, 0.1312, 0.0756, 0.0814, 0.0878, 0.1312, 0.1055,\n",
      "        0.1353, 0.0724, 0.0553, 0.1278, 0.0656, 0.0702, 0.0763, 0.0825,\n",
      "        0.1304, 0.117 , 0.1111, 0.0555, 0.0808, 0.1086, 0.0782, 0.0961,\n",
      "        0.1308, 0.0645, 0.0988, 0.1376, 0.0764, 0.0877, 0.1025, 0.1345,\n",
      "        0.0805, 0.0457, 0.0995, 0.0722, 0.0689, 0.0806, 0.0866, 0.0814,\n",
      "        0.0958, 0.1354, 0.1039, 0.0627, 0.0458, 0.067 , 0.0523, 0.0905,\n",
      "        0.1186, 0.0645, 0.0688, 0.0645, 0.1344, 0.0694, 0.0873, 0.0701,\n",
      "        0.0669, 0.1125, 0.1082, 0.0723, 0.0919, 0.1405, 0.0777, 0.1133,\n",
      "        0.1078, 0.0714, 0.0668, 0.0662, 0.0659, 0.1303, 0.0725, 0.0695,\n",
      "        0.0823, 0.0552, 0.0877, 0.1348, 0.0749, 0.0861, 0.1311, 0.1262,\n",
      "        0.0699, 0.1357, 0.0743, 0.0697, 0.0702, 0.1002, 0.1165, 0.0736,\n",
      "        0.1131, 0.1182, 0.131 , 0.1278, 0.0962, 0.1107, 0.1234, 0.1061,\n",
      "        0.0732, 0.1435, 0.0922, 0.1007, 0.1182, 0.1155, 0.0606, 0.0905,\n",
      "        0.057 , 0.1309, 0.1013, 0.104 , 0.1204, 0.074 , 0.0698, 0.0676,\n",
      "        0.1055, 0.0874, 0.0784, 0.1107, 0.1009, 0.0834, 0.0615, 0.0929,\n",
      "        0.1008, 0.1011, 0.1267, 0.0782, 0.0795, 0.0704, 0.0824, 0.056 ,\n",
      "        0.0834, 0.0881, 0.083 , 0.121 , 0.1005, 0.1266, 0.0704, 0.1323,\n",
      "        0.0787, 0.1343, 0.1119, 0.1088, 0.1068, 0.0641, 0.0642, 0.0737,\n",
      "        0.1706, 0.0613, 0.0733, 0.0938, 0.1356, 0.0632, 0.0788, 0.0831,\n",
      "        0.0825, 0.1362, 0.1403, 0.0692, 0.0652, 0.0817, 0.0801, 0.1409,\n",
      "        0.0725, 0.0828, 0.0609, 0.0692, 0.0578, 0.0648, 0.1104, 0.1081,\n",
      "        0.0972, 0.0952, 0.1133, 0.1001, 0.0803, 0.0782, 0.0963, 0.1138,\n",
      "        0.1309, 0.0723, 0.1517, 0.2042, 0.06  , 0.073 , 0.0711, 0.0989,\n",
      "        0.0764, 0.1154, 0.0741, 0.1063, 0.094 , 0.0515, 0.0827, 0.0634,\n",
      "        0.1177, 0.0799, 0.1305, 0.1039, 0.1108, 0.066 , 0.1008, 0.1364,\n",
      "        0.0878, 0.0753, 0.1323, 0.0913, 0.0691, 0.0876, 0.1142, 0.0607,\n",
      "        0.0952, 0.1053, 0.1284, 0.0608, 0.0837, 0.0955, 0.0906, 0.0877,\n",
      "        0.1265, 0.1135, 0.0853, 0.0914, 0.1146, 0.065 , 0.1129, 0.1096,\n",
      "        0.0605, 0.0834, 0.1311, 0.1019, 0.1049, 0.1356, 0.0748, 0.1253,\n",
      "        0.0833, 0.0525, 0.0655, 0.1005, 0.1341, 0.0826, 0.1276, 0.0698,\n",
      "        0.0877, 0.0701, 0.1309, 0.0776, 0.1338, 0.0877, 0.1421, 0.0777,\n",
      "        0.1483, 0.0802, 0.1187, 0.0583, 0.1089, 0.0909, 0.0879, 0.1065,\n",
      "        0.0911, 0.124 , 0.0729, 0.0828, 0.1066, 0.1114, 0.086 , 0.1086,\n",
      "        0.1439, 0.0748, 0.1207, 0.0797, 0.0699, 0.1267, 0.0707, 0.1309,\n",
      "        0.0787, 0.1051, 0.1059, 0.0836, 0.1076, 0.0921, 0.0787, 0.0816,\n",
      "        0.0626, 0.0867, 0.123 , 0.0869, 0.1008, 0.104 , 0.0878, 0.1228,\n",
      "        0.0778, 0.1264, 0.0445, 0.1058, 0.0571, 0.0573, 0.0698, 0.1003,\n",
      "        0.0876, 0.1282, 0.1607, 0.0702, 0.1055, 0.1304, 0.1005, 0.0752,\n",
      "        0.0833, 0.0922, 0.0993, 0.109 , 0.0953, 0.1127, 0.1515, 0.0574,\n",
      "        0.0785, 0.106 , 0.1212, 0.078 , 0.0729, 0.0554, 0.0701, 0.1107,\n",
      "        0.085 , 0.0983, 0.0581, 0.0608, 0.0652, 0.0837, 0.1136, 0.0795,\n",
      "        0.0888, 0.0974, 0.0833, 0.1079, 0.0797, 0.0649, 0.0788, 0.0651,\n",
      "        0.1434, 0.0689, 0.0685, 0.1091, 0.1149, 0.0616, 0.0698, 0.0605,\n",
      "        0.1007, 0.148 , 0.0561, 0.0708, 0.066 , 0.1168, 0.115 , 0.1289,\n",
      "        0.0623, 0.0755, 0.0846, 0.1146, 0.0823, 0.0702, 0.0863, 0.1308,\n",
      "        0.1091, 0.0607, 0.0995, 0.0645, 0.0649, 0.0901, 0.1516, 0.083 ,\n",
      "        0.1355, 0.0523, 0.0512, 0.0737, 0.1205, 0.067 , 0.0684, 0.1479,\n",
      "        0.0701, 0.0595, 0.0578, 0.0567, 0.1223, 0.0717, 0.0817, 0.1216,\n",
      "        0.0513, 0.0895, 0.0986, 0.056 , 0.0639, 0.0565, 0.0721, 0.1301,\n",
      "        0.0692, 0.0695, 0.152 , 0.0976, 0.0915, 0.0983, 0.1608, 0.1421,\n",
      "        0.0717, 0.0958, 0.0886, 0.1049, 0.0624, 0.0781, 0.1534, 0.0726,\n",
      "        0.1132, 0.1141, 0.1488, 0.0651, 0.0704, 0.0679, 0.1003, 0.0484,\n",
      "        0.065 , 0.1404, 0.1219, 0.1014, 0.1743, 0.1089, 0.116 , 0.066 ,\n",
      "        0.069 , 0.0991, 0.1339, 0.2071, 0.1362, 0.0536, 0.1149, 0.1067,\n",
      "        0.0832, 0.1258, 0.1456, 0.1175, 0.0953, 0.1869, 0.111 , 0.1122,\n",
      "        0.1433, 0.1011, 0.1665, 0.1423, 0.0647, 0.1437, 0.1007, 0.1462,\n",
      "        0.0729, 0.1189, 0.0627, 0.0733, 0.0708, 0.1145, 0.1009, 0.0926,\n",
      "        0.1059, 0.0952, 0.1057, 0.1285, 0.0672, 0.1006, 0.1103, 0.1195,\n",
      "        0.1008, 0.1488, 0.0528, 0.0708, 0.0738, 0.1515, 0.088 , 0.0709,\n",
      "        0.0659, 0.0605, 0.0607, 0.0588, 0.0682, 0.1073, 0.1547, 0.0686,\n",
      "        0.091 , 0.1348, 0.069 , 0.137 , 0.1474, 0.0608, 0.0915, 0.1069,\n",
      "        0.1199, 0.0685, 0.0734, 0.061 , 0.1039, 0.0876, 0.0729, 0.0757,\n",
      "        0.1455, 0.0623, 0.0694, 0.1004, 0.1335, 0.1456, 0.0731, 0.1061,\n",
      "        0.0583, 0.0742, 0.0673, 0.0637, 0.1258, 0.0762, 0.0756, 0.089 ,\n",
      "        0.1207, 0.0927, 0.1004, 0.1297, 0.1805, 0.0781, 0.1153, 0.09  ,\n",
      "        0.0931, 0.1373, 0.0862, 0.0613, 0.2047, 0.0703, 0.0796, 0.1005,\n",
      "        0.1055])\n",
      " array([0.1713, 0.1432, 0.0655, ..., 0.074 , 0.1655, 0.0827])\n",
      " array([0.0895, 0.0954, 0.1738, ..., 0.1805, 0.1844, 0.1446])\n",
      " array([0.1119, 0.1345, 0.3396, ..., 0.1775, 0.3274, 0.1813])\n",
      " array([0.1194, 0.1337, 0.1847, ..., 0.1554, 0.1494, 0.1183])\n",
      " array([0.1528, 0.208 , 0.2084, ..., 0.1945, 0.2004, 0.1557])\n",
      " array([0.1269, 0.1766, 0.1843, ..., 0.1686, 0.1313, 0.1855])\n",
      " array([0.1516, 0.2453, 0.1762, ..., 0.0798, 0.2365, 0.131 ])\n",
      " array([0.0838, 0.0876, 0.0978, ..., 0.0867, 0.1354, 0.2532])\n",
      " array([0.0557, 0.0758, 0.0704, ..., 0.0835, 0.0757, 0.0533])\n",
      " array([0.1753, 0.1835, 0.1707, ..., 0.2444, 0.1715, 0.1709])\n",
      " array([0.1428, 0.2186, 0.1533, ..., 0.1401, 0.0962, 0.1062])\n",
      " array([0.0577, 0.066 , 0.0654, ..., 0.0284, 0.0513, 0.064 ])\n",
      " array([0.1342, 0.1804, 0.2044, ..., 0.5156, 0.3767, 0.1868])\n",
      " array([0.143 , 0.128 , 0.1128, ..., 0.0523, 0.0611, 0.0683])\n",
      " array([0.1342, 0.1258, 0.0939, ..., 0.1575, 0.1324, 0.1571])\n",
      " array([0.1441, 0.1045, 0.0895, ..., 0.2377, 0.1111, 0.1338])\n",
      " array([0.1233, 0.078 , 0.0901, ..., 0.065 , 0.0603, 0.1015])\n",
      " array([0.1048, 0.088 , 0.1005, ..., 0.1149, 0.1002, 0.0826])\n",
      " array([0.0803, 0.0725, 0.124 , ..., 0.0871, 0.0982, 0.053 ])\n",
      " array([0.0713, 0.1311, 0.0792, ..., 0.161 , 0.0749, 0.0716])\n",
      " array([0.1366, 0.1065, 0.1074, ..., 0.0865, 0.0661, 0.093 ])\n",
      " array([0.0906, 0.0726, 0.1019, ..., 0.0996, 0.0855, 0.0951])\n",
      " array([0.0633, 0.0753, 0.1626, ..., 0.1117, 0.1076, 0.1399])\n",
      " array([0.1792, 0.1546, 0.1283, ..., 0.1421, 0.1025, 0.111 ])\n",
      " array([0.1584, 0.2889, 0.1568, ..., 0.091 , 0.2622, 0.0582])\n",
      " array([0.0888, 0.0947, 0.1346, ..., 0.1205, 0.199 , 0.1121])\n",
      " array([0.0823, 0.1657, 0.1174, ..., 0.0918, 0.0915, 0.1214])\n",
      " array([0.158 , 0.0621, 0.1037, ..., 0.1259, 0.0742, 0.0351])\n",
      " array([0.2817, 0.1244, 0.1983, ..., 0.1619, 0.1839, 0.1009])\n",
      " array([0.1759, 0.1583, 0.293 , ..., 0.3192, 0.3687, 0.1052])\n",
      " array([0.1122, 0.0822, 0.0872, ..., 0.0773, 0.0941, 0.0737])]\n",
      "[1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 1 0 0 1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "C:\\Users\\micoa\\AppData\\Local\\Temp\\ipykernel_22040\\4195563876.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_filtered = np.array(X_filtered)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X1,X2),axis=0)\n",
    "y=  np.concatenate((early_stage['gt'],de_novo[\"gt\"]),axis=0)\n",
    "X_filtered =[]\n",
    "y_filtered =[]\n",
    "\n",
    "\n",
    "for i,e in enumerate(X):\n",
    "    if len(e)>1:\n",
    "        X_filtered.append(e)\n",
    "        y_filtered.append(y[i])\n",
    "min = len(X_filtered[0])\n",
    "for i,e in enumerate(X_filtered):\n",
    "    if len(e)<min:\n",
    "        min = len(e)\n",
    "        print(i)\n",
    "X_filtered = np.array(X_filtered)\n",
    "y_filtered = np.array(y_filtered)\n",
    "X_lens = [len(e) for e in X_filtered]\n",
    "avg_len = np.mean(X_lens)\n",
    "print(avg_len)\n",
    "print(min)\n",
    "print(len(X_filtered),len(y_filtered))\n",
    "print(X_filtered)\n",
    "print(y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91950d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11633377817478088\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "flat_list = list(itertools.chain(*X_filtered))\n",
    "value = sum(flat_list) / len(flat_list)\n",
    "print(value)\n",
    "X_filtered =sequence.pad_sequences(X_filtered,dtype='float32',padding='post',maxlen=6000,value =value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5c7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622a0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_filtered,y_filtered,test_size=0.17,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ae63e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 6000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0794f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.46910757 0.12045828 0.21827409 ... 0.29545456 0.08817939 0.31483424]\n",
      "  [0.13366339 0.4864865  0.5429096  ... 0.33734483 0.07084893 0.41397855]\n",
      "  [0.7502198  0.3023858  0.39190257 ... 0.4828869  0.64954126 0.3463395 ]\n",
      "  ...\n",
      "  [1.         1.         0.         ... 0.         1.         0.        ]\n",
      "  [1.         1.         0.         ... 1.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.9999995  1.        ]]\n",
      "\n",
      " [[0.06598015 0.04680851 0.03299493 ... 0.24943884 0.33675408 0.38708556]\n",
      "  [0.20579916 0.21126124 0.44745302 ... 0.99999994 1.         0.13196483]\n",
      "  [0.37642917 1.         0.403695   ... 0.2466518  0.511315   0.3173773 ]\n",
      "  ...\n",
      "  [1.         1.         0.         ... 0.         1.         0.        ]\n",
      "  [1.         1.         0.         ... 1.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.9999995  1.        ]]\n",
      "\n",
      " [[0.44088477 0.2752864  0.05301748 ... 0.2659933  0.05967315 0.23839441]\n",
      "  [0.12623763 0.09774774 0.38182652 ... 0.11733732 0.06835204 0.15982407]\n",
      "  [0.43843445 0.10449418 0.16077046 ... 0.5208334  0.2764526  0.1765889 ]\n",
      "  ...\n",
      "  [1.         1.         0.         ... 0.         1.         0.        ]\n",
      "  [1.         1.         0.         ... 1.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.9999995  1.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.46109837 0.35090014 0.31754088 ... 0.21296297 0.3139491  0.2554974 ]\n",
      "  [0.40947667 0.52702695 0.4125746  ... 0.39526135 0.17915106 0.1490714 ]\n",
      "  [0.25989446 0.24320328 0.3537736  ... 0.38727683 0.6538226  0.45695898]\n",
      "  ...\n",
      "  [0.         0.         1.         ... 1.         0.         1.        ]\n",
      "  [0.         0.         0.9999999  ... 0.         1.         1.        ]\n",
      "  [1.         1.         1.         ... 1.         0.         0.        ]]\n",
      "\n",
      " [[0.16895501 0.08674306 0.1469261  ... 0.03843996 0.12542759 0.15078533]\n",
      "  [0.1350778  0.02072072 0.21064712 ... 0.09552461 0.06679149 0.06500491]\n",
      "  [0.         0.09617163 0.18278302 ... 0.07254465 0.14434251 0.15687853]\n",
      "  ...\n",
      "  [1.         1.         0.         ... 0.         1.         0.        ]\n",
      "  [1.         1.         0.         ... 1.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.9999995  1.        ]]\n",
      "\n",
      " [[0.01678108 0.07463175 0.10434292 ... 0.24943884 0.23755227 0.3556719 ]\n",
      "  [0.14710042 0.24954954 0.45433688 ... 0.22038358 0.20099875 0.20674488]\n",
      "  [0.35004398 0.16238211 0.3231132  ... 0.22879466 0.3211009  0.3982301 ]\n",
      "  ...\n",
      "  [1.         1.         0.         ... 0.         1.         0.        ]\n",
      "  [1.         1.         0.         ... 1.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.9999995  1.        ]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], int(X_train.shape[1]/100),100))\n",
    "print(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], int(X_test.shape[1]/100),100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c51390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afdac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc816cc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 60, 30)            15720     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60, 30)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 60, 30)            7320      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 60, 30)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 60, 30)            7320      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 60, 30)            0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 30)                7320      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,711\n",
      "Trainable params: 37,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/250\n",
      "2/2 [==============================] - 4s 43ms/step - loss: 0.6954 - accuracy: 0.5571\n",
      "Epoch 2/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6884 - accuracy: 0.5571\n",
      "Epoch 3/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6923 - accuracy: 0.5143\n",
      "Epoch 4/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6994 - accuracy: 0.5143\n",
      "Epoch 5/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6869 - accuracy: 0.6000\n",
      "Epoch 6/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6887 - accuracy: 0.5429\n",
      "Epoch 7/250\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6887 - accuracy: 0.5286\n",
      "Epoch 8/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6976 - accuracy: 0.4714\n",
      "Epoch 9/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6980 - accuracy: 0.4143\n",
      "Epoch 10/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6974 - accuracy: 0.4143\n",
      "Epoch 11/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6917 - accuracy: 0.5571\n",
      "Epoch 12/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6969 - accuracy: 0.4857\n",
      "Epoch 13/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6943 - accuracy: 0.4571\n",
      "Epoch 14/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6932 - accuracy: 0.5143\n",
      "Epoch 15/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6822 - accuracy: 0.6429\n",
      "Epoch 16/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6955 - accuracy: 0.4857\n",
      "Epoch 17/250\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6956 - accuracy: 0.3857\n",
      "Epoch 18/250\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6977 - accuracy: 0.5000\n",
      "Epoch 19/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6891 - accuracy: 0.5857\n",
      "Epoch 20/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6944 - accuracy: 0.5286\n",
      "Epoch 21/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6888 - accuracy: 0.5857\n",
      "Epoch 22/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6968 - accuracy: 0.4143\n",
      "Epoch 23/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6977 - accuracy: 0.5000\n",
      "Epoch 24/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6942 - accuracy: 0.4857\n",
      "Epoch 25/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6894 - accuracy: 0.5143\n",
      "Epoch 26/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6947 - accuracy: 0.5143\n",
      "Epoch 27/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6929 - accuracy: 0.5286\n",
      "Epoch 28/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6909 - accuracy: 0.5429\n",
      "Epoch 29/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6946 - accuracy: 0.4857\n",
      "Epoch 30/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6943 - accuracy: 0.5000\n",
      "Epoch 31/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6952 - accuracy: 0.5429\n",
      "Epoch 32/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6930 - accuracy: 0.5143\n",
      "Epoch 33/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 34/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6935 - accuracy: 0.5714\n",
      "Epoch 35/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6947 - accuracy: 0.5286\n",
      "Epoch 36/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6908 - accuracy: 0.5571\n",
      "Epoch 37/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7011 - accuracy: 0.4286\n",
      "Epoch 38/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6938 - accuracy: 0.5857\n",
      "Epoch 39/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6924 - accuracy: 0.4857\n",
      "Epoch 40/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6983 - accuracy: 0.4286\n",
      "Epoch 41/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6923 - accuracy: 0.4000\n",
      "Epoch 42/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7059 - accuracy: 0.3571\n",
      "Epoch 43/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6922 - accuracy: 0.5143\n",
      "Epoch 44/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6950 - accuracy: 0.4857\n",
      "Epoch 45/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6972 - accuracy: 0.4143\n",
      "Epoch 46/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6931 - accuracy: 0.5143\n",
      "Epoch 47/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6899 - accuracy: 0.5571\n",
      "Epoch 48/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7005 - accuracy: 0.4143\n",
      "Epoch 49/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6933 - accuracy: 0.4571\n",
      "Epoch 50/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6926 - accuracy: 0.4429\n",
      "Epoch 51/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6917 - accuracy: 0.5429\n",
      "Epoch 52/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6982 - accuracy: 0.4714\n",
      "Epoch 53/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6983 - accuracy: 0.4143\n",
      "Epoch 54/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6929 - accuracy: 0.5286\n",
      "Epoch 55/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6943 - accuracy: 0.4286\n",
      "Epoch 56/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6940 - accuracy: 0.4571\n",
      "Epoch 57/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6960 - accuracy: 0.4286\n",
      "Epoch 58/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6939 - accuracy: 0.4571\n",
      "Epoch 59/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6928 - accuracy: 0.4714\n",
      "Epoch 60/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6907 - accuracy: 0.5571\n",
      "Epoch 61/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6965 - accuracy: 0.3857\n",
      "Epoch 62/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6959 - accuracy: 0.4429\n",
      "Epoch 63/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6927 - accuracy: 0.5857\n",
      "Epoch 64/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6991 - accuracy: 0.4429\n",
      "Epoch 65/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 66/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6930 - accuracy: 0.5143\n",
      "Epoch 67/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6957 - accuracy: 0.4286\n",
      "Epoch 68/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6934 - accuracy: 0.5286\n",
      "Epoch 69/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6914 - accuracy: 0.5714\n",
      "Epoch 70/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6932 - accuracy: 0.4857\n",
      "Epoch 71/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6943 - accuracy: 0.4286\n",
      "Epoch 72/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6947 - accuracy: 0.4571\n",
      "Epoch 73/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6932 - accuracy: 0.5143\n",
      "Epoch 74/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6946 - accuracy: 0.5000\n",
      "Epoch 75/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6943 - accuracy: 0.4857\n",
      "Epoch 76/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6962 - accuracy: 0.4429\n",
      "Epoch 77/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6961 - accuracy: 0.4286\n",
      "Epoch 78/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6956 - accuracy: 0.5429\n",
      "Epoch 79/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6938 - accuracy: 0.4571\n",
      "Epoch 80/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6929 - accuracy: 0.4857\n",
      "Epoch 81/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6954 - accuracy: 0.5000\n",
      "Epoch 82/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6934 - accuracy: 0.5286\n",
      "Epoch 83/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6938 - accuracy: 0.4286\n",
      "Epoch 84/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6941 - accuracy: 0.5000\n",
      "Epoch 85/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6937 - accuracy: 0.4429\n",
      "Epoch 86/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 87/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6920 - accuracy: 0.5429\n",
      "Epoch 88/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6946 - accuracy: 0.4429\n",
      "Epoch 89/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6941 - accuracy: 0.4571\n",
      "Epoch 90/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6970 - accuracy: 0.4286\n",
      "Epoch 91/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6923 - accuracy: 0.5429\n",
      "Epoch 92/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6941 - accuracy: 0.4429\n",
      "Epoch 93/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6955 - accuracy: 0.4286\n",
      "Epoch 94/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6879 - accuracy: 0.6571\n",
      "Epoch 95/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6983 - accuracy: 0.3714\n",
      "Epoch 96/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6947 - accuracy: 0.3857\n",
      "Epoch 97/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6922 - accuracy: 0.4857\n",
      "Epoch 98/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6935 - accuracy: 0.5000\n",
      "Epoch 99/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6935 - accuracy: 0.5429\n",
      "Epoch 100/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6911 - accuracy: 0.5571\n",
      "Epoch 101/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6904 - accuracy: 0.5571\n",
      "Epoch 102/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6898 - accuracy: 0.6286\n",
      "Epoch 103/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6947 - accuracy: 0.5143\n",
      "Epoch 104/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6910 - accuracy: 0.4857\n",
      "Epoch 105/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 106/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6914 - accuracy: 0.5429\n",
      "Epoch 107/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6875 - accuracy: 0.6143\n",
      "Epoch 108/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6944 - accuracy: 0.5000\n",
      "Epoch 109/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6930 - accuracy: 0.5571\n",
      "Epoch 110/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6942 - accuracy: 0.4429\n",
      "Epoch 111/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6920 - accuracy: 0.6143\n",
      "Epoch 112/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6953 - accuracy: 0.4857\n",
      "Epoch 113/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6867 - accuracy: 0.5714\n",
      "Epoch 114/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6909 - accuracy: 0.6143\n",
      "Epoch 115/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6975 - accuracy: 0.3571\n",
      "Epoch 116/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6928 - accuracy: 0.4571\n",
      "Epoch 117/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6956 - accuracy: 0.4571\n",
      "Epoch 118/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6964 - accuracy: 0.5429\n",
      "Epoch 119/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6903 - accuracy: 0.5857\n",
      "Epoch 120/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6940 - accuracy: 0.4571\n",
      "Epoch 121/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6915 - accuracy: 0.5571\n",
      "Epoch 122/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6939 - accuracy: 0.5286\n",
      "Epoch 123/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6959 - accuracy: 0.4286\n",
      "Epoch 124/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6908 - accuracy: 0.5143\n",
      "Epoch 125/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6892 - accuracy: 0.5286\n",
      "Epoch 126/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6951 - accuracy: 0.4714\n",
      "Epoch 127/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6920 - accuracy: 0.4714\n",
      "Epoch 128/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6959 - accuracy: 0.5286\n",
      "Epoch 129/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6888 - accuracy: 0.5429\n",
      "Epoch 130/250\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.6894 - accuracy: 0.5143\n",
      "Epoch 131/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6946 - accuracy: 0.4143\n",
      "Epoch 132/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6942 - accuracy: 0.4143\n",
      "Epoch 133/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6915 - accuracy: 0.6000\n",
      "Epoch 134/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6875 - accuracy: 0.5571\n",
      "Epoch 135/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6942 - accuracy: 0.5143\n",
      "Epoch 136/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6900 - accuracy: 0.5429\n",
      "Epoch 137/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6949 - accuracy: 0.5000\n",
      "Epoch 138/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6969 - accuracy: 0.4714\n",
      "Epoch 139/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6900 - accuracy: 0.6286\n",
      "Epoch 140/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6974 - accuracy: 0.5143\n",
      "Epoch 141/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6923 - accuracy: 0.4714\n",
      "Epoch 142/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6874 - accuracy: 0.6143\n",
      "Epoch 143/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6912 - accuracy: 0.5714\n",
      "Epoch 144/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6911 - accuracy: 0.5143\n",
      "Epoch 145/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6947 - accuracy: 0.4286\n",
      "Epoch 146/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.7001 - accuracy: 0.3857\n",
      "Epoch 147/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6955 - accuracy: 0.4429\n",
      "Epoch 148/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6848 - accuracy: 0.7286\n",
      "Epoch 149/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6943 - accuracy: 0.4429\n",
      "Epoch 150/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6922 - accuracy: 0.5286\n",
      "Epoch 151/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6929 - accuracy: 0.4429\n",
      "Epoch 152/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6916 - accuracy: 0.5714\n",
      "Epoch 153/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6939 - accuracy: 0.5000\n",
      "Epoch 154/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6946 - accuracy: 0.5000\n",
      "Epoch 155/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6951 - accuracy: 0.5143\n",
      "Epoch 156/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6915 - accuracy: 0.5714\n",
      "Epoch 157/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6966 - accuracy: 0.3714\n",
      "Epoch 158/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6952 - accuracy: 0.5286\n",
      "Epoch 159/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6914 - accuracy: 0.5143\n",
      "Epoch 160/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6911 - accuracy: 0.5571\n",
      "Epoch 161/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6952 - accuracy: 0.4429\n",
      "Epoch 162/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 163/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6967 - accuracy: 0.3714\n",
      "Epoch 164/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6886 - accuracy: 0.5429\n",
      "Epoch 165/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 166/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6941 - accuracy: 0.4429\n",
      "Epoch 167/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6998 - accuracy: 0.3571\n",
      "Epoch 168/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6923 - accuracy: 0.5286\n",
      "Epoch 169/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6930 - accuracy: 0.4857\n",
      "Epoch 170/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6943 - accuracy: 0.4571\n",
      "Epoch 171/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6944 - accuracy: 0.4143\n",
      "Epoch 172/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6943 - accuracy: 0.4714\n",
      "Epoch 173/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6967 - accuracy: 0.4429\n",
      "Epoch 174/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6908 - accuracy: 0.4857\n",
      "Epoch 175/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6920 - accuracy: 0.5571\n",
      "Epoch 176/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6920 - accuracy: 0.5143\n",
      "Epoch 177/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6951 - accuracy: 0.4429\n",
      "Epoch 178/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6952 - accuracy: 0.4857\n",
      "Epoch 179/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6945 - accuracy: 0.4143\n",
      "Epoch 180/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6938 - accuracy: 0.5143\n",
      "Epoch 181/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6921 - accuracy: 0.5286\n",
      "Epoch 182/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6942 - accuracy: 0.4429\n",
      "Epoch 183/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6960 - accuracy: 0.4143\n",
      "Epoch 184/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6951 - accuracy: 0.5000\n",
      "Epoch 185/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6911 - accuracy: 0.5286\n",
      "Epoch 186/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6943 - accuracy: 0.4571\n",
      "Epoch 187/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6950 - accuracy: 0.4286\n",
      "Epoch 188/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6923 - accuracy: 0.5714\n",
      "Epoch 189/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6929 - accuracy: 0.4857\n",
      "Epoch 190/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6958 - accuracy: 0.4286\n",
      "Epoch 191/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6885 - accuracy: 0.6571\n",
      "Epoch 192/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6907 - accuracy: 0.5714\n",
      "Epoch 193/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6977 - accuracy: 0.3286\n",
      "Epoch 194/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6925 - accuracy: 0.5857\n",
      "Epoch 195/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6922 - accuracy: 0.5429\n",
      "Epoch 196/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6939 - accuracy: 0.4571\n",
      "Epoch 197/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6925 - accuracy: 0.5000\n",
      "Epoch 198/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6920 - accuracy: 0.5143\n",
      "Epoch 199/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6922 - accuracy: 0.5143\n",
      "Epoch 200/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6913 - accuracy: 0.5000\n",
      "Epoch 201/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6946 - accuracy: 0.5143\n",
      "Epoch 202/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6918 - accuracy: 0.4857\n",
      "Epoch 203/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6900 - accuracy: 0.5857\n",
      "Epoch 204/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6914 - accuracy: 0.5714\n",
      "Epoch 205/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6951 - accuracy: 0.4000\n",
      "Epoch 206/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6943 - accuracy: 0.4714\n",
      "Epoch 207/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6925 - accuracy: 0.5571\n",
      "Epoch 208/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6917 - accuracy: 0.6286\n",
      "Epoch 209/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6912 - accuracy: 0.5286\n",
      "Epoch 210/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6904 - accuracy: 0.6000\n",
      "Epoch 211/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6965 - accuracy: 0.4857\n",
      "Epoch 212/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6908 - accuracy: 0.6143\n",
      "Epoch 213/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6895 - accuracy: 0.6286\n",
      "Epoch 214/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6884 - accuracy: 0.5857\n",
      "Epoch 215/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6941 - accuracy: 0.4857\n",
      "Epoch 216/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6917 - accuracy: 0.5714\n",
      "Epoch 217/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6945 - accuracy: 0.4857\n",
      "Epoch 218/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6945 - accuracy: 0.4857\n",
      "Epoch 219/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6965 - accuracy: 0.4429\n",
      "Epoch 220/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6910 - accuracy: 0.5429\n",
      "Epoch 221/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6916 - accuracy: 0.5143\n",
      "Epoch 222/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6903 - accuracy: 0.5143\n",
      "Epoch 223/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6942 - accuracy: 0.5000\n",
      "Epoch 224/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6898 - accuracy: 0.5571\n",
      "Epoch 225/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6937 - accuracy: 0.4714\n",
      "Epoch 226/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6944 - accuracy: 0.4714\n",
      "Epoch 227/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6924 - accuracy: 0.4857\n",
      "Epoch 228/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6909 - accuracy: 0.5143\n",
      "Epoch 229/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6932 - accuracy: 0.4429\n",
      "Epoch 230/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6943 - accuracy: 0.4571\n",
      "Epoch 231/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6919 - accuracy: 0.5286\n",
      "Epoch 232/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6941 - accuracy: 0.4429\n",
      "Epoch 233/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6905 - accuracy: 0.5571\n",
      "Epoch 234/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6868 - accuracy: 0.6286\n",
      "Epoch 235/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6957 - accuracy: 0.4286\n",
      "Epoch 236/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6932 - accuracy: 0.5571\n",
      "Epoch 237/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6938 - accuracy: 0.4857\n",
      "Epoch 238/250\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6930 - accuracy: 0.4857\n",
      "Epoch 239/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6926 - accuracy: 0.5000\n",
      "Epoch 240/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6914 - accuracy: 0.5429\n",
      "Epoch 241/250\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.6940 - accuracy: 0.5429\n",
      "Epoch 242/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6871 - accuracy: 0.6571\n",
      "Epoch 243/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6959 - accuracy: 0.4286\n",
      "Epoch 244/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6937 - accuracy: 0.4714\n",
      "Epoch 245/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 246/250\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6957 - accuracy: 0.4286\n",
      "Epoch 247/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 248/250\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6923 - accuracy: 0.4857\n",
      "Epoch 249/250\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.6902 - accuracy: 0.5143\n",
      "Epoch 250/250\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6902 - accuracy: 0.5429\n",
      "Accuracy: 46.67%\n",
      "Train: 0.543, Test: 0.467\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "clf = Sequential()\n",
    "clf.add(LSTM(units = 30,return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units = 30,return_sequences = True))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units = 30,return_sequences = True))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units =30 ))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "clf.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "print(clf.summary())\n",
    "\n",
    "\n",
    "\n",
    "history=clf.fit(X_train, y_train, epochs=250, batch_size=35)\n",
    "scores = clf.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "train_acc = clf.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = clf.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train loss')\n",
    "pyplot.plot(history.history['accuracy'], label='train accuracy')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e44e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "def LSTM_Network(neurons=30,dropout=0.2):\n",
    "\n",
    "    clf = Sequential()\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units =neurons))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "    adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    clf.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "    \n",
    "    print(clf.summary())\n",
    "    return clf\n",
    "lstm_clf = KerasClassifier(model=LSTM_Network, epochs=300, batch_size=35, verbose=0)\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a575c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores1=evaluate_model(lstm_clf,X_filtered,y_filtered)\n",
    "# results = [scores1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d0752b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pyplot.boxplot(results, labels=['RNN'], showmeans=True)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 5s 11ms/step - loss: 0.6930 - accuracy: 0.5789\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6933 - accuracy: 0.3947\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6930 - accuracy: 0.5526\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.4868\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6933 - accuracy: 0.4605\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5132\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.4868\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6934 - accuracy: 0.4211\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.5263\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5789\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.4605\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5526\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5658\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5658\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5395\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.4868\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6933 - accuracy: 0.4737\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5921\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.6053\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5263\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5263\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5526\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5395\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5526\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5789\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5395\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5658\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5263\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5263\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5395\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6929 - accuracy: 0.5395\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6928 - accuracy: 0.5395\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6934 - accuracy: 0.5000\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5132\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5658\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.5132\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5263\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5263\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5132\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5526\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5395\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5395\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5789\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5789\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.4868\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5263\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5658\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5526\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5526\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5526\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5132\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6933 - accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.5526\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.4868\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6935 - accuracy: 0.4605\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5263\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5263\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.4605\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5263\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.5132\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6927 - accuracy: 0.5395\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5395\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.4868\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5395\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.5395\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.4868\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.5000\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.4737\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6933 - accuracy: 0.4605\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5132\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6926 - accuracy: 0.5000\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.4868\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6924 - accuracy: 0.5921\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.5658\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.5395\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.5526\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.5789\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5132\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5395\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5526\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.4868\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6926 - accuracy: 0.5658\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5132\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.4868\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.5000\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5000\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.4868\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.4868\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.4868\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.4868\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.4868\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.4737\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.4868\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.4868\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5000\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.4868\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.4868\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.5263\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.5526\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6923 - accuracy: 0.5263\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5921\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.5658\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6926 - accuracy: 0.5526\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.5395\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6920 - accuracy: 0.5263\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6918 - accuracy: 0.5263\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6918 - accuracy: 0.4868\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5395\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6913 - accuracy: 0.5263\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6913 - accuracy: 0.5395\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6918 - accuracy: 0.5132\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6918 - accuracy: 0.5000\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5132\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6915 - accuracy: 0.5132\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6916 - accuracy: 0.4868\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6916 - accuracy: 0.5526\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.4868\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6904 - accuracy: 0.5132\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.4868\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6915 - accuracy: 0.4868\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6908 - accuracy: 0.5132\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6915 - accuracy: 0.5000\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6912 - accuracy: 0.5000\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6901 - accuracy: 0.5789\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6902 - accuracy: 0.5395\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.5395\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6906 - accuracy: 0.5526\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6904 - accuracy: 0.6447\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6911 - accuracy: 0.5921\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6904 - accuracy: 0.6974\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.6906 - accuracy: 0.7368\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6904 - accuracy: 0.7105\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6899 - accuracy: 0.7237\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6895 - accuracy: 0.7237\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6907 - accuracy: 0.5658\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6889 - accuracy: 0.6184\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6897 - accuracy: 0.5658\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6897 - accuracy: 0.5263\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.5526\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6887 - accuracy: 0.5000\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.5000\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6875 - accuracy: 0.5000\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6867 - accuracy: 0.5000\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6860 - accuracy: 0.4868\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6858 - accuracy: 0.5132\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6874 - accuracy: 0.4868\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6886 - accuracy: 0.4868\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6866 - accuracy: 0.5000\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6843 - accuracy: 0.5000\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6872 - accuracy: 0.5000\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6825 - accuracy: 0.5132\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6851 - accuracy: 0.5000\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6810 - accuracy: 0.5132\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6816 - accuracy: 0.5000\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6812 - accuracy: 0.5000\n",
      "Epoch 166/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6795 - accuracy: 0.5132\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6799 - accuracy: 0.5000\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6779 - accuracy: 0.5132\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6760 - accuracy: 0.5921\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6797 - accuracy: 0.6053\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6762 - accuracy: 0.6842\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6757 - accuracy: 0.6316\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6690 - accuracy: 0.6184\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6704 - accuracy: 0.5921\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6669 - accuracy: 0.6447\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6686 - accuracy: 0.5921\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6652 - accuracy: 0.5263\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6654 - accuracy: 0.6184\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6588 - accuracy: 0.6711\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6483 - accuracy: 0.7105\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6566 - accuracy: 0.6184\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6465 - accuracy: 0.5921\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6534 - accuracy: 0.5658\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6461 - accuracy: 0.6447\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6285 - accuracy: 0.6974\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6335 - accuracy: 0.7763\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6302 - accuracy: 0.7105\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6220 - accuracy: 0.6447\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6135 - accuracy: 0.6579\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6224 - accuracy: 0.6184\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6126 - accuracy: 0.6842\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5968 - accuracy: 0.7763\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5868 - accuracy: 0.7632\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5970 - accuracy: 0.7368\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5826 - accuracy: 0.7237\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5744 - accuracy: 0.7632\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5668 - accuracy: 0.8289\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5704 - accuracy: 0.7895\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5491 - accuracy: 0.8158\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5594 - accuracy: 0.7895\n",
      "accuracy: 66.67%\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 5s 11ms/step - loss: 0.6931 - accuracy: 0.4737\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.6316\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.4737\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.4605\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6932 - accuracy: 0.4737\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5263\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6930 - accuracy: 0.5132\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5921\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5921\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5395\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5132\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5132\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6931 - accuracy: 0.4868\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.4868\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6933 - accuracy: 0.4868\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6926 - accuracy: 0.5658\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5395\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5132\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5395\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5526\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5395\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5395\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5526\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5526\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5263\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.5658\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.6184\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5132\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5395\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6926 - accuracy: 0.5658\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6926 - accuracy: 0.5789\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5658\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5921\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6926 - accuracy: 0.5658\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.6053\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.6579\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5526\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.6184\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5658\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.4737\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.4605\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6924 - accuracy: 0.5132\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6918 - accuracy: 0.5526\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6923 - accuracy: 0.4868\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.5263\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6925 - accuracy: 0.5132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.4868\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.5395\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.5263\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5263\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5395\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.5526\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6918 - accuracy: 0.5658\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.6316\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.6316\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6916 - accuracy: 0.6447\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6918 - accuracy: 0.6447\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6913 - accuracy: 0.6447\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.6184\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6915 - accuracy: 0.6579\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6919 - accuracy: 0.6316\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.5526\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6914 - accuracy: 0.5526\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6911 - accuracy: 0.5132\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6921 - accuracy: 0.5000\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6912 - accuracy: 0.5263\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6907 - accuracy: 0.5658\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6915 - accuracy: 0.5132\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5132\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6911 - accuracy: 0.5658\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6914 - accuracy: 0.5658\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6909 - accuracy: 0.5921\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6911 - accuracy: 0.5789\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6920 - accuracy: 0.5263\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6906 - accuracy: 0.6711\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6903 - accuracy: 0.6842\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6910 - accuracy: 0.6842\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6907 - accuracy: 0.6579\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6908 - accuracy: 0.6316\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6909 - accuracy: 0.6579\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6907 - accuracy: 0.7105\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6894 - accuracy: 0.6842\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6908 - accuracy: 0.5789\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6898 - accuracy: 0.6711\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6905 - accuracy: 0.5526\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6898 - accuracy: 0.5263\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6893 - accuracy: 0.5263\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6891 - accuracy: 0.5000\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6892 - accuracy: 0.5658\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6893 - accuracy: 0.5132\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.5526\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6883 - accuracy: 0.5263\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6890 - accuracy: 0.5263\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6870 - accuracy: 0.5526\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6882 - accuracy: 0.5000\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6886 - accuracy: 0.5395\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6845 - accuracy: 0.4868\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6883 - accuracy: 0.4868\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6893 - accuracy: 0.4868\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6855 - accuracy: 0.4868\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6860 - accuracy: 0.4868\n",
      "Epoch 104/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6855 - accuracy: 0.4868\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6847 - accuracy: 0.5000\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6862 - accuracy: 0.4868\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6857 - accuracy: 0.5000\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6836 - accuracy: 0.5526\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6839 - accuracy: 0.5132\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6837 - accuracy: 0.5000\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6828 - accuracy: 0.5132\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6817 - accuracy: 0.5000\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6839 - accuracy: 0.4868\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6822 - accuracy: 0.6184\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6822 - accuracy: 0.7632\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6826 - accuracy: 0.8158\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6824 - accuracy: 0.7632\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6805 - accuracy: 0.7763\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6818 - accuracy: 0.7632\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6769 - accuracy: 0.7368\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6765 - accuracy: 0.8026\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6776 - accuracy: 0.8158\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6756 - accuracy: 0.7763\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6762 - accuracy: 0.6316\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6737 - accuracy: 0.5921\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6724 - accuracy: 0.5526\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6718 - accuracy: 0.5658\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6724 - accuracy: 0.6053\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6704 - accuracy: 0.6316\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6663 - accuracy: 0.7500\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6684 - accuracy: 0.8421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6680 - accuracy: 0.7895\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6678 - accuracy: 0.7895\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6632 - accuracy: 0.7368\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6586 - accuracy: 0.7105\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6589 - accuracy: 0.6579\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6563 - accuracy: 0.7632\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6514 - accuracy: 0.7632\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6481 - accuracy: 0.8026\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6452 - accuracy: 0.7632\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6413 - accuracy: 0.6579\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6424 - accuracy: 0.5921\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6339 - accuracy: 0.6447\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6300 - accuracy: 0.7105\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6198 - accuracy: 0.8421\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6286 - accuracy: 0.8026\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6210 - accuracy: 0.7895\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6036 - accuracy: 0.7895\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6012 - accuracy: 0.8026\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5926 - accuracy: 0.7763\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6027 - accuracy: 0.7632\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5828 - accuracy: 0.8026\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5735 - accuracy: 0.7763\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5693 - accuracy: 0.8421\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5745 - accuracy: 0.8026\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5569 - accuracy: 0.8289\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5409 - accuracy: 0.8289\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5639 - accuracy: 0.7368\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5320 - accuracy: 0.7632\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5159 - accuracy: 0.8289\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5139 - accuracy: 0.8289\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5018 - accuracy: 0.8026\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4939 - accuracy: 0.8421\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5035 - accuracy: 0.8289\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5085 - accuracy: 0.8026\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4748 - accuracy: 0.8421\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5277 - accuracy: 0.7368\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4615 - accuracy: 0.8421\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4646 - accuracy: 0.8026\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4655 - accuracy: 0.8289\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4470 - accuracy: 0.8684\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4421 - accuracy: 0.8421\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4352 - accuracy: 0.8684\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4237 - accuracy: 0.8684\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4214 - accuracy: 0.8816\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4097 - accuracy: 0.8684\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3973 - accuracy: 0.8947\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4050 - accuracy: 0.8684\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3799 - accuracy: 0.8553\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3991 - accuracy: 0.8684\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3750 - accuracy: 0.8947\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3774 - accuracy: 0.8816\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3593 - accuracy: 0.8816\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3599 - accuracy: 0.8816\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3645 - accuracy: 0.8947\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3469 - accuracy: 0.9079\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3660 - accuracy: 0.8816\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3650 - accuracy: 0.8553\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3428 - accuracy: 0.8947\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3611 - accuracy: 0.8816\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3145 - accuracy: 0.9211\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3219 - accuracy: 0.8684\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3188 - accuracy: 0.8947\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2899 - accuracy: 0.9079\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2898 - accuracy: 0.9211\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2940 - accuracy: 0.9079\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2867 - accuracy: 0.8947\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2963 - accuracy: 0.9079\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2713 - accuracy: 0.9211\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2713 - accuracy: 0.9079\n",
      "accuracy: 44.44%\n",
      "Epoch 1/200\n",
      "3/3 [==============================] - 5s 10ms/step - loss: 0.6934 - accuracy: 0.3947\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.5658\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6933 - accuracy: 0.4211\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.5395\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.4605\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.4868\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5132\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.4737\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.4868\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5395\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6930 - accuracy: 0.5658\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.6579\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.4211\n",
      "Epoch 14/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5263\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5395\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5789\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5789\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.4737\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.4605\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5526\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5263\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5132\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5263\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5921\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.4605\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.5000\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6932 - accuracy: 0.5132\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5263\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6932 - accuracy: 0.4868\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.4868\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6930 - accuracy: 0.5000\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5000\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6930 - accuracy: 0.5000\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6926 - accuracy: 0.5000\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5000\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.5000\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6923 - accuracy: 0.5000\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6924 - accuracy: 0.5000\n",
      "Epoch 57/200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X_filtered, y_filtered):\n",
    "  # create model\n",
    "    X_traincv =X_filtered[train]\n",
    "    y_traincv  = y_filtered[train]\n",
    "    X_testcv = X_filtered[test]\n",
    "    y_testcv = y_filtered[test]\n",
    "    X_traincv = np.log10(X_traincv)\n",
    "    X_traincv = sc.fit_transform(X_traincv)\n",
    "    X_traincv = np.reshape(X_traincv, (X_traincv.shape[0], int(X_traincv.shape[1]/100),100))\n",
    "    X_testcv = np.log10(X_testcv)\n",
    "    X_testcv = sc.transform(X_testcv)\n",
    "    X_testcv = np.reshape(X_testcv, (X_testcv.shape[0], int(X_testcv.shape[1]/100),100))\n",
    "    clf = Sequential()\n",
    "    clf.add(Conv1D(filters=15, kernel_size=4, padding='same', activation='relu'))\n",
    "    clf.add(MaxPooling1D(pool_size=4))\n",
    "    clf.add(Conv1D(filters=15, kernel_size=4, padding='same', activation='relu'))\n",
    "    clf.add(MaxPooling1D(pool_size=4))\n",
    "    clf.add(LSTM(units = 30,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units = 30,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units = 30,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units =30))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "    adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(reduction='sum_over_batch_size')\n",
    "    clf.compile(loss=bce, optimizer=adm, metrics=['accuracy']) \n",
    "    clf.fit(X_traincv, y_traincv, epochs=200, batch_size=35)\n",
    "    # evaluate the model\n",
    "    scores = clf.evaluate(X_testcv, y_testcv, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (clf.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "pyplot.boxplot([cvscores], labels=['RNN'], showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbff9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
