{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "705d3abb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\micoa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvolutional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvolutional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaxPooling1D\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_categorical\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\micoa\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nqDataLoader as nq #data loading library\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7affcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the early stage dataset \n",
    "early_stage = pd.read_csv('GT_DataPD_MIT-CS2PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "early_stage[\"gt\"] = early_stage[\"gt\"].astype(int)\n",
    "early_stage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_time_series = []\n",
    "for index, row in early_stage.iterrows():\n",
    "    fileloc = row.file_1\n",
    "    keyPressed, htArr, pressArr, releaseArr =  nq.getDataFiltHelper( \"data_MIT-CS2PD/\" + early_stage.loc[index]['file_1'])\n",
    "    htArr =np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "\n",
    "X1 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the de-novo dataset \n",
    "de_novo = pd.read_csv('GT_DataPD_MIT-CS1PD.csv')\n",
    "# X = dataset.iloc[:, :-1].values\n",
    "# y = dataset.iloc[:, -1].values\n",
    "print(len(de_novo))\n",
    "de_novo[\"gt\"] = de_novo[\"gt\"].astype(int)\n",
    "de_novo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5259add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##using both files \n",
    "hit_time_series = []\n",
    "for index, row in de_novo.iterrows():\n",
    "    fileloc1 = row.file_1\n",
    "    keyPressed, htArr1, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_1'])\n",
    "    htArr1 = np.array(htArr1)\n",
    "    keyPressed, htArr2, pressArr, releaseArr =  nq.getDataFiltHelper( 'data_MIT-CS1PD/' + de_novo.loc[index]['file_2'])\n",
    "    htArr2 = np.array(htArr2)\n",
    "    htArr =np.concatenate((htArr1,htArr2),axis =0)\n",
    "    htArr=np.array(htArr)\n",
    "    hit_time_series.append(htArr)\n",
    "X2 = hit_time_series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282ec3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X1,X2),axis=0)\n",
    "y=  np.concatenate((early_stage['gt'],de_novo[\"gt\"]),axis=0)\n",
    "X_filtered =[]\n",
    "y_filtered =[]\n",
    "\n",
    "\n",
    "for i,e in enumerate(X):\n",
    "    if len(e)>1:\n",
    "        X_filtered.append(e)\n",
    "        y_filtered.append(y[i])\n",
    "min = len(X_filtered[0])\n",
    "for i,e in enumerate(X_filtered):\n",
    "    if len(e)<min:\n",
    "        min = len(e)\n",
    "        print(i)\n",
    "X_filtered = np.array(X_filtered)\n",
    "y_filtered = np.array(y_filtered)\n",
    "X_lens = [len(e) for e in X_filtered]\n",
    "avg_len = np.mean(X_lens)\n",
    "print(avg_len)\n",
    "print(min)\n",
    "print(len(X_filtered),len(y_filtered))\n",
    "print(X_filtered)\n",
    "print(y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91950d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "flat_list = list(itertools.chain(*X_filtered))\n",
    "value = sum(flat_list) / len(flat_list)\n",
    "print(value)\n",
    "X_filtered =sequence.pad_sequences(X_filtered,dtype='float32',padding='post',maxlen=6000,value =value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c7890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    " \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622a0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_filtered,y_filtered,test_size=0.17,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae63e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0794f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], int(X_train.shape[1]/100),100))\n",
    "print(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], int(X_test.shape[1]/100),100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c51390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41afdac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc816cc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "clf = Sequential()\n",
    "clf.add(LSTM(units = 30,return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units = 30,return_sequences = True))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units = 30,return_sequences = True))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(LSTM(units =30 ))\n",
    "clf.add(Dropout(0.2))\n",
    "clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "clf.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "print(clf.summary())\n",
    "\n",
    "\n",
    "\n",
    "history=clf.fit(X_train, y_train, epochs=250, batch_size=35)\n",
    "scores = clf.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "train_acc = clf.evaluate(X_train, y_train, verbose=0)\n",
    "test_acc = clf.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc[1], test_acc[1]))\n",
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train loss')\n",
    "pyplot.plot(history.history['accuracy'], label='train accuracy')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "def LSTM_Network(neurons=30,dropout=0.2):\n",
    "\n",
    "    clf = Sequential()\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units = neurons ,return_sequences = True))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(LSTM(units =neurons))\n",
    "    clf.add(Dropout(dropout))\n",
    "    clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "    adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    clf.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "    \n",
    "    print(clf.summary())\n",
    "    return clf\n",
    "lstm_clf = KerasClassifier(model=LSTM_Network, epochs=300, batch_size=35, verbose=0)\n",
    "def evaluate_model(model, X, y):\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a575c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores1=evaluate_model(lstm_clf,X_filtered,y_filtered)\n",
    "# results = [scores1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0752b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pyplot.boxplot(results, labels=['RNN'], showmeans=True)\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d084232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X_filtered, y_filtered):\n",
    "  # create model\n",
    "    X_traincv =X_filtered[train]\n",
    "    y_traincv  = y_filtered[train]\n",
    "    X_testcv = X_filtered[test]\n",
    "    y_testcv = y_filtered[test]\n",
    "    X_traincv = np.log10(X_traincv)\n",
    "    X_traincv = sc.fit_transform(X_traincv)\n",
    "    X_traincv = np.reshape(X_traincv, (X_traincv.shape[0], int(X_traincv.shape[1]/60),60))\n",
    "    X_testcv = np.log10(X_testcv)\n",
    "    X_testcv = sc.transform(X_testcv)\n",
    "    X_testcv = np.reshape(X_testcv, (X_testcv.shape[0], int(X_testcv.shape[1]/60),60))\n",
    "    clf = Sequential()\n",
    "    clf.add(Conv1D(filters=15, kernel_size=2, padding='same', activation='relu'))\n",
    "    clf.add(MaxPooling1D(pool_size=2))\n",
    "    clf.add(Conv1D(filters=15, kernel_size=2, padding='same', activation='relu'))\n",
    "    clf.add(MaxPooling1D(pool_size=2))\n",
    "    clf.add(Conv1D(filters=15, kernel_size=2, padding='same', activation='relu'))\n",
    "    clf.add(MaxPooling1D(pool_size=2))\n",
    "    clf.add(LSTM(units = 25,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units = 25,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units = 25,return_sequences = True))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(LSTM(units =25))\n",
    "    clf.add(Dropout(0.2))\n",
    "    clf.add(Dense(units = 1,activation='sigmoid'))\n",
    "    es = EarlyStopping(monitor='loss', mode='auto', patience =50 ,verbose=1,restore_best_weights=True)\n",
    "    adm = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    ada = tf.keras.optimizers.Adadelta(\n",
    "    learning_rate=0.00001, rho=0.95, epsilon=1e-07)\n",
    "    bce = tf.keras.losses.BinaryCrossentropy(reduction='sum_over_batch_size')\n",
    "    clf.compile(loss=bce, optimizer=adm, metrics=['accuracy']) \n",
    "    clf.fit(X_traincv, y_traincv, epochs=300, batch_size=35)\n",
    "    # evaluate the model\n",
    "    scores = clf.evaluate(X_testcv, y_testcv, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (clf.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "pyplot.boxplot([cvscores], labels=['RNN'], showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbff9f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
